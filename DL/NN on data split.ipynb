{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962328fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02feafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc541ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7f89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33064c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5b0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd27c7e",
   "metadata": {},
   "source": [
    "## General split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24238618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5c0fefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/gesture-recognition-and-biometrics-electromyogram-grabmyo-1.0.2/features_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6eaf5bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>session</th>\n",
       "      <th>participant</th>\n",
       "      <th>gesture</th>\n",
       "      <th>index</th>\n",
       "      <th>iemg</th>\n",
       "      <th>mav</th>\n",
       "      <th>ssi</th>\n",
       "      <th>myopulse</th>\n",
       "      <th>wflen</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis_f_w3</th>\n",
       "      <th>kurtosis_f_w4</th>\n",
       "      <th>kurtosis_f_w5</th>\n",
       "      <th>kurtosis_f_w6</th>\n",
       "      <th>kurtosis_f_w7</th>\n",
       "      <th>kurtosis_f_w8</th>\n",
       "      <th>kurtosis_f_w9</th>\n",
       "      <th>kurtosis_f_w10</th>\n",
       "      <th>kurtosis_f_w11</th>\n",
       "      <th>kurtosis_f_w12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11996.863087</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>997.135823</td>\n",
       "      <td>6.224512</td>\n",
       "      <td>8143.485828</td>\n",
       "      <td>...</td>\n",
       "      <td>61.940683</td>\n",
       "      <td>116.130710</td>\n",
       "      <td>31.371157</td>\n",
       "      <td>107.176549</td>\n",
       "      <td>64.459302</td>\n",
       "      <td>241.232060</td>\n",
       "      <td>54.880235</td>\n",
       "      <td>56.951175</td>\n",
       "      <td>42.644289</td>\n",
       "      <td>118.918727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10550.150337</td>\n",
       "      <td>0.036796</td>\n",
       "      <td>1019.627142</td>\n",
       "      <td>6.443555</td>\n",
       "      <td>6646.721181</td>\n",
       "      <td>...</td>\n",
       "      <td>160.171639</td>\n",
       "      <td>87.566832</td>\n",
       "      <td>154.616093</td>\n",
       "      <td>280.595181</td>\n",
       "      <td>160.756112</td>\n",
       "      <td>98.589493</td>\n",
       "      <td>135.655396</td>\n",
       "      <td>194.396727</td>\n",
       "      <td>294.858908</td>\n",
       "      <td>537.374741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9230.366582</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>630.013233</td>\n",
       "      <td>6.033789</td>\n",
       "      <td>6023.438546</td>\n",
       "      <td>...</td>\n",
       "      <td>195.666850</td>\n",
       "      <td>90.568034</td>\n",
       "      <td>94.299202</td>\n",
       "      <td>276.105575</td>\n",
       "      <td>101.698579</td>\n",
       "      <td>49.607516</td>\n",
       "      <td>150.348389</td>\n",
       "      <td>151.177847</td>\n",
       "      <td>156.860637</td>\n",
       "      <td>115.905382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15949.504704</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>1995.234560</td>\n",
       "      <td>4.999609</td>\n",
       "      <td>9318.559036</td>\n",
       "      <td>...</td>\n",
       "      <td>33.886272</td>\n",
       "      <td>80.377706</td>\n",
       "      <td>46.373047</td>\n",
       "      <td>64.445165</td>\n",
       "      <td>31.711430</td>\n",
       "      <td>33.328942</td>\n",
       "      <td>50.491362</td>\n",
       "      <td>81.876212</td>\n",
       "      <td>69.070232</td>\n",
       "      <td>142.723324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15936.956936</td>\n",
       "      <td>0.055584</td>\n",
       "      <td>1830.412922</td>\n",
       "      <td>5.870117</td>\n",
       "      <td>10476.465579</td>\n",
       "      <td>...</td>\n",
       "      <td>48.630658</td>\n",
       "      <td>39.722917</td>\n",
       "      <td>25.079065</td>\n",
       "      <td>84.877336</td>\n",
       "      <td>29.229362</td>\n",
       "      <td>48.843301</td>\n",
       "      <td>44.917769</td>\n",
       "      <td>39.354396</td>\n",
       "      <td>78.705208</td>\n",
       "      <td>176.820713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  session  participant  gesture  index          iemg       mav  \\\n",
       "0           0        1            1       10      0  11996.863087  0.041842   \n",
       "1           1        1            1       10      0  10550.150337  0.036796   \n",
       "2           2        1            1       10      0   9230.366582  0.032193   \n",
       "3           3        1            1       10      0  15949.504704  0.055627   \n",
       "4           4        1            1       10      0  15936.956936  0.055584   \n",
       "\n",
       "           ssi  myopulse         wflen  ...  kurtosis_f_w3  kurtosis_f_w4  \\\n",
       "0   997.135823  6.224512   8143.485828  ...      61.940683     116.130710   \n",
       "1  1019.627142  6.443555   6646.721181  ...     160.171639      87.566832   \n",
       "2   630.013233  6.033789   6023.438546  ...     195.666850      90.568034   \n",
       "3  1995.234560  4.999609   9318.559036  ...      33.886272      80.377706   \n",
       "4  1830.412922  5.870117  10476.465579  ...      48.630658      39.722917   \n",
       "\n",
       "   kurtosis_f_w5  kurtosis_f_w6  kurtosis_f_w7  kurtosis_f_w8  kurtosis_f_w9  \\\n",
       "0      31.371157     107.176549      64.459302     241.232060      54.880235   \n",
       "1     154.616093     280.595181     160.756112      98.589493     135.655396   \n",
       "2      94.299202     276.105575     101.698579      49.607516     150.348389   \n",
       "3      46.373047      64.445165      31.711430      33.328942      50.491362   \n",
       "4      25.079065      84.877336      29.229362      48.843301      44.917769   \n",
       "\n",
       "   kurtosis_f_w10  kurtosis_f_w11  kurtosis_f_w12  \n",
       "0       56.951175       42.644289      118.918727  \n",
       "1      194.396727      294.858908      537.374741  \n",
       "2      151.177847      156.860637      115.905382  \n",
       "3       81.876212       69.070232      142.723324  \n",
       "4       39.354396       78.705208      176.820713  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b2e97bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'session', 'participant', 'gesture', 'index', 'iemg',\n",
       "       'mav', 'ssi', 'myopulse', 'wflen',\n",
       "       ...\n",
       "       'kurtosis_f_w3', 'kurtosis_f_w4', 'kurtosis_f_w5', 'kurtosis_f_w6',\n",
       "       'kurtosis_f_w7', 'kurtosis_f_w8', 'kurtosis_f_w9', 'kurtosis_f_w10',\n",
       "       'kurtosis_f_w11', 'kurtosis_f_w12'],\n",
       "      dtype='object', length=573)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "23ac7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "02a95fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 4:].values\n",
    "Y = (data.iloc[:, 2]-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ea2fc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "54579326",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=3443)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "752185dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11869.30537947961, 11690.020190303136)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_train, axis = 0)[0], np.mean(x_test, axis = 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0a841551",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# scaler.fit(x_train)\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "446e3c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(11869.30537947961 - scaler.mean_[0]) / scaler.scale_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0c49718b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.5448661740336066e-15, -0.025975693904163086)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_train, axis = 0)[0], np.mean(x_test, axis = 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9c9788ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, output_shape):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_shape, 32),  # Input layer: Fully connected (linear) with 64 units\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(64, output_shape)  # Output layer: Fully connected (linear) with 'output_shape' units\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10edbefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "969dcaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.30960533e-03, -4.30960533e-03, -1.27448003e-02,  1.51637296e-02,\n",
       "       -7.31201255e-03, -1.07725574e-02, -9.60707174e-03, -1.13731242e-02,\n",
       "       -4.26524144e-03, -4.10398843e-03, -1.05209144e-02, -1.16482641e-02,\n",
       "       -8.39661147e-03, -9.22285350e-03, -2.23799034e-02, -1.45045427e-02,\n",
       "        1.21200481e-02, -1.04699546e-02, -9.60281236e-03, -1.48386588e-02,\n",
       "       -1.07118956e-02, -1.12877531e-02, -1.50093771e-02, -9.97453521e-03,\n",
       "       -8.32076348e-03, -1.19983399e-02, -8.32280337e-03, -1.44709320e-02,\n",
       "        1.46963853e-02,  4.62257456e-04, -1.23439304e-02, -1.08278686e-02,\n",
       "        5.09971237e-04, -1.57961928e-02,  2.80223352e-02, -1.81294651e-03,\n",
       "       -1.18381888e-02, -1.21528148e-02, -1.17554037e-02, -1.57028921e-02,\n",
       "       -1.97276395e-02, -2.04161153e-02, -2.80162313e-02, -2.09194462e-02,\n",
       "        8.64505555e-03, -1.59181720e-02, -1.25563527e-02, -2.03507592e-02,\n",
       "       -1.81027113e-02, -1.96928469e-02, -1.84992339e-02, -1.80151075e-02,\n",
       "       -2.57879847e-02, -2.88602566e-02, -2.44142760e-02, -3.38884874e-02,\n",
       "       -1.94373885e-03, -6.70189599e-03, -2.46043653e-02, -2.86859140e-02,\n",
       "       -1.67841135e-02, -3.25001751e-02,  1.55721444e-02, -4.77246782e-03,\n",
       "       -1.10532957e-02, -1.33761917e-02, -1.24874551e-02, -1.60888017e-02,\n",
       "       -1.79960101e-02, -2.03657186e-02, -3.07628999e-02, -2.36387533e-02,\n",
       "        9.01757357e-03, -1.79130103e-02, -1.35356145e-02, -1.85697452e-02,\n",
       "       -1.88785210e-02, -1.95655259e-02, -2.04880357e-02, -2.19620663e-02,\n",
       "       -2.43919237e-02, -2.70564838e-02, -2.16036506e-02, -3.00444867e-02,\n",
       "        1.50332544e-03, -6.97528244e-03, -2.45111535e-02, -2.72164019e-02,\n",
       "       -1.39873228e-02, -2.82619525e-02,  1.37229630e-02, -5.52293173e-03,\n",
       "       -1.78045378e-02, -1.22361831e-02, -1.08698335e-02, -2.12278328e-02,\n",
       "       -2.32499604e-02, -2.71554970e-02, -3.39557568e-02, -2.43443872e-02,\n",
       "       -1.09762445e-02, -1.54063388e-02, -1.44521759e-02, -1.96194597e-02,\n",
       "       -2.25340365e-02, -3.33053912e-02, -3.54449374e-02, -2.58407709e-02,\n",
       "       -2.89783244e-02, -1.94171923e-02,  4.54282770e-03, -1.55705567e-02,\n",
       "       -9.53432826e-03, -1.52955826e-03, -2.44121709e-02, -1.41202006e-02,\n",
       "       -4.87634933e-03, -1.00110354e-02, -5.18772852e-03, -4.36789164e-03,\n",
       "        2.51986036e-02,  1.02313782e-02, -1.17756357e-03, -5.47588691e-03,\n",
       "        3.55367804e-03,  1.36374243e-02,  1.76186721e-02,  7.39039846e-03,\n",
       "        1.83831144e-02,  5.45746026e-03,  9.91294397e-03, -5.84262183e-03,\n",
       "        1.85312045e-03,  1.19516322e-02,  2.10382982e-03, -1.14428841e-03,\n",
       "        8.90516785e-03,  1.34475379e-02, -1.49540590e-02, -1.57819880e-02,\n",
       "        5.90928918e-03, -4.44817443e-03,  2.27950949e-02,  1.34507543e-02,\n",
       "       -5.55771065e-03, -2.43564795e-02,  4.39332252e-03,  1.66887450e-02,\n",
       "       -1.78292094e-02, -1.27254654e-02, -1.09449357e-02, -7.47740378e-03,\n",
       "       -1.53149030e-02, -1.57036952e-02, -2.30465714e-02, -2.01963637e-02,\n",
       "       -2.13322882e-02, -1.25196000e-02, -7.16847400e-03, -2.93102751e-03,\n",
       "       -2.05142263e-02, -2.15053876e-02, -3.07133906e-02, -2.44211752e-02,\n",
       "       -2.86248752e-02, -2.34787850e-02, -1.91581219e-02, -2.18830852e-02,\n",
       "       -2.66896809e-02, -2.38408802e-02, -2.98007399e-02, -2.37011998e-02,\n",
       "       -1.66573285e-02, -1.42008425e-02, -2.01538146e-02, -2.55050388e-02,\n",
       "       -9.02440340e-03, -6.33923271e-03, -9.02440340e-03,  1.19674770e-02,\n",
       "       -9.02440340e-03, -1.66678886e-02, -4.74965931e-02, -2.93600974e-02,\n",
       "        9.02440340e-03,  1.38528030e-02,  9.22503410e-03, -6.55593856e-03,\n",
       "       -5.32828227e-03, -3.57293106e-04, -1.34137052e-02,  3.64354853e-02,\n",
       "       -2.49892123e-02, -3.56156439e-02, -5.47851646e-02, -2.57383642e-02,\n",
       "        1.53687676e-02,  6.46165945e-03,  3.49073037e-03,  7.66625271e-03,\n",
       "       -4.53941090e+09, -1.37315796e+09,  1.02854623e-02,  1.69313804e-02,\n",
       "       -9.02440340e-03, -8.61222259e-04, -9.02440340e-03,  1.55670357e-02,\n",
       "       -9.02440340e-03, -1.33986292e-02, -4.83663050e-02, -3.74285503e-02,\n",
       "        9.02440340e-03,  1.14904048e-02,  1.16865096e-02, -4.93346230e-03,\n",
       "       -5.10609528e-03,  5.15679323e-03, -1.29204724e-02,  4.09180959e-02,\n",
       "       -2.26329965e-02, -2.37435589e-02, -4.70921034e-02, -2.69975105e-02,\n",
       "        1.51235841e-02,  6.01309156e-03,  2.09835708e-03,  7.75385362e-03,\n",
       "       -3.03699480e+09, -1.41279560e+09,  1.10400178e-02,  1.94382465e-02,\n",
       "       -1.31417085e-02,  5.70786820e-03, -3.12010291e-03, -1.50463902e-02,\n",
       "       -1.86833814e-02, -1.48033015e-02, -1.09325339e-02, -6.54466008e-03,\n",
       "       -7.18285600e-03, -3.68033771e-03, -1.18157970e-02, -2.74144961e-02,\n",
       "       -1.86465772e-02, -1.98482406e-02, -5.09337542e-03,  1.33265590e-03,\n",
       "       -4.88173358e-03, -1.87319938e-03,  9.44846856e-03, -6.47644001e-03,\n",
       "        9.06895926e-03,  4.03156182e-03, -3.81071635e-03, -1.55049012e-02,\n",
       "       -8.94979470e-03, -1.25916630e-03,  5.82991153e-03, -2.71836229e-02,\n",
       "        1.51604747e-02,  1.59673434e-02,  8.65976339e-03,  1.32855836e-02,\n",
       "        1.62871026e-02,  1.95881405e-02,  2.74573526e-02,  2.03413639e-02,\n",
       "       -1.88967309e-03,  1.69679440e-02,  1.24976474e-02,  1.78640923e-02,\n",
       "        1.58698735e-02,  1.86328201e-02,  1.62623231e-02,  1.68709607e-02,\n",
       "        2.44031374e-02,  2.45614623e-02,  1.87058528e-02,  2.91551698e-02,\n",
       "        3.27444689e-03,  5.52841895e-03,  2.63567778e-02,  2.53201372e-02,\n",
       "        1.32523082e-02,  2.69258554e-02, -8.58818134e-03,  1.11032909e-02,\n",
       "       -5.33048384e-03, -9.51684989e-03, -1.66080150e-02, -1.90150739e-02,\n",
       "       -1.90998495e-02, -2.07365330e-02, -3.42149633e-02, -2.70416691e-02,\n",
       "        1.68546429e-02, -1.80331795e-02, -1.41547058e-02, -1.86456297e-02,\n",
       "       -2.18176818e-02, -2.01658644e-02, -2.54506325e-02, -2.76510291e-02,\n",
       "       -2.29869110e-02, -2.82251571e-02, -2.36957273e-02, -2.98757919e-02,\n",
       "        5.66008679e-03, -8.08755600e-03, -2.16642057e-02, -2.76979603e-02,\n",
       "       -1.42339448e-02, -2.82828297e-02,  1.75105537e-02, -2.06170531e-04,\n",
       "        1.03557949e-03, -6.55491506e-05, -4.05251005e-03, -6.50374606e-03,\n",
       "       -4.88531500e-03, -8.16227060e-03, -1.75775149e-02, -8.89951174e-03,\n",
       "        9.93217338e-03, -4.52070708e-03, -2.82360603e-03, -1.10777861e-02,\n",
       "       -5.83879348e-03, -8.16155982e-03, -8.50182469e-03, -6.05696380e-03,\n",
       "       -8.30887806e-03, -1.16323018e-02, -9.99659192e-03, -1.93991066e-02,\n",
       "        9.03849481e-03,  5.42167599e-03, -6.29390066e-03, -8.94580261e-03,\n",
       "        4.13936295e-03, -1.23961956e-02,  2.56599079e-02,  7.80769795e-03,\n",
       "        1.03553706e-03, -6.56650425e-05, -4.05263130e-03, -6.50376609e-03,\n",
       "       -4.88536885e-03, -8.16230989e-03, -1.75775529e-02, -8.89954437e-03,\n",
       "        9.93213981e-03, -4.52082829e-03, -2.82370872e-03, -1.10778739e-02,\n",
       "       -5.83884754e-03, -8.16161611e-03, -8.50184951e-03, -6.05699351e-03,\n",
       "       -8.30885456e-03, -1.16322827e-02, -9.99659912e-03, -1.93991293e-02,\n",
       "        9.03848991e-03,  5.42166691e-03, -6.29385546e-03, -8.94577504e-03,\n",
       "        4.13937161e-03, -1.23961834e-02,  2.56599295e-02,  7.80780364e-03,\n",
       "       -2.85269815e-02, -2.23995256e-02, -2.31258808e-02, -2.79235466e-02,\n",
       "       -1.96979512e-02,  6.31120204e-03, -1.87345876e-02, -1.86375545e-02,\n",
       "       -1.25300608e-02,  1.21444304e-02,  3.50793236e-03, -9.72372156e-03,\n",
       "       -6.56699032e-03,  1.88329099e-03,  1.49282609e-02,  1.88329099e-03,\n",
       "       -6.56699032e-03, -9.72372156e-03,  3.50793236e-03,  1.21444304e-02,\n",
       "       -1.25300608e-02, -1.86375545e-02, -1.87345876e-02,  6.31120204e-03,\n",
       "       -1.96979512e-02, -2.79235466e-02, -2.31258808e-02, -2.23995256e-02,\n",
       "       -1.10106074e-02, -1.28167870e-02, -1.40307095e-02, -2.11175171e-02,\n",
       "       -1.88912260e-02,  4.77760507e-03, -8.00567504e-03, -2.44581036e-02,\n",
       "       -1.83461284e-02,  1.26304870e-02, -1.95977904e-04, -1.67626275e-02,\n",
       "       -1.25353497e-02, -4.01266334e-03, -3.62373176e-04, -4.01266334e-03,\n",
       "       -1.25353497e-02, -1.67626275e-02, -1.95977904e-04,  1.26304870e-02,\n",
       "       -1.83461284e-02, -2.44581036e-02, -8.00567504e-03,  4.77760507e-03,\n",
       "       -1.88912260e-02, -2.11175171e-02, -1.40307095e-02, -1.28167870e-02,\n",
       "       -1.10106074e-02, -1.28167871e-02, -1.40307103e-02, -2.11175169e-02,\n",
       "       -1.88912261e-02,  4.77760596e-03, -8.00567523e-03, -2.44581029e-02,\n",
       "       -1.83461279e-02,  1.26304860e-02, -1.95978015e-04, -1.67626272e-02,\n",
       "       -1.25353496e-02, -4.01266070e-03, -3.62374042e-04, -4.01266070e-03,\n",
       "       -1.25353496e-02, -1.67626272e-02, -1.95978015e-04,  1.26304860e-02,\n",
       "       -1.83461279e-02, -2.44581029e-02, -8.00567523e-03,  4.77760596e-03,\n",
       "       -1.88912261e-02, -2.11175169e-02, -1.40307103e-02, -1.28167871e-02,\n",
       "       -1.58742494e-02, -1.29375109e-02, -1.36992857e-02, -2.14067339e-02,\n",
       "       -1.96668769e-02,  3.43075570e-03, -1.43624591e-02,  6.62463875e-03,\n",
       "       -6.82398011e-03,  3.58648413e-03,  1.14415456e-02,  6.03924387e-03,\n",
       "       -7.16225389e-05, -1.22495432e-04,  1.76128205e-02, -1.22495432e-04,\n",
       "       -7.16225389e-05,  6.03924387e-03,  1.14415456e-02,  3.58648413e-03,\n",
       "       -6.82398011e-03,  6.62463875e-03, -1.43624591e-02,  3.43075570e-03,\n",
       "       -1.96668769e-02, -2.14067339e-02, -1.36992857e-02, -1.29375109e-02,\n",
       "       -2.85269815e-02, -2.23995256e-02, -2.31258808e-02, -2.79235466e-02,\n",
       "       -1.96979512e-02,  6.31120204e-03, -1.87345876e-02, -1.86375545e-02,\n",
       "       -1.25300608e-02,  1.21444304e-02,  3.50793236e-03, -9.72372156e-03,\n",
       "       -6.56699032e-03,  1.88329099e-03,  1.49282609e-02,  1.88329099e-03,\n",
       "       -6.56699032e-03, -9.72372156e-03,  3.50793236e-03,  1.21444304e-02,\n",
       "       -1.25300608e-02, -1.86375545e-02, -1.87345876e-02,  6.31120204e-03,\n",
       "       -1.96979512e-02, -2.79235466e-02, -2.31258808e-02, -2.23995256e-02,\n",
       "       -7.91573496e-03, -2.41367843e-02, -1.87778107e-02, -3.74292328e-02,\n",
       "       -2.72855390e-02,  7.65349359e-04, -3.19055048e-02, -2.65182456e-02,\n",
       "       -2.48665640e-02,  1.00017959e-02, -1.88926866e-02, -2.86243525e-02,\n",
       "       -3.05229705e-03, -1.03522271e-02,  2.26776688e-03, -1.03522271e-02,\n",
       "       -3.05229705e-03, -2.86243525e-02, -1.88926866e-02,  1.00017959e-02,\n",
       "       -2.48665640e-02, -2.65182456e-02, -3.19055048e-02,  7.65349359e-04,\n",
       "       -2.72855390e-02, -3.74292328e-02, -1.87778107e-02, -2.41367843e-02,\n",
       "        7.86585470e-04, -2.61967284e-02, -2.27436137e-02, -3.92914482e-02,\n",
       "       -2.50821015e-02, -4.99853284e-03, -3.17932491e-02, -3.11936792e-02,\n",
       "       -2.67584620e-02, -1.21326532e-04, -2.54633368e-02, -2.66081047e-02,\n",
       "       -3.01070111e-03, -1.80620980e-02,  2.38511543e-03, -1.80620980e-02,\n",
       "       -3.01070111e-03, -2.66081047e-02, -2.54633368e-02, -1.21326532e-04,\n",
       "       -2.67584620e-02, -3.11936792e-02, -3.17932491e-02, -4.99853284e-03,\n",
       "       -2.50821015e-02, -3.92914482e-02, -2.27436137e-02, -2.61967284e-02])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1716454",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(x_train).type(torch.float32), torch.tensor(y_train).type(torch.LongTensor))\n",
    "test_dataset = TensorDataset(torch.tensor(x_test).type(torch.float32), torch.tensor(y_test).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f85db70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a9380055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and whether to shuffle the data\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4a15a50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12280"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "50366915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12280, 568)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ede084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model(x_train.shape[1], len(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fc999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4a705d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "18d940c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "766fcb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] - Train Loss: 2.5608, Val Loss: 95683148.8650, Val Accuracy: 28.39%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    val_loss = total_val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5451955a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296395572.7591124"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c48f401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12280, 568)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b55a98d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3071, 568)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f21fa25c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.16023784e-15,  3.01636204e-15, -5.76628864e-16,  1.08755296e-16,\n",
       "        5.21586032e-15,  5.48678909e-16,  8.53307653e-16,  1.49446145e-17,\n",
       "       -2.08252706e-15,  1.38542815e-15, -1.31577476e-15, -1.39494822e-15,\n",
       "        1.61962825e-15,  2.95488389e-16,  5.51486110e-16, -4.46299711e-16,\n",
       "        2.46360569e-15,  3.00624075e-15,  3.83921993e-16,  9.48702752e-16,\n",
       "       -8.63454937e-16, -1.57236918e-15, -2.15885489e-15,  1.54192168e-15,\n",
       "        7.82811203e-16, -4.27060667e-16, -1.90170897e-16, -2.94602381e-16,\n",
       "        2.68845749e-15, -9.13890751e-16, -4.36819193e-16, -5.36385540e-16,\n",
       "       -4.23631904e-16, -3.45154596e-16, -3.72132654e-16,  4.27142035e-16,\n",
       "       -1.56725881e-15, -4.63647624e-15,  5.60974087e-15,  3.92590412e-15,\n",
       "       -4.54134558e-15,  3.87670352e-15,  3.96387592e-15, -3.48008358e-15,\n",
       "        2.50669011e-15,  3.47822567e-15, -4.71223675e-15,  5.69889774e-15,\n",
       "       -3.10716436e-15,  2.80744709e-15,  4.43224899e-15,  3.61261871e-15,\n",
       "       -4.65710532e-16,  1.60010441e-15,  2.07610802e-15,  5.17475133e-15,\n",
       "       -7.49942996e-18, -9.92013203e-16,  4.86046803e-15, -5.16705752e-16,\n",
       "        2.23870868e-16,  1.61823595e-15, -2.53148027e-15,  5.00138295e-15,\n",
       "        5.13623707e-15,  3.18818443e-16,  5.57183235e-15,  3.43914184e-15,\n",
       "       -3.25603189e-15,  4.63830024e-15,  2.74559601e-15,  5.89415415e-15,\n",
       "       -1.74488545e-15,  1.95727889e-15, -5.19544596e-16,  8.09018976e-15,\n",
       "        1.54798360e-15,  4.90723097e-15,  5.66619679e-15,  1.59953484e-15,\n",
       "       -2.58733950e-15,  6.17567886e-15,  1.60055646e-15, -6.97320414e-16,\n",
       "        6.06384740e-17,  1.63317604e-15,  1.15203947e-15,  1.32539654e-15,\n",
       "        2.43670448e-15,  5.26045910e-15, -1.97909007e-15,  2.98199757e-15,\n",
       "        5.79412558e-15, -1.07328370e-14,  1.79748543e-15,  1.42152531e-14,\n",
       "        1.35226792e-14,  1.51332778e-14, -1.86477010e-15, -1.79074533e-14,\n",
       "        1.18258280e-14, -2.96379823e-15,  7.41363178e-15, -5.91356497e-16,\n",
       "       -5.15274124e-15, -2.21447634e-14,  3.53050018e-15, -9.63371619e-16,\n",
       "       -5.58278089e-15, -7.85109401e-15,  4.82680309e-16, -1.16248962e-15,\n",
       "       -7.25149220e-15,  9.46228257e-15,  3.66802589e-15,  3.53310509e-15,\n",
       "        8.12143060e-15,  1.26740095e-14, -1.47257704e-14, -1.29076310e-14,\n",
       "       -2.47263303e-15, -4.54250621e-15, -1.92318790e-15, -2.56752636e-15,\n",
       "        1.86938774e-15,  2.42721153e-15,  1.33606481e-16, -1.54606693e-15,\n",
       "       -1.14209221e-16, -5.56700451e-16, -2.13888805e-15, -3.36973482e-15,\n",
       "       -4.77737647e-15,  2.10313128e-15,  6.86839745e-15, -1.40921363e-15,\n",
       "       -1.90084105e-15,  1.04258802e-15,  5.55461056e-15, -1.11862428e-15,\n",
       "        2.46818942e-16, -1.23980630e-15, -9.80269069e-16,  1.83144985e-15,\n",
       "       -3.45520753e-15,  6.63385380e-16,  3.28908091e-17, -1.44925693e-17,\n",
       "       -1.27838384e-15,  3.40071348e-16,  8.50382920e-16, -9.24884490e-18,\n",
       "        4.42530290e-16,  3.57745468e-16,  1.52088349e-16,  3.46161527e-16,\n",
       "        1.20861066e-16,  2.41331792e-15,  4.50203121e-16,  1.60222677e-15,\n",
       "       -1.74493786e-15, -1.37406825e-15,  5.26099251e-16,  1.03534400e-15,\n",
       "       -4.98862397e-16,  7.70712213e-16, -2.07127113e-16,  4.37289320e-16,\n",
       "       -4.66199588e-16, -2.49849906e-16, -4.43331328e-16, -5.30272758e-16,\n",
       "        3.48053901e-16,  8.25527214e-16, -4.63021375e-16,  3.06978248e-16,\n",
       "        5.98084060e-18,  6.35225224e-17, -1.70107436e-17,  1.58715897e-17,\n",
       "       -4.18276016e-17, -1.48361236e-17, -1.28753776e-17,  3.51256076e-17,\n",
       "        3.53479574e-17, -2.21989512e-17, -8.15611391e-18,  3.75006814e-17,\n",
       "       -8.87336486e-18, -1.84479648e-17,  5.89692971e-17, -1.71664167e-18,\n",
       "        7.25210473e-17, -1.07487874e-17, -1.51802431e-18,  4.35172618e-17,\n",
       "       -1.20412129e-17,  1.25690905e-17,  3.06543155e-19, -8.09711848e-18,\n",
       "        3.17736924e-17,  9.07593761e-18,  6.32298231e-19,  4.91895816e-18,\n",
       "       -4.24247251e-17, -2.49408879e-17,  1.07759525e-16, -9.70908343e-19,\n",
       "       -3.46191757e-17,  7.79551957e-18, -1.98651266e-17, -7.04286430e-18,\n",
       "       -4.48495238e-17,  3.03774378e-18, -1.41085781e-17,  7.06603162e-19,\n",
       "       -2.32605511e-18, -3.81588310e-17, -2.79906391e-17, -8.07861288e-18,\n",
       "        7.38983725e-17, -1.64106536e-17,  8.22312608e-18,  2.38894590e-17,\n",
       "       -8.82660643e-18, -4.69285609e-17,  1.17989802e-17, -2.66791783e-17,\n",
       "       -5.15122463e-17, -1.80535554e-17, -8.76420300e-18, -1.65502226e-17,\n",
       "       -1.70421042e-17,  1.38217342e-16, -2.71227123e-19,  6.01129714e-17,\n",
       "        1.83180018e-17,  1.54692447e-16, -1.41671815e-16,  6.66066008e-17,\n",
       "       -1.13987719e-16, -5.30881889e-17,  2.80629664e-17, -5.55608762e-17,\n",
       "        4.41670768e-17, -8.38860288e-17,  6.24240525e-17, -6.64077009e-17,\n",
       "        4.50191820e-17,  8.59789981e-17, -4.21825983e-16,  5.07330334e-17,\n",
       "       -1.56226823e-17,  6.38287830e-18,  7.07902792e-18, -3.08746875e-18,\n",
       "        5.61214123e-18,  3.79243325e-17,  6.00090010e-18,  7.58622264e-17,\n",
       "        1.84206613e-15,  6.32163522e-15, -5.67084382e-15, -4.93259523e-15,\n",
       "       -1.07261286e-16, -2.93980819e-15, -4.17263039e-15,  4.15476557e-15,\n",
       "       -3.49138019e-15, -1.98081237e-15, -2.19648765e-16, -4.84091594e-15,\n",
       "        1.46947691e-15, -3.27912236e-15, -4.66322601e-15, -5.03555757e-15,\n",
       "       -2.47576118e-15, -9.62282190e-16, -2.67298850e-15, -5.88919973e-15,\n",
       "       -6.16745164e-15, -1.54330041e-15, -3.22267547e-15, -1.41151567e-15,\n",
       "       -2.66893818e-15, -1.71821026e-15, -2.86999433e-15,  4.64720553e-16,\n",
       "        3.00149880e-15,  2.65121575e-15,  4.38734282e-15, -1.63546791e-15,\n",
       "        1.89767758e-15,  5.36219187e-15,  4.40490930e-16,  5.58194461e-16,\n",
       "        5.59447530e-15,  8.35551769e-15, -3.74400565e-15, -1.04390347e-15,\n",
       "       -6.73373319e-16, -3.59699829e-15, -3.81622891e-15, -5.59972806e-15,\n",
       "       -2.44820903e-16, -5.59391476e-15,  4.07192376e-15, -2.73377050e-15,\n",
       "       -4.29515272e-16, -2.08871895e-15, -4.15519275e-15,  5.89479605e-15,\n",
       "       -3.70306391e-16, -1.40455870e-15, -1.07508103e-15, -1.95309747e-15,\n",
       "       -3.91663719e-15,  3.84139427e-15, -8.95290899e-15, -3.17878188e-16,\n",
       "       -4.49129514e-15, -6.73175097e-15,  6.01093551e-16,  1.87040936e-15,\n",
       "        2.50077284e-15, -4.20194552e-15,  6.93133119e-15, -2.04082589e-15,\n",
       "       -7.09724534e-16,  3.61916941e-15, -8.78540816e-16, -6.08428436e-15,\n",
       "       -4.82585832e-15, -4.37877205e-15, -4.83957789e-15, -7.77794857e-15,\n",
       "        3.47662543e-15,  1.03565399e-14,  2.93544143e-15, -5.70827316e-15,\n",
       "       -1.66375690e-15, -1.29926381e-15,  8.93414007e-15,  1.75572550e-15,\n",
       "       -1.54665459e-15,  1.13652527e-14, -5.55518692e-15, -2.82097002e-15,\n",
       "       -6.16687302e-15, -5.19777173e-15, -2.30474344e-15,  7.21805894e-15,\n",
       "        8.67226124e-16,  6.36750876e-17, -1.86486729e-16,  1.07461814e-14,\n",
       "       -1.09642208e-15,  4.24354936e-15, -1.50054598e-15, -3.63308732e-17,\n",
       "       -7.54192221e-17, -2.65869484e-15, -5.79235357e-15, -1.25466503e-15,\n",
       "       -3.43825583e-17,  7.62330278e-15, -6.88169323e-16, -1.07372942e-15,\n",
       "        2.49976026e-15, -2.87018870e-15,  5.91242581e-15,  1.28603245e-15,\n",
       "        2.17864543e-15, -1.73775218e-16,  2.52466117e-15, -3.23644025e-15,\n",
       "        1.62190655e-15, -1.34847345e-15,  2.42433652e-15, -7.01565118e-16,\n",
       "        5.10178219e-17, -1.38234972e-15, -6.72887370e-16, -5.59541555e-16,\n",
       "       -1.69708619e-15, -3.32754996e-16,  6.42392401e-16, -3.32754996e-16,\n",
       "       -1.69708619e-15, -5.59541555e-16, -6.72887370e-16, -1.38234972e-15,\n",
       "        5.10178219e-17, -7.01565118e-16,  2.42433652e-15, -1.34847345e-15,\n",
       "        1.62190655e-15, -3.23644025e-15,  2.52466117e-15, -1.73775218e-16,\n",
       "       -2.54949880e-15, -2.07782127e-15,  1.02808641e-15,  1.95543003e-15,\n",
       "       -2.20577718e-15, -2.68368841e-15, -2.46401253e-15, -9.35557278e-16,\n",
       "       -4.68517733e-16,  4.01373650e-15,  4.29722761e-15,  1.32897674e-15,\n",
       "        3.46881409e-16,  1.67014430e-15,  1.45540474e-16,  1.67014430e-15,\n",
       "        3.46881409e-16,  1.32897674e-15,  4.29722761e-15,  4.01373650e-15,\n",
       "       -4.68517733e-16, -9.35557278e-16, -2.46401253e-15, -2.68368841e-15,\n",
       "       -2.20577718e-15,  1.95543003e-15,  1.02808641e-15, -2.07782127e-15,\n",
       "        3.25296702e-15, -2.24951256e-16,  6.77905072e-16,  1.78814844e-15,\n",
       "        1.39681968e-16,  4.46290670e-16, -2.68686629e-16, -2.59075696e-15,\n",
       "       -5.79214563e-16,  1.10218114e-15,  2.33205601e-15,  1.02724109e-15,\n",
       "       -8.25430025e-16, -1.47269999e-15, -6.82222104e-16, -1.47269999e-15,\n",
       "       -8.25430025e-16,  1.02724109e-15,  2.33205601e-15,  1.10218114e-15,\n",
       "       -5.79214563e-16, -2.59075696e-15, -2.68686629e-16,  4.46290670e-16,\n",
       "        1.39681968e-16,  1.78814844e-15,  6.77905072e-16, -2.24951256e-16,\n",
       "        1.18158966e-16,  8.36198872e-17, -1.22944995e-16,  3.69394390e-16,\n",
       "       -2.05332494e-16,  1.69455926e-16,  2.19502981e-16,  8.29623874e-16,\n",
       "       -8.15805982e-17,  4.95102511e-18, -2.38504701e-16, -2.23534094e-16,\n",
       "       -3.71778929e-17, -3.74895215e-17, -2.57100711e-17, -3.74895215e-17,\n",
       "       -3.71778929e-17, -2.23534094e-16, -2.38504701e-16,  4.95102511e-18,\n",
       "       -8.15805982e-17,  8.29623874e-16,  2.19502981e-16,  1.69455926e-16,\n",
       "       -2.05332494e-16,  3.69394390e-16, -1.22944995e-16,  8.36198872e-17,\n",
       "        2.17864543e-15, -1.73775218e-16,  2.52466117e-15, -3.23644025e-15,\n",
       "        1.62190655e-15, -1.34847345e-15,  2.42433652e-15, -7.01565118e-16,\n",
       "        5.10178219e-17, -1.38234972e-15, -6.72887370e-16, -5.59541555e-16,\n",
       "       -1.69708619e-15, -3.32754996e-16,  6.42392401e-16, -3.32754996e-16,\n",
       "       -1.69708619e-15, -5.59541555e-16, -6.72887370e-16, -1.38234972e-15,\n",
       "        5.10178219e-17, -7.01565118e-16,  2.42433652e-15, -1.34847345e-15,\n",
       "        1.62190655e-15, -3.23644025e-15,  2.52466117e-15, -1.73775218e-16,\n",
       "        3.31487009e-15, -1.40629229e-15, -3.57931654e-15,  2.35896174e-15,\n",
       "       -3.28764793e-15, -2.37396060e-16, -5.52624360e-15,  1.38399516e-15,\n",
       "        1.54239632e-15, -9.17835297e-15, -2.89532242e-15,  3.66913792e-15,\n",
       "       -3.67025447e-15, -7.34731675e-15, -1.40495650e-16, -7.34731675e-15,\n",
       "       -3.67025447e-15,  3.66913792e-15, -2.89532242e-15, -9.17835297e-15,\n",
       "        1.54239632e-15,  1.38399516e-15, -5.52624360e-15, -2.37396060e-16,\n",
       "       -3.28764793e-15,  2.35896174e-15, -3.57931654e-15, -1.40629229e-15,\n",
       "        1.72689857e-15, -3.88426623e-16,  5.11177239e-16, -4.47040217e-16,\n",
       "        3.23242157e-15,  4.33955712e-15, -1.87884453e-15,  1.00113548e-15,\n",
       "        4.32583755e-15, -7.97000902e-17,  1.03166209e-15, -1.08300312e-15,\n",
       "        1.39759268e-15, -1.57161200e-15, -3.07878949e-16, -1.57161200e-15,\n",
       "        1.39759268e-15, -1.08300312e-15,  1.03166209e-15, -7.97000902e-17,\n",
       "        4.32583755e-15,  1.00113548e-15, -1.87884453e-15,  4.33955712e-15,\n",
       "        3.23242157e-15, -4.47040217e-16,  5.11177239e-16, -3.88426623e-16])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "65b59126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.30960533e-03, -4.30960533e-03, -1.27448003e-02,  1.51637296e-02,\n",
       "       -7.31201255e-03, -1.07725574e-02, -9.60707174e-03, -1.13731242e-02,\n",
       "       -4.26524144e-03, -4.10398843e-03, -1.05209144e-02, -1.16482641e-02,\n",
       "       -8.39661147e-03, -9.22285350e-03, -2.23799034e-02, -1.45045427e-02,\n",
       "        1.21200481e-02, -1.04699546e-02, -9.60281236e-03, -1.48386588e-02,\n",
       "       -1.07118956e-02, -1.12877531e-02, -1.50093771e-02, -9.97453521e-03,\n",
       "       -8.32076348e-03, -1.19983399e-02, -8.32280337e-03, -1.44709320e-02,\n",
       "        1.46963853e-02,  4.62257456e-04, -1.23439304e-02, -1.08278686e-02,\n",
       "        5.09971237e-04, -1.57961928e-02,  2.80223352e-02, -1.81294651e-03,\n",
       "       -1.18381888e-02, -1.21528148e-02, -1.17554037e-02, -1.57028921e-02,\n",
       "       -1.97276395e-02, -2.04161153e-02, -2.80162313e-02, -2.09194462e-02,\n",
       "        8.64505555e-03, -1.59181720e-02, -1.25563527e-02, -2.03507592e-02,\n",
       "       -1.81027113e-02, -1.96928469e-02, -1.84992339e-02, -1.80151075e-02,\n",
       "       -2.57879847e-02, -2.88602566e-02, -2.44142760e-02, -3.38884874e-02,\n",
       "       -1.94373885e-03, -6.70189599e-03, -2.46043653e-02, -2.86859140e-02,\n",
       "       -1.67841135e-02, -3.25001751e-02,  1.55721444e-02, -4.77246782e-03,\n",
       "       -1.10532957e-02, -1.33761917e-02, -1.24874551e-02, -1.60888017e-02,\n",
       "       -1.79960101e-02, -2.03657186e-02, -3.07628999e-02, -2.36387533e-02,\n",
       "        9.01757357e-03, -1.79130103e-02, -1.35356145e-02, -1.85697452e-02,\n",
       "       -1.88785210e-02, -1.95655259e-02, -2.04880357e-02, -2.19620663e-02,\n",
       "       -2.43919237e-02, -2.70564838e-02, -2.16036506e-02, -3.00444867e-02,\n",
       "        1.50332544e-03, -6.97528244e-03, -2.45111535e-02, -2.72164019e-02,\n",
       "       -1.39873228e-02, -2.82619525e-02,  1.37229630e-02, -5.52293173e-03,\n",
       "       -1.78045378e-02, -1.22361831e-02, -1.08698335e-02, -2.12278328e-02,\n",
       "       -2.32499604e-02, -2.71554970e-02, -3.39557568e-02, -2.43443872e-02,\n",
       "       -1.09762445e-02, -1.54063388e-02, -1.44521759e-02, -1.96194597e-02,\n",
       "       -2.25340365e-02, -3.33053912e-02, -3.54449374e-02, -2.58407709e-02,\n",
       "       -2.89783244e-02, -1.94171923e-02,  4.54282770e-03, -1.55705567e-02,\n",
       "       -9.53432826e-03, -1.52955826e-03, -2.44121709e-02, -1.41202006e-02,\n",
       "       -4.87634933e-03, -1.00110354e-02, -5.18772852e-03, -4.36789164e-03,\n",
       "        2.51986036e-02,  1.02313782e-02, -1.17756357e-03, -5.47588691e-03,\n",
       "        3.55367804e-03,  1.36374243e-02,  1.76186721e-02,  7.39039846e-03,\n",
       "        1.83831144e-02,  5.45746026e-03,  9.91294397e-03, -5.84262183e-03,\n",
       "        1.85312045e-03,  1.19516322e-02,  2.10382982e-03, -1.14428841e-03,\n",
       "        8.90516785e-03,  1.34475379e-02, -1.49540590e-02, -1.57819880e-02,\n",
       "        5.90928918e-03, -4.44817443e-03,  2.27950949e-02,  1.34507543e-02,\n",
       "       -5.55771065e-03, -2.43564795e-02,  4.39332252e-03,  1.66887450e-02,\n",
       "       -1.78292094e-02, -1.27254654e-02, -1.09449357e-02, -7.47740378e-03,\n",
       "       -1.53149030e-02, -1.57036952e-02, -2.30465714e-02, -2.01963637e-02,\n",
       "       -2.13322882e-02, -1.25196000e-02, -7.16847400e-03, -2.93102751e-03,\n",
       "       -2.05142263e-02, -2.15053876e-02, -3.07133906e-02, -2.44211752e-02,\n",
       "       -2.86248752e-02, -2.34787850e-02, -1.91581219e-02, -2.18830852e-02,\n",
       "       -2.66896809e-02, -2.38408802e-02, -2.98007399e-02, -2.37011998e-02,\n",
       "       -1.66573285e-02, -1.42008425e-02, -2.01538146e-02, -2.55050388e-02,\n",
       "       -9.02440340e-03, -6.33923271e-03, -9.02440340e-03,  1.19674770e-02,\n",
       "       -9.02440340e-03, -1.66678886e-02, -4.74965931e-02, -2.93600974e-02,\n",
       "        9.02440340e-03,  1.38528030e-02,  9.22503410e-03, -6.55593856e-03,\n",
       "       -5.32828227e-03, -3.57293106e-04, -1.34137052e-02,  3.64354853e-02,\n",
       "       -2.49892123e-02, -3.56156439e-02, -5.47851646e-02, -2.57383642e-02,\n",
       "        1.53687676e-02,  6.46165945e-03,  3.49073037e-03,  7.66625271e-03,\n",
       "       -4.53941090e+09, -1.37315796e+09,  1.02854623e-02,  1.69313804e-02,\n",
       "       -9.02440340e-03, -8.61222259e-04, -9.02440340e-03,  1.55670357e-02,\n",
       "       -9.02440340e-03, -1.33986292e-02, -4.83663050e-02, -3.74285503e-02,\n",
       "        9.02440340e-03,  1.14904048e-02,  1.16865096e-02, -4.93346230e-03,\n",
       "       -5.10609528e-03,  5.15679323e-03, -1.29204724e-02,  4.09180959e-02,\n",
       "       -2.26329965e-02, -2.37435589e-02, -4.70921034e-02, -2.69975105e-02,\n",
       "        1.51235841e-02,  6.01309156e-03,  2.09835708e-03,  7.75385362e-03,\n",
       "       -3.03699480e+09, -1.41279560e+09,  1.10400178e-02,  1.94382465e-02,\n",
       "       -1.31417085e-02,  5.70786820e-03, -3.12010291e-03, -1.50463902e-02,\n",
       "       -1.86833814e-02, -1.48033015e-02, -1.09325339e-02, -6.54466008e-03,\n",
       "       -7.18285600e-03, -3.68033771e-03, -1.18157970e-02, -2.74144961e-02,\n",
       "       -1.86465772e-02, -1.98482406e-02, -5.09337542e-03,  1.33265590e-03,\n",
       "       -4.88173358e-03, -1.87319938e-03,  9.44846856e-03, -6.47644001e-03,\n",
       "        9.06895926e-03,  4.03156182e-03, -3.81071635e-03, -1.55049012e-02,\n",
       "       -8.94979470e-03, -1.25916630e-03,  5.82991153e-03, -2.71836229e-02,\n",
       "        1.51604747e-02,  1.59673434e-02,  8.65976339e-03,  1.32855836e-02,\n",
       "        1.62871026e-02,  1.95881405e-02,  2.74573526e-02,  2.03413639e-02,\n",
       "       -1.88967309e-03,  1.69679440e-02,  1.24976474e-02,  1.78640923e-02,\n",
       "        1.58698735e-02,  1.86328201e-02,  1.62623231e-02,  1.68709607e-02,\n",
       "        2.44031374e-02,  2.45614623e-02,  1.87058528e-02,  2.91551698e-02,\n",
       "        3.27444689e-03,  5.52841895e-03,  2.63567778e-02,  2.53201372e-02,\n",
       "        1.32523082e-02,  2.69258554e-02, -8.58818134e-03,  1.11032909e-02,\n",
       "       -5.33048384e-03, -9.51684989e-03, -1.66080150e-02, -1.90150739e-02,\n",
       "       -1.90998495e-02, -2.07365330e-02, -3.42149633e-02, -2.70416691e-02,\n",
       "        1.68546429e-02, -1.80331795e-02, -1.41547058e-02, -1.86456297e-02,\n",
       "       -2.18176818e-02, -2.01658644e-02, -2.54506325e-02, -2.76510291e-02,\n",
       "       -2.29869110e-02, -2.82251571e-02, -2.36957273e-02, -2.98757919e-02,\n",
       "        5.66008679e-03, -8.08755600e-03, -2.16642057e-02, -2.76979603e-02,\n",
       "       -1.42339448e-02, -2.82828297e-02,  1.75105537e-02, -2.06170531e-04,\n",
       "        1.03557949e-03, -6.55491506e-05, -4.05251005e-03, -6.50374606e-03,\n",
       "       -4.88531500e-03, -8.16227060e-03, -1.75775149e-02, -8.89951174e-03,\n",
       "        9.93217338e-03, -4.52070708e-03, -2.82360603e-03, -1.10777861e-02,\n",
       "       -5.83879348e-03, -8.16155982e-03, -8.50182469e-03, -6.05696380e-03,\n",
       "       -8.30887806e-03, -1.16323018e-02, -9.99659192e-03, -1.93991066e-02,\n",
       "        9.03849481e-03,  5.42167599e-03, -6.29390066e-03, -8.94580261e-03,\n",
       "        4.13936295e-03, -1.23961956e-02,  2.56599079e-02,  7.80769795e-03,\n",
       "        1.03553706e-03, -6.56650425e-05, -4.05263130e-03, -6.50376609e-03,\n",
       "       -4.88536885e-03, -8.16230989e-03, -1.75775529e-02, -8.89954437e-03,\n",
       "        9.93213981e-03, -4.52082829e-03, -2.82370872e-03, -1.10778739e-02,\n",
       "       -5.83884754e-03, -8.16161611e-03, -8.50184951e-03, -6.05699351e-03,\n",
       "       -8.30885456e-03, -1.16322827e-02, -9.99659912e-03, -1.93991293e-02,\n",
       "        9.03848991e-03,  5.42166691e-03, -6.29385546e-03, -8.94577504e-03,\n",
       "        4.13937161e-03, -1.23961834e-02,  2.56599295e-02,  7.80780364e-03,\n",
       "       -2.85269815e-02, -2.23995256e-02, -2.31258808e-02, -2.79235466e-02,\n",
       "       -1.96979512e-02,  6.31120204e-03, -1.87345876e-02, -1.86375545e-02,\n",
       "       -1.25300608e-02,  1.21444304e-02,  3.50793236e-03, -9.72372156e-03,\n",
       "       -6.56699032e-03,  1.88329099e-03,  1.49282609e-02,  1.88329099e-03,\n",
       "       -6.56699032e-03, -9.72372156e-03,  3.50793236e-03,  1.21444304e-02,\n",
       "       -1.25300608e-02, -1.86375545e-02, -1.87345876e-02,  6.31120204e-03,\n",
       "       -1.96979512e-02, -2.79235466e-02, -2.31258808e-02, -2.23995256e-02,\n",
       "       -1.10106074e-02, -1.28167870e-02, -1.40307095e-02, -2.11175171e-02,\n",
       "       -1.88912260e-02,  4.77760507e-03, -8.00567504e-03, -2.44581036e-02,\n",
       "       -1.83461284e-02,  1.26304870e-02, -1.95977904e-04, -1.67626275e-02,\n",
       "       -1.25353497e-02, -4.01266334e-03, -3.62373176e-04, -4.01266334e-03,\n",
       "       -1.25353497e-02, -1.67626275e-02, -1.95977904e-04,  1.26304870e-02,\n",
       "       -1.83461284e-02, -2.44581036e-02, -8.00567504e-03,  4.77760507e-03,\n",
       "       -1.88912260e-02, -2.11175171e-02, -1.40307095e-02, -1.28167870e-02,\n",
       "       -1.10106074e-02, -1.28167871e-02, -1.40307103e-02, -2.11175169e-02,\n",
       "       -1.88912261e-02,  4.77760596e-03, -8.00567523e-03, -2.44581029e-02,\n",
       "       -1.83461279e-02,  1.26304860e-02, -1.95978015e-04, -1.67626272e-02,\n",
       "       -1.25353496e-02, -4.01266070e-03, -3.62374042e-04, -4.01266070e-03,\n",
       "       -1.25353496e-02, -1.67626272e-02, -1.95978015e-04,  1.26304860e-02,\n",
       "       -1.83461279e-02, -2.44581029e-02, -8.00567523e-03,  4.77760596e-03,\n",
       "       -1.88912261e-02, -2.11175169e-02, -1.40307103e-02, -1.28167871e-02,\n",
       "       -1.58742494e-02, -1.29375109e-02, -1.36992857e-02, -2.14067339e-02,\n",
       "       -1.96668769e-02,  3.43075570e-03, -1.43624591e-02,  6.62463875e-03,\n",
       "       -6.82398011e-03,  3.58648413e-03,  1.14415456e-02,  6.03924387e-03,\n",
       "       -7.16225389e-05, -1.22495432e-04,  1.76128205e-02, -1.22495432e-04,\n",
       "       -7.16225389e-05,  6.03924387e-03,  1.14415456e-02,  3.58648413e-03,\n",
       "       -6.82398011e-03,  6.62463875e-03, -1.43624591e-02,  3.43075570e-03,\n",
       "       -1.96668769e-02, -2.14067339e-02, -1.36992857e-02, -1.29375109e-02,\n",
       "       -2.85269815e-02, -2.23995256e-02, -2.31258808e-02, -2.79235466e-02,\n",
       "       -1.96979512e-02,  6.31120204e-03, -1.87345876e-02, -1.86375545e-02,\n",
       "       -1.25300608e-02,  1.21444304e-02,  3.50793236e-03, -9.72372156e-03,\n",
       "       -6.56699032e-03,  1.88329099e-03,  1.49282609e-02,  1.88329099e-03,\n",
       "       -6.56699032e-03, -9.72372156e-03,  3.50793236e-03,  1.21444304e-02,\n",
       "       -1.25300608e-02, -1.86375545e-02, -1.87345876e-02,  6.31120204e-03,\n",
       "       -1.96979512e-02, -2.79235466e-02, -2.31258808e-02, -2.23995256e-02,\n",
       "       -7.91573496e-03, -2.41367843e-02, -1.87778107e-02, -3.74292328e-02,\n",
       "       -2.72855390e-02,  7.65349359e-04, -3.19055048e-02, -2.65182456e-02,\n",
       "       -2.48665640e-02,  1.00017959e-02, -1.88926866e-02, -2.86243525e-02,\n",
       "       -3.05229705e-03, -1.03522271e-02,  2.26776688e-03, -1.03522271e-02,\n",
       "       -3.05229705e-03, -2.86243525e-02, -1.88926866e-02,  1.00017959e-02,\n",
       "       -2.48665640e-02, -2.65182456e-02, -3.19055048e-02,  7.65349359e-04,\n",
       "       -2.72855390e-02, -3.74292328e-02, -1.87778107e-02, -2.41367843e-02,\n",
       "        7.86585470e-04, -2.61967284e-02, -2.27436137e-02, -3.92914482e-02,\n",
       "       -2.50821015e-02, -4.99853284e-03, -3.17932491e-02, -3.11936792e-02,\n",
       "       -2.67584620e-02, -1.21326532e-04, -2.54633368e-02, -2.66081047e-02,\n",
       "       -3.01070111e-03, -1.80620980e-02,  2.38511543e-03, -1.80620980e-02,\n",
       "       -3.01070111e-03, -2.66081047e-02, -2.54633368e-02, -1.21326532e-04,\n",
       "       -2.67584620e-02, -3.11936792e-02, -3.17932491e-02, -4.99853284e-03,\n",
       "       -2.50821015e-02, -3.92914482e-02, -2.27436137e-02, -2.61967284e-02])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d47d9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.tolist())\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e439479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db4c22e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.69      0.71       159\n",
      "           1       0.65      0.68      0.67       165\n",
      "           2       0.78      0.77      0.78       180\n",
      "           3       0.71      0.70      0.71       204\n",
      "           4       0.83      0.82      0.82       186\n",
      "           5       0.86      0.93      0.90       180\n",
      "           6       0.75      0.71      0.73       184\n",
      "           7       0.83      0.85      0.84       170\n",
      "           8       0.78      0.74      0.76       192\n",
      "           9       0.85      0.81      0.83       207\n",
      "          10       0.96      0.96      0.96       167\n",
      "          11       0.92      0.92      0.92       191\n",
      "          12       0.85      0.89      0.87       149\n",
      "          13       0.91      0.92      0.91       192\n",
      "          14       0.80      0.89      0.84       177\n",
      "          15       0.78      0.72      0.75       191\n",
      "          16       0.93      0.96      0.94       177\n",
      "\n",
      "    accuracy                           0.82      3071\n",
      "   macro avg       0.82      0.82      0.82      3071\n",
      "weighted avg       0.82      0.82      0.82      3071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7f6cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_micro = precision_score(y_test, y_test_pred, average = \"micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fc2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2046bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f0edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2c8f28b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 40, 41, 42, 43]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceeda61",
   "metadata": {},
   "source": [
    "## Split on participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62da2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = list(range(5,43))\n",
    "test_participants = list(range(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94a655c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data['participant'].isin(participants)]\n",
    "test_df = data[data['participant'].isin(test_participants)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "201da092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>participant</th>\n",
       "      <th>gesture</th>\n",
       "      <th>index</th>\n",
       "      <th>iemg</th>\n",
       "      <th>mav</th>\n",
       "      <th>ssi</th>\n",
       "      <th>myopulse</th>\n",
       "      <th>wflen</th>\n",
       "      <th>diffvar</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis_f_w3</th>\n",
       "      <th>kurtosis_f_w4</th>\n",
       "      <th>kurtosis_f_w5</th>\n",
       "      <th>kurtosis_f_w6</th>\n",
       "      <th>kurtosis_f_w7</th>\n",
       "      <th>kurtosis_f_w8</th>\n",
       "      <th>kurtosis_f_w9</th>\n",
       "      <th>kurtosis_f_w10</th>\n",
       "      <th>kurtosis_f_w11</th>\n",
       "      <th>kurtosis_f_w12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6797.436339</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>313.982240</td>\n",
       "      <td>7.401855</td>\n",
       "      <td>5733.301325</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>...</td>\n",
       "      <td>79.115045</td>\n",
       "      <td>32.591676</td>\n",
       "      <td>58.994440</td>\n",
       "      <td>54.839028</td>\n",
       "      <td>50.292229</td>\n",
       "      <td>104.307591</td>\n",
       "      <td>61.155945</td>\n",
       "      <td>196.373417</td>\n",
       "      <td>92.713179</td>\n",
       "      <td>405.962532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>14079.062764</td>\n",
       "      <td>0.049104</td>\n",
       "      <td>1291.830907</td>\n",
       "      <td>6.844043</td>\n",
       "      <td>10385.642760</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>...</td>\n",
       "      <td>12.381097</td>\n",
       "      <td>12.099599</td>\n",
       "      <td>10.995862</td>\n",
       "      <td>17.486544</td>\n",
       "      <td>14.211067</td>\n",
       "      <td>14.489695</td>\n",
       "      <td>16.576432</td>\n",
       "      <td>160.712336</td>\n",
       "      <td>79.746286</td>\n",
       "      <td>170.230168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11472.717905</td>\n",
       "      <td>0.040014</td>\n",
       "      <td>960.778273</td>\n",
       "      <td>6.713477</td>\n",
       "      <td>8706.221451</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>...</td>\n",
       "      <td>48.730862</td>\n",
       "      <td>305.459894</td>\n",
       "      <td>212.763290</td>\n",
       "      <td>36.980016</td>\n",
       "      <td>29.056499</td>\n",
       "      <td>299.450092</td>\n",
       "      <td>55.901194</td>\n",
       "      <td>64.921149</td>\n",
       "      <td>59.408569</td>\n",
       "      <td>167.539726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>17507.830172</td>\n",
       "      <td>0.061062</td>\n",
       "      <td>2332.784349</td>\n",
       "      <td>6.788477</td>\n",
       "      <td>13368.566593</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>...</td>\n",
       "      <td>51.993926</td>\n",
       "      <td>51.528948</td>\n",
       "      <td>20.339587</td>\n",
       "      <td>43.117535</td>\n",
       "      <td>34.757809</td>\n",
       "      <td>59.816264</td>\n",
       "      <td>20.777295</td>\n",
       "      <td>59.169183</td>\n",
       "      <td>45.939629</td>\n",
       "      <td>43.873509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>18614.528512</td>\n",
       "      <td>0.064922</td>\n",
       "      <td>2580.763916</td>\n",
       "      <td>6.342676</td>\n",
       "      <td>13226.563635</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>...</td>\n",
       "      <td>306.354173</td>\n",
       "      <td>55.281935</td>\n",
       "      <td>15.773219</td>\n",
       "      <td>26.464685</td>\n",
       "      <td>84.562723</td>\n",
       "      <td>76.696436</td>\n",
       "      <td>19.323875</td>\n",
       "      <td>37.940924</td>\n",
       "      <td>24.052192</td>\n",
       "      <td>41.660051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15346</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7435.557234</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>369.818221</td>\n",
       "      <td>6.280762</td>\n",
       "      <td>5792.222983</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>...</td>\n",
       "      <td>29.589425</td>\n",
       "      <td>25.460694</td>\n",
       "      <td>182.962449</td>\n",
       "      <td>34.957161</td>\n",
       "      <td>18.846525</td>\n",
       "      <td>37.714119</td>\n",
       "      <td>26.569683</td>\n",
       "      <td>32.614845</td>\n",
       "      <td>12.793219</td>\n",
       "      <td>16.657922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15347</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10471.625678</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>713.052200</td>\n",
       "      <td>6.967187</td>\n",
       "      <td>8628.380136</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>...</td>\n",
       "      <td>18.226247</td>\n",
       "      <td>35.715660</td>\n",
       "      <td>113.822913</td>\n",
       "      <td>64.223311</td>\n",
       "      <td>8.214120</td>\n",
       "      <td>39.211218</td>\n",
       "      <td>29.910432</td>\n",
       "      <td>26.426457</td>\n",
       "      <td>24.892159</td>\n",
       "      <td>82.241098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15348</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>11279.138159</td>\n",
       "      <td>0.039339</td>\n",
       "      <td>815.440185</td>\n",
       "      <td>6.917773</td>\n",
       "      <td>9229.795138</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>...</td>\n",
       "      <td>15.486525</td>\n",
       "      <td>29.417342</td>\n",
       "      <td>25.153035</td>\n",
       "      <td>19.632020</td>\n",
       "      <td>15.590478</td>\n",
       "      <td>15.462391</td>\n",
       "      <td>34.940941</td>\n",
       "      <td>37.564280</td>\n",
       "      <td>14.208021</td>\n",
       "      <td>15.811267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15349</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9210.946156</td>\n",
       "      <td>0.032125</td>\n",
       "      <td>563.422838</td>\n",
       "      <td>7.359570</td>\n",
       "      <td>8040.357766</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>...</td>\n",
       "      <td>14.527208</td>\n",
       "      <td>20.281059</td>\n",
       "      <td>20.418717</td>\n",
       "      <td>18.471741</td>\n",
       "      <td>14.276295</td>\n",
       "      <td>10.083487</td>\n",
       "      <td>27.337412</td>\n",
       "      <td>12.804675</td>\n",
       "      <td>20.220452</td>\n",
       "      <td>37.494222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15350</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9557.645293</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>622.632313</td>\n",
       "      <td>7.815820</td>\n",
       "      <td>8616.136489</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>...</td>\n",
       "      <td>28.617196</td>\n",
       "      <td>17.396729</td>\n",
       "      <td>21.821603</td>\n",
       "      <td>27.706896</td>\n",
       "      <td>16.773846</td>\n",
       "      <td>23.562969</td>\n",
       "      <td>20.576495</td>\n",
       "      <td>21.367926</td>\n",
       "      <td>12.371417</td>\n",
       "      <td>39.203146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13566 rows Ã— 572 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       session  participant  gesture  index          iemg       mav  \\\n",
       "119          1           10       10      0   6797.436339  0.023708   \n",
       "120          1           10       10      0  14079.062764  0.049104   \n",
       "121          1           10       10      0  11472.717905  0.040014   \n",
       "122          1           10       10      0  17507.830172  0.061062   \n",
       "123          1           10       10      0  18614.528512  0.064922   \n",
       "...        ...          ...      ...    ...           ...       ...   \n",
       "15346        3            9        9      0   7435.557234  0.025933   \n",
       "15347        3            9        9      0  10471.625678  0.036522   \n",
       "15348        3            9        9      0  11279.138159  0.039339   \n",
       "15349        3            9        9      0   9210.946156  0.032125   \n",
       "15350        3            9        9      0   9557.645293  0.033334   \n",
       "\n",
       "               ssi  myopulse         wflen   diffvar  ...  kurtosis_f_w3  \\\n",
       "119     313.982240  7.401855   5733.301325  0.000904  ...      79.115045   \n",
       "120    1291.830907  6.844043  10385.642760  0.002613  ...      12.381097   \n",
       "121     960.778273  6.713477   8706.221451  0.002182  ...      48.730862   \n",
       "122    2332.784349  6.788477  13368.566593  0.005666  ...      51.993926   \n",
       "123    2580.763916  6.342676  13226.563635  0.005325  ...     306.354173   \n",
       "...            ...       ...           ...       ...  ...            ...   \n",
       "15346   369.818221  6.280762   5792.222983  0.001070  ...      29.589425   \n",
       "15347   713.052200  6.967187   8628.380136  0.002070  ...      18.226247   \n",
       "15348   815.440185  6.917773   9229.795138  0.002355  ...      15.486525   \n",
       "15349   563.422838  7.359570   8040.357766  0.001839  ...      14.527208   \n",
       "15350   622.632313  7.815820   8616.136489  0.002036  ...      28.617196   \n",
       "\n",
       "       kurtosis_f_w4  kurtosis_f_w5  kurtosis_f_w6  kurtosis_f_w7  \\\n",
       "119        32.591676      58.994440      54.839028      50.292229   \n",
       "120        12.099599      10.995862      17.486544      14.211067   \n",
       "121       305.459894     212.763290      36.980016      29.056499   \n",
       "122        51.528948      20.339587      43.117535      34.757809   \n",
       "123        55.281935      15.773219      26.464685      84.562723   \n",
       "...              ...            ...            ...            ...   \n",
       "15346      25.460694     182.962449      34.957161      18.846525   \n",
       "15347      35.715660     113.822913      64.223311       8.214120   \n",
       "15348      29.417342      25.153035      19.632020      15.590478   \n",
       "15349      20.281059      20.418717      18.471741      14.276295   \n",
       "15350      17.396729      21.821603      27.706896      16.773846   \n",
       "\n",
       "       kurtosis_f_w8  kurtosis_f_w9  kurtosis_f_w10  kurtosis_f_w11  \\\n",
       "119       104.307591      61.155945      196.373417       92.713179   \n",
       "120        14.489695      16.576432      160.712336       79.746286   \n",
       "121       299.450092      55.901194       64.921149       59.408569   \n",
       "122        59.816264      20.777295       59.169183       45.939629   \n",
       "123        76.696436      19.323875       37.940924       24.052192   \n",
       "...              ...            ...             ...             ...   \n",
       "15346      37.714119      26.569683       32.614845       12.793219   \n",
       "15347      39.211218      29.910432       26.426457       24.892159   \n",
       "15348      15.462391      34.940941       37.564280       14.208021   \n",
       "15349      10.083487      27.337412       12.804675       20.220452   \n",
       "15350      23.562969      20.576495       21.367926       12.371417   \n",
       "\n",
       "       kurtosis_f_w12  \n",
       "119        405.962532  \n",
       "120        170.230168  \n",
       "121        167.539726  \n",
       "122         43.873509  \n",
       "123         41.660051  \n",
       "...               ...  \n",
       "15346       16.657922  \n",
       "15347       82.241098  \n",
       "15348       15.811267  \n",
       "15349       37.494222  \n",
       "15350       39.203146  \n",
       "\n",
       "[13566 rows x 572 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3080f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.iloc[:, 4:].values\n",
    "y_train = (train_df.iloc[:, 2] - 1).values\n",
    "\n",
    "x_test = test_df.iloc[:, 4:].values\n",
    "y_test = (test_df.iloc[:, 2] - 1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0cd65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb3078c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "374e281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "663e5674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.71151404, -0.71151404, -0.44104706, ...,  1.18273398,\n",
       "         0.3947441 ,  2.51393605],\n",
       "       [ 0.3502168 ,  0.3502168 , -0.06034413, ...,  0.86820923,\n",
       "         0.26368513,  0.75742463],\n",
       "       [-0.02981323, -0.02981323, -0.18923188, ...,  0.02334703,\n",
       "         0.05812778,  0.73737735],\n",
       "       ...,\n",
       "       [-0.05803901, -0.05803901, -0.24581592, ..., -0.21793596,\n",
       "        -0.39872317, -0.39319618],\n",
       "       [-0.35960121, -0.35960121, -0.34393309, ..., -0.43631153,\n",
       "        -0.33795433, -0.23163008],\n",
       "       [-0.30904915, -0.30904915, -0.32088124, ..., -0.36078509,\n",
       "        -0.41728609, -0.21889639]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aad3c3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1428, 568)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07655ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13566, 568)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57056f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, output_shape):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_shape, 32),  # Input layer: Fully connected (linear) with 64 units\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(64, output_shape)  # Output layer: Fully connected (linear) with 'output_shape' units\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195513cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0389d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(x_train).type(torch.float32), torch.tensor(y_train).type(torch.LongTensor))\n",
    "test_dataset = TensorDataset(torch.tensor(x_test).type(torch.float32), torch.tensor(y_test).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc5c89db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e97bb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and whether to shuffle the data\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8b9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6b93e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model(x_train.shape[1], len(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583af4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4643bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6374bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47504fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "961c2628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0466,  0.0466, -0.1751,  ..., -0.0469, -0.1113,  0.3751],\n",
       "         [-0.1643, -0.1643, -0.1663,  ...,  1.1653,  2.4379,  3.4931],\n",
       "         [-0.3568, -0.3568, -0.3180,  ...,  0.7841,  1.0431,  0.3526],\n",
       "         ...,\n",
       "         [-0.3627, -0.3627, -0.2277,  ...,  0.1066,  0.2954,  0.2957],\n",
       "         [ 1.9771,  1.9771,  1.6013,  ..., -0.2454, -0.2063, -0.1831],\n",
       "         [ 2.0838,  2.0838,  2.2605,  ...,  0.4058,  0.3251,  0.2316]]),\n",
       " tensor([ 9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11,\n",
       "         11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 14,\n",
       "         14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16,\n",
       "         16, 16,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  2,  2,\n",
       "          2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,\n",
       "          4,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  7,  7,  7,\n",
       "          7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,\n",
       "         10, 10])]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcc87418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Train Loss: 0.0609, Val Loss: 5.1502, Val Accuracy: 60.15%\n",
      "Epoch [2/100] - Train Loss: 0.0669, Val Loss: 5.3280, Val Accuracy: 59.38%\n",
      "Epoch [3/100] - Train Loss: 0.0637, Val Loss: 5.3819, Val Accuracy: 59.45%\n",
      "Epoch [4/100] - Train Loss: 0.0502, Val Loss: 5.4860, Val Accuracy: 60.22%\n",
      "Epoch [5/100] - Train Loss: 0.0521, Val Loss: 5.4284, Val Accuracy: 59.73%\n",
      "Epoch [6/100] - Train Loss: 0.0549, Val Loss: 5.4382, Val Accuracy: 60.50%\n",
      "Epoch [7/100] - Train Loss: 0.0659, Val Loss: 5.4062, Val Accuracy: 59.52%\n",
      "Epoch [8/100] - Train Loss: 0.0739, Val Loss: 5.7117, Val Accuracy: 58.96%\n",
      "Epoch [9/100] - Train Loss: 0.0774, Val Loss: 5.5320, Val Accuracy: 60.29%\n",
      "Epoch [10/100] - Train Loss: 0.0575, Val Loss: 5.4719, Val Accuracy: 60.85%\n",
      "Epoch [11/100] - Train Loss: 0.0552, Val Loss: 5.5890, Val Accuracy: 60.50%\n",
      "Epoch [12/100] - Train Loss: 0.0455, Val Loss: 5.6710, Val Accuracy: 59.80%\n",
      "Epoch [13/100] - Train Loss: 0.0605, Val Loss: 5.6944, Val Accuracy: 60.22%\n",
      "Epoch [14/100] - Train Loss: 0.0566, Val Loss: 5.8046, Val Accuracy: 59.80%\n",
      "Epoch [15/100] - Train Loss: 0.0495, Val Loss: 5.7927, Val Accuracy: 59.73%\n",
      "Epoch [16/100] - Train Loss: 0.0500, Val Loss: 5.8092, Val Accuracy: 61.13%\n",
      "Epoch [17/100] - Train Loss: 0.0471, Val Loss: 5.8874, Val Accuracy: 59.73%\n",
      "Epoch [18/100] - Train Loss: 0.0388, Val Loss: 5.8886, Val Accuracy: 59.73%\n",
      "Epoch [19/100] - Train Loss: 0.0457, Val Loss: 5.9252, Val Accuracy: 60.57%\n",
      "Epoch [20/100] - Train Loss: 0.0388, Val Loss: 5.8875, Val Accuracy: 59.94%\n",
      "Epoch [21/100] - Train Loss: 0.0387, Val Loss: 6.0299, Val Accuracy: 59.45%\n",
      "Epoch [22/100] - Train Loss: 0.0386, Val Loss: 6.1243, Val Accuracy: 59.10%\n",
      "Epoch [23/100] - Train Loss: 0.0363, Val Loss: 6.1158, Val Accuracy: 59.03%\n",
      "Epoch [24/100] - Train Loss: 0.0421, Val Loss: 6.3677, Val Accuracy: 58.68%\n",
      "Epoch [25/100] - Train Loss: 0.0394, Val Loss: 6.0958, Val Accuracy: 60.99%\n",
      "Epoch [26/100] - Train Loss: 0.0376, Val Loss: 6.3512, Val Accuracy: 60.15%\n",
      "Epoch [27/100] - Train Loss: 0.0373, Val Loss: 6.2391, Val Accuracy: 59.87%\n",
      "Epoch [28/100] - Train Loss: 0.0697, Val Loss: 6.4679, Val Accuracy: 59.45%\n",
      "Epoch [29/100] - Train Loss: 0.0935, Val Loss: 6.4904, Val Accuracy: 59.87%\n",
      "Epoch [30/100] - Train Loss: 0.0726, Val Loss: 6.3761, Val Accuracy: 60.50%\n",
      "Epoch [31/100] - Train Loss: 0.0720, Val Loss: 6.6175, Val Accuracy: 58.47%\n",
      "Epoch [32/100] - Train Loss: 0.0387, Val Loss: 6.4957, Val Accuracy: 59.24%\n",
      "Epoch [33/100] - Train Loss: 0.0338, Val Loss: 6.4860, Val Accuracy: 60.22%\n",
      "Epoch [34/100] - Train Loss: 0.0337, Val Loss: 6.5440, Val Accuracy: 60.15%\n",
      "Epoch [35/100] - Train Loss: 0.0449, Val Loss: 6.7397, Val Accuracy: 59.24%\n",
      "Epoch [36/100] - Train Loss: 0.0297, Val Loss: 6.6461, Val Accuracy: 59.24%\n",
      "Epoch [37/100] - Train Loss: 0.0247, Val Loss: 6.6989, Val Accuracy: 59.17%\n",
      "Epoch [38/100] - Train Loss: 0.0276, Val Loss: 6.7075, Val Accuracy: 60.36%\n",
      "Epoch [39/100] - Train Loss: 0.0344, Val Loss: 6.9181, Val Accuracy: 58.75%\n",
      "Epoch [40/100] - Train Loss: 0.0207, Val Loss: 6.8054, Val Accuracy: 60.15%\n",
      "Epoch [41/100] - Train Loss: 0.0211, Val Loss: 6.8089, Val Accuracy: 59.80%\n",
      "Epoch [42/100] - Train Loss: 0.0211, Val Loss: 6.8758, Val Accuracy: 59.66%\n",
      "Epoch [43/100] - Train Loss: 0.0249, Val Loss: 6.8116, Val Accuracy: 59.73%\n",
      "Epoch [44/100] - Train Loss: 0.0349, Val Loss: 7.0923, Val Accuracy: 58.40%\n",
      "Epoch [45/100] - Train Loss: 0.0728, Val Loss: 6.7456, Val Accuracy: 58.40%\n",
      "Epoch [46/100] - Train Loss: 0.0781, Val Loss: 6.7362, Val Accuracy: 59.10%\n",
      "Epoch [47/100] - Train Loss: 0.0526, Val Loss: 6.8889, Val Accuracy: 59.73%\n",
      "Epoch [48/100] - Train Loss: 0.0331, Val Loss: 6.9564, Val Accuracy: 59.59%\n",
      "Epoch [49/100] - Train Loss: 0.0255, Val Loss: 7.0278, Val Accuracy: 59.17%\n",
      "Epoch [50/100] - Train Loss: 0.0231, Val Loss: 6.9296, Val Accuracy: 60.57%\n",
      "Epoch [51/100] - Train Loss: 0.0198, Val Loss: 7.0425, Val Accuracy: 59.80%\n",
      "Epoch [52/100] - Train Loss: 0.0193, Val Loss: 7.1717, Val Accuracy: 59.45%\n",
      "Epoch [53/100] - Train Loss: 0.0183, Val Loss: 7.1641, Val Accuracy: 60.08%\n",
      "Epoch [54/100] - Train Loss: 0.0163, Val Loss: 7.2673, Val Accuracy: 59.17%\n",
      "Epoch [55/100] - Train Loss: 0.0155, Val Loss: 7.2933, Val Accuracy: 60.01%\n",
      "Epoch [56/100] - Train Loss: 0.0173, Val Loss: 7.2168, Val Accuracy: 59.66%\n",
      "Epoch [57/100] - Train Loss: 0.0236, Val Loss: 7.3621, Val Accuracy: 59.73%\n",
      "Epoch [58/100] - Train Loss: 0.0434, Val Loss: 7.2636, Val Accuracy: 59.03%\n",
      "Epoch [59/100] - Train Loss: 0.0417, Val Loss: 7.5556, Val Accuracy: 58.75%\n",
      "Epoch [60/100] - Train Loss: 0.0509, Val Loss: 7.5675, Val Accuracy: 59.87%\n",
      "Epoch [61/100] - Train Loss: 0.0377, Val Loss: 7.4070, Val Accuracy: 59.52%\n",
      "Epoch [62/100] - Train Loss: 0.0227, Val Loss: 7.5951, Val Accuracy: 60.78%\n",
      "Epoch [63/100] - Train Loss: 0.0379, Val Loss: 7.4381, Val Accuracy: 60.15%\n",
      "Epoch [64/100] - Train Loss: 0.0485, Val Loss: 7.7716, Val Accuracy: 59.24%\n",
      "Epoch [65/100] - Train Loss: 0.0447, Val Loss: 7.5822, Val Accuracy: 59.94%\n",
      "Epoch [66/100] - Train Loss: 0.0386, Val Loss: 7.7493, Val Accuracy: 60.36%\n",
      "Epoch [67/100] - Train Loss: 0.0538, Val Loss: 7.5449, Val Accuracy: 59.45%\n",
      "Epoch [68/100] - Train Loss: 0.0336, Val Loss: 7.6939, Val Accuracy: 59.66%\n",
      "Epoch [69/100] - Train Loss: 0.0373, Val Loss: 7.7421, Val Accuracy: 59.03%\n",
      "Epoch [70/100] - Train Loss: 0.0406, Val Loss: 7.9484, Val Accuracy: 58.33%\n",
      "Epoch [71/100] - Train Loss: 0.0286, Val Loss: 7.6545, Val Accuracy: 60.43%\n",
      "Epoch [72/100] - Train Loss: 0.0123, Val Loss: 7.8133, Val Accuracy: 60.22%\n",
      "Epoch [73/100] - Train Loss: 0.0115, Val Loss: 7.8421, Val Accuracy: 58.96%\n",
      "Epoch [74/100] - Train Loss: 0.0088, Val Loss: 7.8351, Val Accuracy: 59.87%\n",
      "Epoch [75/100] - Train Loss: 0.0090, Val Loss: 7.8943, Val Accuracy: 60.22%\n",
      "Epoch [76/100] - Train Loss: 0.0083, Val Loss: 7.8967, Val Accuracy: 60.29%\n",
      "Epoch [77/100] - Train Loss: 0.0090, Val Loss: 7.9008, Val Accuracy: 59.66%\n",
      "Epoch [78/100] - Train Loss: 0.0088, Val Loss: 7.9308, Val Accuracy: 59.45%\n",
      "Epoch [79/100] - Train Loss: 0.0080, Val Loss: 8.0567, Val Accuracy: 59.52%\n",
      "Epoch [80/100] - Train Loss: 0.0205, Val Loss: 8.2411, Val Accuracy: 58.61%\n",
      "Epoch [81/100] - Train Loss: 0.0763, Val Loss: 8.4046, Val Accuracy: 59.52%\n",
      "Epoch [82/100] - Train Loss: 0.1199, Val Loss: 8.1005, Val Accuracy: 59.80%\n",
      "Epoch [83/100] - Train Loss: 0.0651, Val Loss: 7.9845, Val Accuracy: 60.15%\n",
      "Epoch [84/100] - Train Loss: 0.0397, Val Loss: 8.1196, Val Accuracy: 59.17%\n",
      "Epoch [85/100] - Train Loss: 0.0306, Val Loss: 8.1375, Val Accuracy: 58.89%\n",
      "Epoch [86/100] - Train Loss: 0.0109, Val Loss: 8.1393, Val Accuracy: 60.01%\n",
      "Epoch [87/100] - Train Loss: 0.0085, Val Loss: 8.1468, Val Accuracy: 60.43%\n",
      "Epoch [88/100] - Train Loss: 0.0073, Val Loss: 8.1594, Val Accuracy: 60.15%\n",
      "Epoch [89/100] - Train Loss: 0.0062, Val Loss: 8.1899, Val Accuracy: 60.08%\n",
      "Epoch [90/100] - Train Loss: 0.0059, Val Loss: 8.2327, Val Accuracy: 59.94%\n",
      "Epoch [91/100] - Train Loss: 0.0059, Val Loss: 8.2657, Val Accuracy: 59.87%\n",
      "Epoch [92/100] - Train Loss: 0.0058, Val Loss: 8.3446, Val Accuracy: 59.52%\n",
      "Epoch [93/100] - Train Loss: 0.0057, Val Loss: 8.3569, Val Accuracy: 59.87%\n",
      "Epoch [94/100] - Train Loss: 0.0058, Val Loss: 8.3292, Val Accuracy: 59.94%\n",
      "Epoch [95/100] - Train Loss: 0.0053, Val Loss: 8.4031, Val Accuracy: 60.50%\n",
      "Epoch [96/100] - Train Loss: 0.0050, Val Loss: 8.4065, Val Accuracy: 59.66%\n",
      "Epoch [97/100] - Train Loss: 0.0067, Val Loss: 8.5717, Val Accuracy: 59.66%\n",
      "Epoch [98/100] - Train Loss: 0.0673, Val Loss: 8.4850, Val Accuracy: 59.38%\n",
      "Epoch [99/100] - Train Loss: 0.1482, Val Loss: 8.8026, Val Accuracy: 58.82%\n",
      "Epoch [100/100] - Train Loss: 0.0621, Val Loss: 8.1588, Val Accuracy: 60.01%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    print(total_val_loss)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    val_loss = total_val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee89ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "94a4cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "6b65d48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 9,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 12,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 12,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 12,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 12,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 12,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 1,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 12,\n",
       " 16,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 12,\n",
       " 2,\n",
       " 2,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 7,\n",
       " 7,\n",
       " 13,\n",
       " 8,\n",
       " 11,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 11,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 13,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 12,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 6,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 14,\n",
       " 12,\n",
       " 14,\n",
       " 12,\n",
       " 3,\n",
       " 14,\n",
       " 3,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 15,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 8,\n",
       " 0,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 14,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 10,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 14,\n",
       " 14,\n",
       " 3,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 14,\n",
       " 3,\n",
       " 14,\n",
       " 6,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 12,\n",
       " 3,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 9,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 9,\n",
       " 9,\n",
       " 14,\n",
       " 12,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 15,\n",
       " 1,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 3,\n",
       " 12,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 15,\n",
       " 15,\n",
       " 12,\n",
       " 1,\n",
       " 12,\n",
       " 3,\n",
       " 13,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 13,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 10,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 13,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 13,\n",
       " 3,\n",
       " 6,\n",
       " 13,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 10,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 0,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 2,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 1,\n",
       " 12,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 12,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 13,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 12,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 15,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 13,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 16,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 1,\n",
       " 14,\n",
       " 1,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 3,\n",
       " 11,\n",
       " 14,\n",
       " 11,\n",
       " 12,\n",
       " 3,\n",
       " 6,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 14,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 7,\n",
       " 14,\n",
       " 15,\n",
       " 6,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 14,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 10,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 14,\n",
       " 4,\n",
       " 1,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 14,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 12,\n",
       " 9,\n",
       " 11,\n",
       " 5,\n",
       " 12,\n",
       " 12,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 15,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 11,\n",
       " 1,\n",
       " 12,\n",
       " 11,\n",
       " 1,\n",
       " 1,\n",
       " 13,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 12,\n",
       " 10,\n",
       " 12,\n",
       " 2,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 13,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 12,\n",
       " 2,\n",
       " 12,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 1,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " ...]"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "3454b69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, ..., 8, 8, 8], dtype=int64)"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "2150215b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.61      0.57        84\n",
      "           1       0.49      0.43      0.46        84\n",
      "           2       0.52      0.57      0.55        84\n",
      "           3       0.30      0.27      0.29        84\n",
      "           4       0.72      0.50      0.59        84\n",
      "           5       0.79      0.60      0.68        84\n",
      "           6       0.37      0.55      0.44        84\n",
      "           7       0.64      0.40      0.50        84\n",
      "           8       0.69      0.52      0.59        84\n",
      "           9       0.79      0.45      0.58        84\n",
      "          10       0.83      0.89      0.86        84\n",
      "          11       0.88      0.93      0.90        84\n",
      "          12       0.47      0.74      0.58        84\n",
      "          13       0.66      0.74      0.70        84\n",
      "          14       0.52      0.79      0.63        84\n",
      "          15       0.66      0.46      0.55        84\n",
      "          16       0.88      0.92      0.90        84\n",
      "\n",
      "    accuracy                           0.61      1428\n",
      "   macro avg       0.63      0.61      0.61      1428\n",
      "weighted avg       0.63      0.61      0.61      1428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2d719",
   "metadata": {},
   "source": [
    "### Split by session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c980d167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "e082def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = [1,3]\n",
    "test_participants = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "e4f8cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data['session'].isin(participants)]\n",
    "test_df = data[data['session'].isin(test_participants)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36e60310",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "0d3592d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>participant</th>\n",
       "      <th>gesture</th>\n",
       "      <th>index</th>\n",
       "      <th>iemg</th>\n",
       "      <th>mav</th>\n",
       "      <th>ssi</th>\n",
       "      <th>myopulse</th>\n",
       "      <th>wflen</th>\n",
       "      <th>diffvar</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis_f_w3</th>\n",
       "      <th>kurtosis_f_w4</th>\n",
       "      <th>kurtosis_f_w5</th>\n",
       "      <th>kurtosis_f_w6</th>\n",
       "      <th>kurtosis_f_w7</th>\n",
       "      <th>kurtosis_f_w8</th>\n",
       "      <th>kurtosis_f_w9</th>\n",
       "      <th>kurtosis_f_w10</th>\n",
       "      <th>kurtosis_f_w11</th>\n",
       "      <th>kurtosis_f_w12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11996.863087</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>997.135823</td>\n",
       "      <td>6.224512</td>\n",
       "      <td>8143.485828</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>...</td>\n",
       "      <td>61.940683</td>\n",
       "      <td>116.130710</td>\n",
       "      <td>31.371157</td>\n",
       "      <td>107.176549</td>\n",
       "      <td>64.459302</td>\n",
       "      <td>241.232060</td>\n",
       "      <td>54.880235</td>\n",
       "      <td>56.951175</td>\n",
       "      <td>42.644289</td>\n",
       "      <td>118.918727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10550.150337</td>\n",
       "      <td>0.036796</td>\n",
       "      <td>1019.627142</td>\n",
       "      <td>6.443555</td>\n",
       "      <td>6646.721181</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>...</td>\n",
       "      <td>160.171639</td>\n",
       "      <td>87.566832</td>\n",
       "      <td>154.616093</td>\n",
       "      <td>280.595181</td>\n",
       "      <td>160.756112</td>\n",
       "      <td>98.589493</td>\n",
       "      <td>135.655396</td>\n",
       "      <td>194.396727</td>\n",
       "      <td>294.858908</td>\n",
       "      <td>537.374741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9230.366582</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>630.013233</td>\n",
       "      <td>6.033789</td>\n",
       "      <td>6023.438546</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>...</td>\n",
       "      <td>195.666850</td>\n",
       "      <td>90.568034</td>\n",
       "      <td>94.299202</td>\n",
       "      <td>276.105575</td>\n",
       "      <td>101.698579</td>\n",
       "      <td>49.607516</td>\n",
       "      <td>150.348389</td>\n",
       "      <td>151.177847</td>\n",
       "      <td>156.860637</td>\n",
       "      <td>115.905382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15949.504704</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>1995.234560</td>\n",
       "      <td>4.999609</td>\n",
       "      <td>9318.559036</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>...</td>\n",
       "      <td>33.886272</td>\n",
       "      <td>80.377706</td>\n",
       "      <td>46.373047</td>\n",
       "      <td>64.445165</td>\n",
       "      <td>31.711430</td>\n",
       "      <td>33.328942</td>\n",
       "      <td>50.491362</td>\n",
       "      <td>81.876212</td>\n",
       "      <td>69.070232</td>\n",
       "      <td>142.723324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15936.956936</td>\n",
       "      <td>0.055584</td>\n",
       "      <td>1830.412922</td>\n",
       "      <td>5.870117</td>\n",
       "      <td>10476.465579</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>...</td>\n",
       "      <td>48.630658</td>\n",
       "      <td>39.722917</td>\n",
       "      <td>25.079065</td>\n",
       "      <td>84.877336</td>\n",
       "      <td>29.229362</td>\n",
       "      <td>48.843301</td>\n",
       "      <td>44.917769</td>\n",
       "      <td>39.354396</td>\n",
       "      <td>78.705208</td>\n",
       "      <td>176.820713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15346</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7435.557234</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>369.818221</td>\n",
       "      <td>6.280762</td>\n",
       "      <td>5792.222983</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>...</td>\n",
       "      <td>29.589425</td>\n",
       "      <td>25.460694</td>\n",
       "      <td>182.962449</td>\n",
       "      <td>34.957161</td>\n",
       "      <td>18.846525</td>\n",
       "      <td>37.714119</td>\n",
       "      <td>26.569683</td>\n",
       "      <td>32.614845</td>\n",
       "      <td>12.793219</td>\n",
       "      <td>16.657922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15347</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10471.625678</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>713.052200</td>\n",
       "      <td>6.967187</td>\n",
       "      <td>8628.380136</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>...</td>\n",
       "      <td>18.226247</td>\n",
       "      <td>35.715660</td>\n",
       "      <td>113.822913</td>\n",
       "      <td>64.223311</td>\n",
       "      <td>8.214120</td>\n",
       "      <td>39.211218</td>\n",
       "      <td>29.910432</td>\n",
       "      <td>26.426457</td>\n",
       "      <td>24.892159</td>\n",
       "      <td>82.241098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15348</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>11279.138159</td>\n",
       "      <td>0.039339</td>\n",
       "      <td>815.440185</td>\n",
       "      <td>6.917773</td>\n",
       "      <td>9229.795138</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>...</td>\n",
       "      <td>15.486525</td>\n",
       "      <td>29.417342</td>\n",
       "      <td>25.153035</td>\n",
       "      <td>19.632020</td>\n",
       "      <td>15.590478</td>\n",
       "      <td>15.462391</td>\n",
       "      <td>34.940941</td>\n",
       "      <td>37.564280</td>\n",
       "      <td>14.208021</td>\n",
       "      <td>15.811267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15349</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9210.946156</td>\n",
       "      <td>0.032125</td>\n",
       "      <td>563.422838</td>\n",
       "      <td>7.359570</td>\n",
       "      <td>8040.357766</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>...</td>\n",
       "      <td>14.527208</td>\n",
       "      <td>20.281059</td>\n",
       "      <td>20.418717</td>\n",
       "      <td>18.471741</td>\n",
       "      <td>14.276295</td>\n",
       "      <td>10.083487</td>\n",
       "      <td>27.337412</td>\n",
       "      <td>12.804675</td>\n",
       "      <td>20.220452</td>\n",
       "      <td>37.494222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15350</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9557.645293</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>622.632313</td>\n",
       "      <td>7.815820</td>\n",
       "      <td>8616.136489</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>...</td>\n",
       "      <td>28.617196</td>\n",
       "      <td>17.396729</td>\n",
       "      <td>21.821603</td>\n",
       "      <td>27.706896</td>\n",
       "      <td>16.773846</td>\n",
       "      <td>23.562969</td>\n",
       "      <td>20.576495</td>\n",
       "      <td>21.367926</td>\n",
       "      <td>12.371417</td>\n",
       "      <td>39.203146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10234 rows Ã— 572 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       session  participant  gesture  index          iemg       mav  \\\n",
       "0            1            1       10      0  11996.863087  0.041842   \n",
       "1            1            1       10      0  10550.150337  0.036796   \n",
       "2            1            1       10      0   9230.366582  0.032193   \n",
       "3            1            1       10      0  15949.504704  0.055627   \n",
       "4            1            1       10      0  15936.956936  0.055584   \n",
       "...        ...          ...      ...    ...           ...       ...   \n",
       "15346        3            9        9      0   7435.557234  0.025933   \n",
       "15347        3            9        9      0  10471.625678  0.036522   \n",
       "15348        3            9        9      0  11279.138159  0.039339   \n",
       "15349        3            9        9      0   9210.946156  0.032125   \n",
       "15350        3            9        9      0   9557.645293  0.033334   \n",
       "\n",
       "               ssi  myopulse         wflen   diffvar  ...  kurtosis_f_w3  \\\n",
       "0       997.135823  6.224512   8143.485828  0.001888  ...      61.940683   \n",
       "1      1019.627142  6.443555   6646.721181  0.001438  ...     160.171639   \n",
       "2       630.013233  6.033789   6023.438546  0.001169  ...     195.666850   \n",
       "3      1995.234560  4.999609   9318.559036  0.003030  ...      33.886272   \n",
       "4      1830.412922  5.870117  10476.465579  0.003688  ...      48.630658   \n",
       "...            ...       ...           ...       ...  ...            ...   \n",
       "15346   369.818221  6.280762   5792.222983  0.001070  ...      29.589425   \n",
       "15347   713.052200  6.967187   8628.380136  0.002070  ...      18.226247   \n",
       "15348   815.440185  6.917773   9229.795138  0.002355  ...      15.486525   \n",
       "15349   563.422838  7.359570   8040.357766  0.001839  ...      14.527208   \n",
       "15350   622.632313  7.815820   8616.136489  0.002036  ...      28.617196   \n",
       "\n",
       "       kurtosis_f_w4  kurtosis_f_w5  kurtosis_f_w6  kurtosis_f_w7  \\\n",
       "0         116.130710      31.371157     107.176549      64.459302   \n",
       "1          87.566832     154.616093     280.595181     160.756112   \n",
       "2          90.568034      94.299202     276.105575     101.698579   \n",
       "3          80.377706      46.373047      64.445165      31.711430   \n",
       "4          39.722917      25.079065      84.877336      29.229362   \n",
       "...              ...            ...            ...            ...   \n",
       "15346      25.460694     182.962449      34.957161      18.846525   \n",
       "15347      35.715660     113.822913      64.223311       8.214120   \n",
       "15348      29.417342      25.153035      19.632020      15.590478   \n",
       "15349      20.281059      20.418717      18.471741      14.276295   \n",
       "15350      17.396729      21.821603      27.706896      16.773846   \n",
       "\n",
       "       kurtosis_f_w8  kurtosis_f_w9  kurtosis_f_w10  kurtosis_f_w11  \\\n",
       "0         241.232060      54.880235       56.951175       42.644289   \n",
       "1          98.589493     135.655396      194.396727      294.858908   \n",
       "2          49.607516     150.348389      151.177847      156.860637   \n",
       "3          33.328942      50.491362       81.876212       69.070232   \n",
       "4          48.843301      44.917769       39.354396       78.705208   \n",
       "...              ...            ...             ...             ...   \n",
       "15346      37.714119      26.569683       32.614845       12.793219   \n",
       "15347      39.211218      29.910432       26.426457       24.892159   \n",
       "15348      15.462391      34.940941       37.564280       14.208021   \n",
       "15349      10.083487      27.337412       12.804675       20.220452   \n",
       "15350      23.562969      20.576495       21.367926       12.371417   \n",
       "\n",
       "       kurtosis_f_w12  \n",
       "0          118.918727  \n",
       "1          537.374741  \n",
       "2          115.905382  \n",
       "3          142.723324  \n",
       "4          176.820713  \n",
       "...               ...  \n",
       "15346       16.657922  \n",
       "15347       82.241098  \n",
       "15348       15.811267  \n",
       "15349       37.494222  \n",
       "15350       39.203146  \n",
       "\n",
       "[10234 rows x 572 columns]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "b9cba7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.iloc[:, 4:].values\n",
    "y_train = (train_df.iloc[:, 2] - 1).values\n",
    "\n",
    "x_test = test_df.iloc[:, 4:].values\n",
    "y_test = (test_df.iloc[:, 2] - 1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "5d981398",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "7004f0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "63d96bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "c18322e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, output_shape):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_shape, 32),  # Input layer: Fully connected (linear) with 64 units\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(64, output_shape)  # Output layer: Fully connected (linear) with 'output_shape' units\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "d5a67aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(x_train).type(torch.float32), torch.tensor(y_train).type(torch.LongTensor))\n",
    "test_dataset = TensorDataset(torch.tensor(x_test).type(torch.float32), torch.tensor(y_test).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "36978a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "3a2c43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and whether to shuffle the data\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912d5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "81adb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model(x_train.shape[1], len(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c9af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "2fafd05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "701f945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "eb0d15cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Train Loss: 2.4191, Val Loss: 1365127.5563, Val Accuracy: 38.38%\n",
      "Epoch [2/100] - Train Loss: 1.5826, Val Loss: 9924327.7019, Val Accuracy: 58.84%\n",
      "Epoch [3/100] - Train Loss: 1.1271, Val Loss: 21853153.0808, Val Accuracy: 65.88%\n",
      "Epoch [4/100] - Train Loss: 0.9310, Val Loss: 27828983.3690, Val Accuracy: 69.55%\n",
      "Epoch [5/100] - Train Loss: 0.8197, Val Loss: 34200506.5026, Val Accuracy: 72.11%\n",
      "Epoch [6/100] - Train Loss: 0.7383, Val Loss: 36717312.8797, Val Accuracy: 72.52%\n",
      "Epoch [7/100] - Train Loss: 0.6748, Val Loss: 37467072.8526, Val Accuracy: 73.99%\n",
      "Epoch [8/100] - Train Loss: 0.6270, Val Loss: 47859888.8476, Val Accuracy: 74.42%\n",
      "Epoch [9/100] - Train Loss: 0.5921, Val Loss: 45645607.2486, Val Accuracy: 74.52%\n",
      "Epoch [10/100] - Train Loss: 0.5652, Val Loss: 42729972.0853, Val Accuracy: 75.20%\n",
      "Epoch [11/100] - Train Loss: 0.5350, Val Loss: 38915735.2525, Val Accuracy: 75.83%\n",
      "Epoch [12/100] - Train Loss: 0.5119, Val Loss: 47672640.8599, Val Accuracy: 77.14%\n",
      "Epoch [13/100] - Train Loss: 0.4853, Val Loss: 86664724.0665, Val Accuracy: 76.55%\n",
      "Epoch [14/100] - Train Loss: 0.4633, Val Loss: 88890061.6927, Val Accuracy: 76.57%\n",
      "Epoch [15/100] - Train Loss: 0.4529, Val Loss: 85026317.6827, Val Accuracy: 76.96%\n",
      "Epoch [16/100] - Train Loss: 0.4307, Val Loss: 81932404.0708, Val Accuracy: 77.04%\n",
      "Epoch [17/100] - Train Loss: 0.4174, Val Loss: 78384909.7131, Val Accuracy: 76.90%\n",
      "Epoch [18/100] - Train Loss: 0.4016, Val Loss: 75472295.2895, Val Accuracy: 77.33%\n",
      "Epoch [19/100] - Train Loss: 0.3854, Val Loss: 70022407.2815, Val Accuracy: 77.29%\n",
      "Epoch [20/100] - Train Loss: 0.3755, Val Loss: 68599156.0822, Val Accuracy: 77.66%\n",
      "Epoch [21/100] - Train Loss: 0.3677, Val Loss: 65412199.3252, Val Accuracy: 76.88%\n",
      "Epoch [22/100] - Train Loss: 0.3537, Val Loss: 66018285.7340, Val Accuracy: 76.74%\n",
      "Epoch [23/100] - Train Loss: 0.3492, Val Loss: 62719687.3525, Val Accuracy: 77.55%\n",
      "Epoch [24/100] - Train Loss: 0.3417, Val Loss: 63045850.5935, Val Accuracy: 76.02%\n",
      "Epoch [25/100] - Train Loss: 0.3334, Val Loss: 65428045.7338, Val Accuracy: 77.96%\n",
      "Epoch [26/100] - Train Loss: 0.3146, Val Loss: 68037748.1543, Val Accuracy: 77.15%\n",
      "Epoch [27/100] - Train Loss: 0.3084, Val Loss: 67697076.1590, Val Accuracy: 77.66%\n",
      "Epoch [28/100] - Train Loss: 0.3049, Val Loss: 70738113.0365, Val Accuracy: 76.31%\n",
      "Epoch [29/100] - Train Loss: 0.2896, Val Loss: 70450740.1963, Val Accuracy: 77.76%\n",
      "Epoch [30/100] - Train Loss: 0.2821, Val Loss: 76075149.8170, Val Accuracy: 77.08%\n",
      "Epoch [31/100] - Train Loss: 0.2775, Val Loss: 78187316.2379, Val Accuracy: 76.80%\n",
      "Epoch [32/100] - Train Loss: 0.2750, Val Loss: 81730765.8407, Val Accuracy: 76.72%\n",
      "Epoch [33/100] - Train Loss: 0.2606, Val Loss: 80457588.2693, Val Accuracy: 77.35%\n",
      "Epoch [34/100] - Train Loss: 0.2523, Val Loss: 88110996.2567, Val Accuracy: 77.56%\n",
      "Epoch [35/100] - Train Loss: 0.2484, Val Loss: 169435841.0533, Val Accuracy: 76.98%\n",
      "Epoch [36/100] - Train Loss: 0.2421, Val Loss: 173063002.6800, Val Accuracy: 77.23%\n",
      "Epoch [37/100] - Train Loss: 0.2368, Val Loss: 175804877.8477, Val Accuracy: 77.47%\n",
      "Epoch [38/100] - Train Loss: 0.2313, Val Loss: 175110234.6514, Val Accuracy: 77.58%\n",
      "Epoch [39/100] - Train Loss: 0.2251, Val Loss: 177514868.3370, Val Accuracy: 76.43%\n",
      "Epoch [40/100] - Train Loss: 0.2244, Val Loss: 170522215.6166, Val Accuracy: 76.10%\n",
      "Epoch [41/100] - Train Loss: 0.2411, Val Loss: 170830657.1904, Val Accuracy: 76.28%\n",
      "Epoch [42/100] - Train Loss: 0.2128, Val Loss: 170227546.7590, Val Accuracy: 76.96%\n",
      "Epoch [43/100] - Train Loss: 0.2073, Val Loss: 175739009.1528, Val Accuracy: 77.04%\n",
      "Epoch [44/100] - Train Loss: 0.2036, Val Loss: 171890893.9680, Val Accuracy: 76.43%\n",
      "Epoch [45/100] - Train Loss: 0.2019, Val Loss: 171385908.3619, Val Accuracy: 77.39%\n",
      "Epoch [46/100] - Train Loss: 0.1879, Val Loss: 169566222.0132, Val Accuracy: 77.04%\n",
      "Epoch [47/100] - Train Loss: 0.1867, Val Loss: 175823962.8024, Val Accuracy: 77.41%\n",
      "Epoch [48/100] - Train Loss: 0.1825, Val Loss: 174580890.8081, Val Accuracy: 77.12%\n",
      "Epoch [49/100] - Train Loss: 0.1752, Val Loss: 178459764.4206, Val Accuracy: 77.33%\n",
      "Epoch [50/100] - Train Loss: 0.1724, Val Loss: 183333313.2472, Val Accuracy: 77.08%\n",
      "Epoch [51/100] - Train Loss: 0.1665, Val Loss: 178261159.6909, Val Accuracy: 76.51%\n",
      "Epoch [52/100] - Train Loss: 0.1601, Val Loss: 166247425.2730, Val Accuracy: 76.80%\n",
      "Epoch [53/100] - Train Loss: 0.1602, Val Loss: 169240116.5489, Val Accuracy: 75.53%\n",
      "Epoch [54/100] - Train Loss: 0.1669, Val Loss: 167587444.5211, Val Accuracy: 76.65%\n",
      "Epoch [55/100] - Train Loss: 0.1630, Val Loss: 165759156.4903, Val Accuracy: 77.56%\n",
      "Epoch [56/100] - Train Loss: 0.1536, Val Loss: 182070081.2932, Val Accuracy: 77.19%\n",
      "Epoch [57/100] - Train Loss: 0.1673, Val Loss: 180188302.1783, Val Accuracy: 77.25%\n",
      "Epoch [58/100] - Train Loss: 0.1589, Val Loss: 176147329.4172, Val Accuracy: 76.53%\n",
      "Epoch [59/100] - Train Loss: 0.1640, Val Loss: 186464794.9949, Val Accuracy: 76.59%\n",
      "Epoch [60/100] - Train Loss: 0.1441, Val Loss: 193792884.6033, Val Accuracy: 76.86%\n",
      "Epoch [61/100] - Train Loss: 0.1422, Val Loss: 200549159.7971, Val Accuracy: 77.23%\n",
      "Epoch [62/100] - Train Loss: 0.1314, Val Loss: 204225537.4299, Val Accuracy: 76.55%\n",
      "Epoch [63/100] - Train Loss: 0.1248, Val Loss: 208206593.4222, Val Accuracy: 76.76%\n",
      "Epoch [64/100] - Train Loss: 0.1229, Val Loss: 198986702.2557, Val Accuracy: 77.25%\n",
      "Epoch [65/100] - Train Loss: 0.1219, Val Loss: 201475380.6665, Val Accuracy: 76.24%\n",
      "Epoch [66/100] - Train Loss: 0.1248, Val Loss: 224214580.6708, Val Accuracy: 76.51%\n",
      "Epoch [67/100] - Train Loss: 0.1219, Val Loss: 233791310.3234, Val Accuracy: 76.78%\n",
      "Epoch [68/100] - Train Loss: 0.1132, Val Loss: 251150363.1053, Val Accuracy: 76.53%\n",
      "Epoch [69/100] - Train Loss: 0.1114, Val Loss: 259406695.9373, Val Accuracy: 77.06%\n",
      "Epoch [70/100] - Train Loss: 0.1091, Val Loss: 257289755.1875, Val Accuracy: 76.76%\n",
      "Epoch [71/100] - Train Loss: 0.1130, Val Loss: 257079783.9611, Val Accuracy: 76.57%\n",
      "Epoch [72/100] - Train Loss: 0.1071, Val Loss: 195716801.5777, Val Accuracy: 76.51%\n",
      "Epoch [73/100] - Train Loss: 0.1058, Val Loss: 208856219.1665, Val Accuracy: 76.37%\n",
      "Epoch [74/100] - Train Loss: 0.0970, Val Loss: 208171265.5665, Val Accuracy: 76.63%\n",
      "Epoch [75/100] - Train Loss: 0.1017, Val Loss: 210092737.6429, Val Accuracy: 76.14%\n",
      "Epoch [76/100] - Train Loss: 0.0940, Val Loss: 219242139.2504, Val Accuracy: 76.65%\n",
      "Epoch [77/100] - Train Loss: 0.0919, Val Loss: 219165620.8440, Val Accuracy: 76.88%\n",
      "Epoch [78/100] - Train Loss: 0.1251, Val Loss: 243174529.7724, Val Accuracy: 76.31%\n",
      "Epoch [79/100] - Train Loss: 0.1185, Val Loss: 232306152.1887, Val Accuracy: 76.06%\n",
      "Epoch [80/100] - Train Loss: 0.1010, Val Loss: 265340724.8878, Val Accuracy: 77.17%\n",
      "Epoch [81/100] - Train Loss: 0.0868, Val Loss: 265929140.9649, Val Accuracy: 76.06%\n",
      "Epoch [82/100] - Train Loss: 0.0815, Val Loss: 270091982.5355, Val Accuracy: 76.43%\n",
      "Epoch [83/100] - Train Loss: 0.0841, Val Loss: 262595841.7810, Val Accuracy: 76.28%\n",
      "Epoch [84/100] - Train Loss: 0.0896, Val Loss: 267096961.7674, Val Accuracy: 76.59%\n",
      "Epoch [85/100] - Train Loss: 0.0839, Val Loss: 276849486.5737, Val Accuracy: 76.41%\n",
      "Epoch [86/100] - Train Loss: 0.0744, Val Loss: 270521806.6442, Val Accuracy: 75.94%\n",
      "Epoch [87/100] - Train Loss: 0.0790, Val Loss: 272238542.6056, Val Accuracy: 76.72%\n",
      "Epoch [88/100] - Train Loss: 0.0783, Val Loss: 272589800.3747, Val Accuracy: 76.82%\n",
      "Epoch [89/100] - Train Loss: 0.0741, Val Loss: 289934286.6931, Val Accuracy: 76.51%\n",
      "Epoch [90/100] - Train Loss: 0.0741, Val Loss: 287199233.8924, Val Accuracy: 77.15%\n",
      "Epoch [91/100] - Train Loss: 0.0661, Val Loss: 270696296.3404, Val Accuracy: 76.53%\n",
      "Epoch [92/100] - Train Loss: 0.0641, Val Loss: 272510875.5224, Val Accuracy: 76.35%\n",
      "Epoch [93/100] - Train Loss: 0.0725, Val Loss: 273197749.1660, Val Accuracy: 76.57%\n",
      "Epoch [94/100] - Train Loss: 0.0603, Val Loss: 275136334.7655, Val Accuracy: 76.39%\n",
      "Epoch [95/100] - Train Loss: 0.0558, Val Loss: 280884097.9720, Val Accuracy: 76.72%\n",
      "Epoch [96/100] - Train Loss: 0.0584, Val Loss: 278566376.3913, Val Accuracy: 76.80%\n",
      "Epoch [97/100] - Train Loss: 0.0729, Val Loss: 272707841.9990, Val Accuracy: 76.80%\n",
      "Epoch [98/100] - Train Loss: 0.0753, Val Loss: 289343771.7241, Val Accuracy: 76.20%\n",
      "Epoch [99/100] - Train Loss: 0.0714, Val Loss: 280992053.3292, Val Accuracy: 75.86%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100] - Train Loss: 0.0618, Val Loss: 299416731.6430, Val Accuracy: 76.76%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    val_loss = total_val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31084d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "0d500a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "31ea3cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, ..., 8, 8, 8], dtype=int64)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "f44dfab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.61      0.63       301\n",
      "           1       0.65      0.51      0.57       301\n",
      "           2       0.69      0.72      0.71       301\n",
      "           3       0.57      0.64      0.61       301\n",
      "           4       0.74      0.80      0.77       301\n",
      "           5       0.88      0.87      0.87       301\n",
      "           6       0.66      0.65      0.66       301\n",
      "           7       0.78      0.85      0.81       301\n",
      "           8       0.73      0.72      0.72       301\n",
      "           9       0.79      0.82      0.80       301\n",
      "          10       0.95      0.92      0.93       301\n",
      "          11       0.86      0.85      0.86       301\n",
      "          12       0.81      0.82      0.82       301\n",
      "          13       0.84      0.83      0.84       301\n",
      "          14       0.82      0.79      0.81       301\n",
      "          15       0.70      0.66      0.68       301\n",
      "          16       0.93      0.96      0.94       301\n",
      "\n",
      "    accuracy                           0.77      5117\n",
      "   macro avg       0.77      0.77      0.77      5117\n",
      "weighted avg       0.77      0.77      0.77      5117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "re"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
