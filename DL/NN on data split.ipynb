{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "962328fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "02feafb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0dc541ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "eb7f89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "33064c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4b5b0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972deead",
   "metadata": {},
   "source": [
    "### Baseline Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3175c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/gesture-recognition-and-biometrics-electromyogram-grabmyo-1.0.2/features_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "371d1205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>session</th>\n",
       "      <th>participant</th>\n",
       "      <th>gesture</th>\n",
       "      <th>index</th>\n",
       "      <th>iemg</th>\n",
       "      <th>mav</th>\n",
       "      <th>ssi</th>\n",
       "      <th>myopulse</th>\n",
       "      <th>wflen</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis_f_w3</th>\n",
       "      <th>kurtosis_f_w4</th>\n",
       "      <th>kurtosis_f_w5</th>\n",
       "      <th>kurtosis_f_w6</th>\n",
       "      <th>kurtosis_f_w7</th>\n",
       "      <th>kurtosis_f_w8</th>\n",
       "      <th>kurtosis_f_w9</th>\n",
       "      <th>kurtosis_f_w10</th>\n",
       "      <th>kurtosis_f_w11</th>\n",
       "      <th>kurtosis_f_w12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11996.863087</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>997.135823</td>\n",
       "      <td>6.224512</td>\n",
       "      <td>8143.485828</td>\n",
       "      <td>...</td>\n",
       "      <td>61.940683</td>\n",
       "      <td>116.130710</td>\n",
       "      <td>31.371157</td>\n",
       "      <td>107.176549</td>\n",
       "      <td>64.459302</td>\n",
       "      <td>241.232060</td>\n",
       "      <td>54.880235</td>\n",
       "      <td>56.951175</td>\n",
       "      <td>42.644289</td>\n",
       "      <td>118.918727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10550.150337</td>\n",
       "      <td>0.036796</td>\n",
       "      <td>1019.627142</td>\n",
       "      <td>6.443555</td>\n",
       "      <td>6646.721181</td>\n",
       "      <td>...</td>\n",
       "      <td>160.171639</td>\n",
       "      <td>87.566832</td>\n",
       "      <td>154.616093</td>\n",
       "      <td>280.595181</td>\n",
       "      <td>160.756112</td>\n",
       "      <td>98.589493</td>\n",
       "      <td>135.655396</td>\n",
       "      <td>194.396727</td>\n",
       "      <td>294.858908</td>\n",
       "      <td>537.374741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9230.366582</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>630.013233</td>\n",
       "      <td>6.033789</td>\n",
       "      <td>6023.438546</td>\n",
       "      <td>...</td>\n",
       "      <td>195.666850</td>\n",
       "      <td>90.568034</td>\n",
       "      <td>94.299202</td>\n",
       "      <td>276.105575</td>\n",
       "      <td>101.698579</td>\n",
       "      <td>49.607516</td>\n",
       "      <td>150.348389</td>\n",
       "      <td>151.177847</td>\n",
       "      <td>156.860637</td>\n",
       "      <td>115.905382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15949.504704</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>1995.234560</td>\n",
       "      <td>4.999609</td>\n",
       "      <td>9318.559036</td>\n",
       "      <td>...</td>\n",
       "      <td>33.886272</td>\n",
       "      <td>80.377706</td>\n",
       "      <td>46.373047</td>\n",
       "      <td>64.445165</td>\n",
       "      <td>31.711430</td>\n",
       "      <td>33.328942</td>\n",
       "      <td>50.491362</td>\n",
       "      <td>81.876212</td>\n",
       "      <td>69.070232</td>\n",
       "      <td>142.723324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15936.956936</td>\n",
       "      <td>0.055584</td>\n",
       "      <td>1830.412922</td>\n",
       "      <td>5.870117</td>\n",
       "      <td>10476.465579</td>\n",
       "      <td>...</td>\n",
       "      <td>48.630658</td>\n",
       "      <td>39.722917</td>\n",
       "      <td>25.079065</td>\n",
       "      <td>84.877336</td>\n",
       "      <td>29.229362</td>\n",
       "      <td>48.843301</td>\n",
       "      <td>44.917769</td>\n",
       "      <td>39.354396</td>\n",
       "      <td>78.705208</td>\n",
       "      <td>176.820713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  session  participant  gesture  index          iemg       mav  \\\n",
       "0           0        1            1       10      0  11996.863087  0.041842   \n",
       "1           1        1            1       10      0  10550.150337  0.036796   \n",
       "2           2        1            1       10      0   9230.366582  0.032193   \n",
       "3           3        1            1       10      0  15949.504704  0.055627   \n",
       "4           4        1            1       10      0  15936.956936  0.055584   \n",
       "\n",
       "           ssi  myopulse         wflen  ...  kurtosis_f_w3  kurtosis_f_w4  \\\n",
       "0   997.135823  6.224512   8143.485828  ...      61.940683     116.130710   \n",
       "1  1019.627142  6.443555   6646.721181  ...     160.171639      87.566832   \n",
       "2   630.013233  6.033789   6023.438546  ...     195.666850      90.568034   \n",
       "3  1995.234560  4.999609   9318.559036  ...      33.886272      80.377706   \n",
       "4  1830.412922  5.870117  10476.465579  ...      48.630658      39.722917   \n",
       "\n",
       "   kurtosis_f_w5  kurtosis_f_w6  kurtosis_f_w7  kurtosis_f_w8  kurtosis_f_w9  \\\n",
       "0      31.371157     107.176549      64.459302     241.232060      54.880235   \n",
       "1     154.616093     280.595181     160.756112      98.589493     135.655396   \n",
       "2      94.299202     276.105575     101.698579      49.607516     150.348389   \n",
       "3      46.373047      64.445165      31.711430      33.328942      50.491362   \n",
       "4      25.079065      84.877336      29.229362      48.843301      44.917769   \n",
       "\n",
       "   kurtosis_f_w10  kurtosis_f_w11  kurtosis_f_w12  \n",
       "0       56.951175       42.644289      118.918727  \n",
       "1      194.396727      294.858908      537.374741  \n",
       "2      151.177847      156.860637      115.905382  \n",
       "3       81.876212       69.070232      142.723324  \n",
       "4       39.354396       78.705208      176.820713  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "335c2ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'session', 'participant', 'gesture', 'index', 'iemg',\n",
       "       'mav', 'ssi', 'myopulse', 'wflen',\n",
       "       ...\n",
       "       'kurtosis_f_w3', 'kurtosis_f_w4', 'kurtosis_f_w5', 'kurtosis_f_w6',\n",
       "       'kurtosis_f_w7', 'kurtosis_f_w8', 'kurtosis_f_w9', 'kurtosis_f_w10',\n",
       "       'kurtosis_f_w11', 'kurtosis_f_w12'],\n",
       "      dtype='object', length=573)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7929ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b3eb5233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>participant</th>\n",
       "      <th>gesture</th>\n",
       "      <th>index</th>\n",
       "      <th>iemg</th>\n",
       "      <th>mav</th>\n",
       "      <th>ssi</th>\n",
       "      <th>myopulse</th>\n",
       "      <th>wflen</th>\n",
       "      <th>diffvar</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis_f_w3</th>\n",
       "      <th>kurtosis_f_w4</th>\n",
       "      <th>kurtosis_f_w5</th>\n",
       "      <th>kurtosis_f_w6</th>\n",
       "      <th>kurtosis_f_w7</th>\n",
       "      <th>kurtosis_f_w8</th>\n",
       "      <th>kurtosis_f_w9</th>\n",
       "      <th>kurtosis_f_w10</th>\n",
       "      <th>kurtosis_f_w11</th>\n",
       "      <th>kurtosis_f_w12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11996.863087</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>997.135823</td>\n",
       "      <td>6.224512</td>\n",
       "      <td>8143.485828</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>...</td>\n",
       "      <td>61.940683</td>\n",
       "      <td>116.130710</td>\n",
       "      <td>31.371157</td>\n",
       "      <td>107.176549</td>\n",
       "      <td>64.459302</td>\n",
       "      <td>241.232060</td>\n",
       "      <td>54.880235</td>\n",
       "      <td>56.951175</td>\n",
       "      <td>42.644289</td>\n",
       "      <td>118.918727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10550.150337</td>\n",
       "      <td>0.036796</td>\n",
       "      <td>1019.627142</td>\n",
       "      <td>6.443555</td>\n",
       "      <td>6646.721181</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>...</td>\n",
       "      <td>160.171639</td>\n",
       "      <td>87.566832</td>\n",
       "      <td>154.616093</td>\n",
       "      <td>280.595181</td>\n",
       "      <td>160.756112</td>\n",
       "      <td>98.589493</td>\n",
       "      <td>135.655396</td>\n",
       "      <td>194.396727</td>\n",
       "      <td>294.858908</td>\n",
       "      <td>537.374741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9230.366582</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>630.013233</td>\n",
       "      <td>6.033789</td>\n",
       "      <td>6023.438546</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>...</td>\n",
       "      <td>195.666850</td>\n",
       "      <td>90.568034</td>\n",
       "      <td>94.299202</td>\n",
       "      <td>276.105575</td>\n",
       "      <td>101.698579</td>\n",
       "      <td>49.607516</td>\n",
       "      <td>150.348389</td>\n",
       "      <td>151.177847</td>\n",
       "      <td>156.860637</td>\n",
       "      <td>115.905382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15949.504704</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>1995.234560</td>\n",
       "      <td>4.999609</td>\n",
       "      <td>9318.559036</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>...</td>\n",
       "      <td>33.886272</td>\n",
       "      <td>80.377706</td>\n",
       "      <td>46.373047</td>\n",
       "      <td>64.445165</td>\n",
       "      <td>31.711430</td>\n",
       "      <td>33.328942</td>\n",
       "      <td>50.491362</td>\n",
       "      <td>81.876212</td>\n",
       "      <td>69.070232</td>\n",
       "      <td>142.723324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15936.956936</td>\n",
       "      <td>0.055584</td>\n",
       "      <td>1830.412922</td>\n",
       "      <td>5.870117</td>\n",
       "      <td>10476.465579</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>...</td>\n",
       "      <td>48.630658</td>\n",
       "      <td>39.722917</td>\n",
       "      <td>25.079065</td>\n",
       "      <td>84.877336</td>\n",
       "      <td>29.229362</td>\n",
       "      <td>48.843301</td>\n",
       "      <td>44.917769</td>\n",
       "      <td>39.354396</td>\n",
       "      <td>78.705208</td>\n",
       "      <td>176.820713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10302.082204</td>\n",
       "      <td>0.035931</td>\n",
       "      <td>957.747699</td>\n",
       "      <td>5.384570</td>\n",
       "      <td>6660.235040</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>...</td>\n",
       "      <td>194.169246</td>\n",
       "      <td>160.867419</td>\n",
       "      <td>127.623463</td>\n",
       "      <td>144.913630</td>\n",
       "      <td>157.298899</td>\n",
       "      <td>94.204471</td>\n",
       "      <td>155.422302</td>\n",
       "      <td>54.005036</td>\n",
       "      <td>53.646785</td>\n",
       "      <td>61.060966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>19771.041940</td>\n",
       "      <td>0.068956</td>\n",
       "      <td>2959.381173</td>\n",
       "      <td>5.280957</td>\n",
       "      <td>11976.937109</td>\n",
       "      <td>0.005174</td>\n",
       "      <td>...</td>\n",
       "      <td>113.893317</td>\n",
       "      <td>54.651356</td>\n",
       "      <td>46.359047</td>\n",
       "      <td>96.793045</td>\n",
       "      <td>54.653056</td>\n",
       "      <td>182.248943</td>\n",
       "      <td>65.594571</td>\n",
       "      <td>81.105593</td>\n",
       "      <td>66.837683</td>\n",
       "      <td>61.510619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>28104.586469</td>\n",
       "      <td>0.098021</td>\n",
       "      <td>7096.757592</td>\n",
       "      <td>4.965234</td>\n",
       "      <td>12613.872878</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>...</td>\n",
       "      <td>140.633258</td>\n",
       "      <td>44.404009</td>\n",
       "      <td>68.097183</td>\n",
       "      <td>49.395457</td>\n",
       "      <td>147.219607</td>\n",
       "      <td>45.064573</td>\n",
       "      <td>157.636975</td>\n",
       "      <td>59.939376</td>\n",
       "      <td>66.865776</td>\n",
       "      <td>91.715022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>27253.345839</td>\n",
       "      <td>0.095052</td>\n",
       "      <td>6467.184093</td>\n",
       "      <td>4.996094</td>\n",
       "      <td>12258.059044</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>...</td>\n",
       "      <td>52.195250</td>\n",
       "      <td>43.058702</td>\n",
       "      <td>29.454097</td>\n",
       "      <td>74.433499</td>\n",
       "      <td>48.285962</td>\n",
       "      <td>29.769905</td>\n",
       "      <td>114.525041</td>\n",
       "      <td>80.717165</td>\n",
       "      <td>52.239566</td>\n",
       "      <td>94.334606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>24522.650663</td>\n",
       "      <td>0.085528</td>\n",
       "      <td>4589.346795</td>\n",
       "      <td>4.919336</td>\n",
       "      <td>11242.445344</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>...</td>\n",
       "      <td>38.664579</td>\n",
       "      <td>39.118622</td>\n",
       "      <td>30.194664</td>\n",
       "      <td>28.584404</td>\n",
       "      <td>38.329797</td>\n",
       "      <td>46.371623</td>\n",
       "      <td>34.716442</td>\n",
       "      <td>31.226083</td>\n",
       "      <td>49.508191</td>\n",
       "      <td>34.297186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>31742.205936</td>\n",
       "      <td>0.110708</td>\n",
       "      <td>7667.393736</td>\n",
       "      <td>4.846484</td>\n",
       "      <td>14106.465485</td>\n",
       "      <td>0.007467</td>\n",
       "      <td>...</td>\n",
       "      <td>15.008300</td>\n",
       "      <td>27.062279</td>\n",
       "      <td>27.067901</td>\n",
       "      <td>16.489398</td>\n",
       "      <td>18.553551</td>\n",
       "      <td>30.774797</td>\n",
       "      <td>54.415505</td>\n",
       "      <td>46.828774</td>\n",
       "      <td>42.710088</td>\n",
       "      <td>52.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>36412.348291</td>\n",
       "      <td>0.126996</td>\n",
       "      <td>10261.995480</td>\n",
       "      <td>4.799121</td>\n",
       "      <td>15714.702877</td>\n",
       "      <td>0.009415</td>\n",
       "      <td>...</td>\n",
       "      <td>13.195526</td>\n",
       "      <td>29.477076</td>\n",
       "      <td>22.342876</td>\n",
       "      <td>25.260824</td>\n",
       "      <td>46.858799</td>\n",
       "      <td>42.441574</td>\n",
       "      <td>57.733548</td>\n",
       "      <td>79.930363</td>\n",
       "      <td>62.170107</td>\n",
       "      <td>103.636508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>29334.356528</td>\n",
       "      <td>0.102310</td>\n",
       "      <td>7696.291516</td>\n",
       "      <td>4.863770</td>\n",
       "      <td>13094.098954</td>\n",
       "      <td>0.007279</td>\n",
       "      <td>...</td>\n",
       "      <td>26.360612</td>\n",
       "      <td>21.980495</td>\n",
       "      <td>33.952634</td>\n",
       "      <td>28.771759</td>\n",
       "      <td>23.137365</td>\n",
       "      <td>29.683661</td>\n",
       "      <td>50.666970</td>\n",
       "      <td>46.655760</td>\n",
       "      <td>44.238902</td>\n",
       "      <td>47.857019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>40562.091730</td>\n",
       "      <td>0.141469</td>\n",
       "      <td>11729.491266</td>\n",
       "      <td>4.848145</td>\n",
       "      <td>17725.698335</td>\n",
       "      <td>0.011385</td>\n",
       "      <td>...</td>\n",
       "      <td>14.889469</td>\n",
       "      <td>24.544620</td>\n",
       "      <td>16.127072</td>\n",
       "      <td>21.020319</td>\n",
       "      <td>21.517667</td>\n",
       "      <td>35.291645</td>\n",
       "      <td>36.992259</td>\n",
       "      <td>32.222933</td>\n",
       "      <td>55.959554</td>\n",
       "      <td>38.280952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>7304.356544</td>\n",
       "      <td>0.025476</td>\n",
       "      <td>414.201641</td>\n",
       "      <td>6.174902</td>\n",
       "      <td>4752.641057</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>...</td>\n",
       "      <td>63.079901</td>\n",
       "      <td>82.825368</td>\n",
       "      <td>93.750642</td>\n",
       "      <td>340.397936</td>\n",
       "      <td>40.822826</td>\n",
       "      <td>64.858434</td>\n",
       "      <td>105.261496</td>\n",
       "      <td>43.192037</td>\n",
       "      <td>101.315301</td>\n",
       "      <td>99.887088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5195.415342</td>\n",
       "      <td>0.018120</td>\n",
       "      <td>231.959814</td>\n",
       "      <td>6.103418</td>\n",
       "      <td>3131.817615</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>...</td>\n",
       "      <td>65.755548</td>\n",
       "      <td>95.883174</td>\n",
       "      <td>75.837115</td>\n",
       "      <td>110.616004</td>\n",
       "      <td>43.524357</td>\n",
       "      <td>61.941302</td>\n",
       "      <td>38.202959</td>\n",
       "      <td>31.752584</td>\n",
       "      <td>62.789179</td>\n",
       "      <td>34.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5010.829409</td>\n",
       "      <td>0.017476</td>\n",
       "      <td>203.733441</td>\n",
       "      <td>5.874609</td>\n",
       "      <td>2787.482041</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>...</td>\n",
       "      <td>45.348221</td>\n",
       "      <td>41.100077</td>\n",
       "      <td>31.168374</td>\n",
       "      <td>56.728636</td>\n",
       "      <td>17.881988</td>\n",
       "      <td>23.522819</td>\n",
       "      <td>45.925652</td>\n",
       "      <td>26.525906</td>\n",
       "      <td>26.392454</td>\n",
       "      <td>36.021888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>7350.757597</td>\n",
       "      <td>0.025637</td>\n",
       "      <td>383.271227</td>\n",
       "      <td>4.712891</td>\n",
       "      <td>3837.262402</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>...</td>\n",
       "      <td>45.747711</td>\n",
       "      <td>74.443871</td>\n",
       "      <td>29.224383</td>\n",
       "      <td>131.190941</td>\n",
       "      <td>41.256890</td>\n",
       "      <td>139.938953</td>\n",
       "      <td>71.327693</td>\n",
       "      <td>30.567399</td>\n",
       "      <td>38.536984</td>\n",
       "      <td>51.473024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5930.391480</td>\n",
       "      <td>0.020684</td>\n",
       "      <td>282.502321</td>\n",
       "      <td>5.810059</td>\n",
       "      <td>3728.486813</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>...</td>\n",
       "      <td>105.469401</td>\n",
       "      <td>222.510896</td>\n",
       "      <td>207.738038</td>\n",
       "      <td>80.352971</td>\n",
       "      <td>230.100837</td>\n",
       "      <td>92.210323</td>\n",
       "      <td>47.756320</td>\n",
       "      <td>150.702806</td>\n",
       "      <td>663.318150</td>\n",
       "      <td>65.703425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6955.934960</td>\n",
       "      <td>0.024260</td>\n",
       "      <td>352.964119</td>\n",
       "      <td>5.118652</td>\n",
       "      <td>3978.625608</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>...</td>\n",
       "      <td>465.894665</td>\n",
       "      <td>95.080503</td>\n",
       "      <td>78.408643</td>\n",
       "      <td>85.474455</td>\n",
       "      <td>620.178189</td>\n",
       "      <td>603.220092</td>\n",
       "      <td>98.472945</td>\n",
       "      <td>22.263459</td>\n",
       "      <td>52.502367</td>\n",
       "      <td>90.342676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>9045.979282</td>\n",
       "      <td>0.031550</td>\n",
       "      <td>588.097343</td>\n",
       "      <td>5.393750</td>\n",
       "      <td>5006.275744</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>...</td>\n",
       "      <td>70.957359</td>\n",
       "      <td>21.592368</td>\n",
       "      <td>63.256911</td>\n",
       "      <td>37.571874</td>\n",
       "      <td>15.469279</td>\n",
       "      <td>71.542324</td>\n",
       "      <td>27.100936</td>\n",
       "      <td>18.777073</td>\n",
       "      <td>25.112471</td>\n",
       "      <td>47.132607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>8875.532088</td>\n",
       "      <td>0.030955</td>\n",
       "      <td>532.555278</td>\n",
       "      <td>5.152539</td>\n",
       "      <td>5203.419240</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>...</td>\n",
       "      <td>39.003446</td>\n",
       "      <td>31.982491</td>\n",
       "      <td>23.855374</td>\n",
       "      <td>28.893059</td>\n",
       "      <td>23.465405</td>\n",
       "      <td>42.555156</td>\n",
       "      <td>54.950908</td>\n",
       "      <td>29.127613</td>\n",
       "      <td>24.097623</td>\n",
       "      <td>24.753706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>10592.204025</td>\n",
       "      <td>0.036943</td>\n",
       "      <td>752.144711</td>\n",
       "      <td>5.253613</td>\n",
       "      <td>6415.712933</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>...</td>\n",
       "      <td>152.884534</td>\n",
       "      <td>69.062597</td>\n",
       "      <td>23.951527</td>\n",
       "      <td>24.003702</td>\n",
       "      <td>25.455082</td>\n",
       "      <td>40.894391</td>\n",
       "      <td>49.621631</td>\n",
       "      <td>19.847025</td>\n",
       "      <td>19.994933</td>\n",
       "      <td>24.823230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>9731.159154</td>\n",
       "      <td>0.033940</td>\n",
       "      <td>618.720960</td>\n",
       "      <td>5.091406</td>\n",
       "      <td>5515.909179</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>...</td>\n",
       "      <td>71.992401</td>\n",
       "      <td>82.890722</td>\n",
       "      <td>57.260277</td>\n",
       "      <td>40.922301</td>\n",
       "      <td>75.593420</td>\n",
       "      <td>103.570111</td>\n",
       "      <td>119.230986</td>\n",
       "      <td>24.368567</td>\n",
       "      <td>54.669215</td>\n",
       "      <td>45.409277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>12820.720916</td>\n",
       "      <td>0.044715</td>\n",
       "      <td>1136.042982</td>\n",
       "      <td>4.730762</td>\n",
       "      <td>6590.809282</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>...</td>\n",
       "      <td>48.256554</td>\n",
       "      <td>48.972477</td>\n",
       "      <td>35.419183</td>\n",
       "      <td>89.784736</td>\n",
       "      <td>50.830995</td>\n",
       "      <td>62.833457</td>\n",
       "      <td>151.136037</td>\n",
       "      <td>29.640032</td>\n",
       "      <td>31.596233</td>\n",
       "      <td>75.134168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>10617.751939</td>\n",
       "      <td>0.037032</td>\n",
       "      <td>847.354803</td>\n",
       "      <td>4.915527</td>\n",
       "      <td>5577.078964</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>...</td>\n",
       "      <td>192.018160</td>\n",
       "      <td>98.828400</td>\n",
       "      <td>143.299959</td>\n",
       "      <td>264.546101</td>\n",
       "      <td>110.475223</td>\n",
       "      <td>128.568584</td>\n",
       "      <td>569.618089</td>\n",
       "      <td>40.722672</td>\n",
       "      <td>148.687828</td>\n",
       "      <td>257.921238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>10439.047803</td>\n",
       "      <td>0.036409</td>\n",
       "      <td>850.331477</td>\n",
       "      <td>4.795605</td>\n",
       "      <td>5384.394570</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>...</td>\n",
       "      <td>193.279634</td>\n",
       "      <td>55.559911</td>\n",
       "      <td>64.994975</td>\n",
       "      <td>92.609519</td>\n",
       "      <td>112.456492</td>\n",
       "      <td>68.572813</td>\n",
       "      <td>297.356866</td>\n",
       "      <td>44.128120</td>\n",
       "      <td>82.210209</td>\n",
       "      <td>131.263017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>13985.006084</td>\n",
       "      <td>0.048776</td>\n",
       "      <td>1263.992191</td>\n",
       "      <td>5.063184</td>\n",
       "      <td>7628.842343</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>...</td>\n",
       "      <td>24.186367</td>\n",
       "      <td>59.761577</td>\n",
       "      <td>40.005262</td>\n",
       "      <td>46.190406</td>\n",
       "      <td>30.271866</td>\n",
       "      <td>25.461142</td>\n",
       "      <td>70.599621</td>\n",
       "      <td>25.570931</td>\n",
       "      <td>42.177811</td>\n",
       "      <td>97.192214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>10257.986061</td>\n",
       "      <td>0.035777</td>\n",
       "      <td>667.682308</td>\n",
       "      <td>4.768555</td>\n",
       "      <td>5383.309874</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>...</td>\n",
       "      <td>17.303333</td>\n",
       "      <td>46.896767</td>\n",
       "      <td>46.745473</td>\n",
       "      <td>26.370703</td>\n",
       "      <td>28.492148</td>\n",
       "      <td>22.010739</td>\n",
       "      <td>32.706802</td>\n",
       "      <td>49.447835</td>\n",
       "      <td>32.097283</td>\n",
       "      <td>31.151592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>10763.072725</td>\n",
       "      <td>0.037539</td>\n",
       "      <td>757.483042</td>\n",
       "      <td>4.927344</td>\n",
       "      <td>6048.310204</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>...</td>\n",
       "      <td>339.470892</td>\n",
       "      <td>570.541843</td>\n",
       "      <td>182.252893</td>\n",
       "      <td>36.882279</td>\n",
       "      <td>321.256111</td>\n",
       "      <td>220.345239</td>\n",
       "      <td>127.329866</td>\n",
       "      <td>25.578953</td>\n",
       "      <td>29.518323</td>\n",
       "      <td>16.797806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 572 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    session  participant  gesture  index          iemg       mav  \\\n",
       "0         1            1       10      0  11996.863087  0.041842   \n",
       "1         1            1       10      0  10550.150337  0.036796   \n",
       "2         1            1       10      0   9230.366582  0.032193   \n",
       "3         1            1       10      0  15949.504704  0.055627   \n",
       "4         1            1       10      0  15936.956936  0.055584   \n",
       "5         1            1       10      0  10302.082204  0.035931   \n",
       "6         1            1       10      0  19771.041940  0.068956   \n",
       "7         1            1       11      0  28104.586469  0.098021   \n",
       "8         1            1       11      0  27253.345839  0.095052   \n",
       "9         1            1       11      0  24522.650663  0.085528   \n",
       "10        1            1       11      0  31742.205936  0.110708   \n",
       "11        1            1       11      0  36412.348291  0.126996   \n",
       "12        1            1       11      0  29334.356528  0.102310   \n",
       "13        1            1       11      0  40562.091730  0.141469   \n",
       "14        1            1       12      0   7304.356544  0.025476   \n",
       "15        1            1       12      0   5195.415342  0.018120   \n",
       "16        1            1       12      0   5010.829409  0.017476   \n",
       "17        1            1       12      0   7350.757597  0.025637   \n",
       "18        1            1       12      0   5930.391480  0.020684   \n",
       "19        1            1       12      0   6955.934960  0.024260   \n",
       "20        1            1       12      0   9045.979282  0.031550   \n",
       "21        1            1       13      0   8875.532088  0.030955   \n",
       "22        1            1       13      0  10592.204025  0.036943   \n",
       "23        1            1       13      0   9731.159154  0.033940   \n",
       "24        1            1       13      0  12820.720916  0.044715   \n",
       "25        1            1       13      0  10617.751939  0.037032   \n",
       "26        1            1       13      0  10439.047803  0.036409   \n",
       "27        1            1       13      0  13985.006084  0.048776   \n",
       "28        1            1       14      0  10257.986061  0.035777   \n",
       "29        1            1       14      0  10763.072725  0.037539   \n",
       "\n",
       "             ssi  myopulse         wflen   diffvar  ...  kurtosis_f_w3  \\\n",
       "0     997.135823  6.224512   8143.485828  0.001888  ...      61.940683   \n",
       "1    1019.627142  6.443555   6646.721181  0.001438  ...     160.171639   \n",
       "2     630.013233  6.033789   6023.438546  0.001169  ...     195.666850   \n",
       "3    1995.234560  4.999609   9318.559036  0.003030  ...      33.886272   \n",
       "4    1830.412922  5.870117  10476.465579  0.003688  ...      48.630658   \n",
       "5     957.747699  5.384570   6660.235040  0.002048  ...     194.169246   \n",
       "6    2959.381173  5.280957  11976.937109  0.005174  ...     113.893317   \n",
       "7    7096.757592  4.965234  12613.872878  0.006058  ...     140.633258   \n",
       "8    6467.184093  4.996094  12258.059044  0.005827  ...      52.195250   \n",
       "9    4589.346795  4.919336  11242.445344  0.004575  ...      38.664579   \n",
       "10   7667.393736  4.846484  14106.465485  0.007467  ...      15.008300   \n",
       "11  10261.995480  4.799121  15714.702877  0.009415  ...      13.195526   \n",
       "12   7696.291516  4.863770  13094.098954  0.007279  ...      26.360612   \n",
       "13  11729.491266  4.848145  17725.698335  0.011385  ...      14.889469   \n",
       "14    414.201641  6.174902   4752.641057  0.000673  ...      63.079901   \n",
       "15    231.959814  6.103418   3131.817615  0.000335  ...      65.755548   \n",
       "16    203.733441  5.874609   2787.482041  0.000228  ...      45.348221   \n",
       "17    383.271227  4.712891   3837.262402  0.000459  ...      45.747711   \n",
       "18    282.502321  5.810059   3728.486813  0.000448  ...     105.469401   \n",
       "19    352.964119  5.118652   3978.625608  0.000541  ...     465.894665   \n",
       "20    588.097343  5.393750   5006.275744  0.000723  ...      70.957359   \n",
       "21    532.555278  5.152539   5203.419240  0.000852  ...      39.003446   \n",
       "22    752.144711  5.253613   6415.712933  0.001356  ...     152.884534   \n",
       "23    618.720960  5.091406   5515.909179  0.000911  ...      71.992401   \n",
       "24   1136.042982  4.730762   6590.809282  0.001517  ...      48.256554   \n",
       "25    847.354803  4.915527   5577.078964  0.001127  ...     192.018160   \n",
       "26    850.331477  4.795605   5384.394570  0.001039  ...     193.279634   \n",
       "27   1263.992191  5.063184   7628.842343  0.001858  ...      24.186367   \n",
       "28    667.682308  4.768555   5383.309874  0.000852  ...      17.303333   \n",
       "29    757.483042  4.927344   6048.310204  0.001215  ...     339.470892   \n",
       "\n",
       "    kurtosis_f_w4  kurtosis_f_w5  kurtosis_f_w6  kurtosis_f_w7  kurtosis_f_w8  \\\n",
       "0      116.130710      31.371157     107.176549      64.459302     241.232060   \n",
       "1       87.566832     154.616093     280.595181     160.756112      98.589493   \n",
       "2       90.568034      94.299202     276.105575     101.698579      49.607516   \n",
       "3       80.377706      46.373047      64.445165      31.711430      33.328942   \n",
       "4       39.722917      25.079065      84.877336      29.229362      48.843301   \n",
       "5      160.867419     127.623463     144.913630     157.298899      94.204471   \n",
       "6       54.651356      46.359047      96.793045      54.653056     182.248943   \n",
       "7       44.404009      68.097183      49.395457     147.219607      45.064573   \n",
       "8       43.058702      29.454097      74.433499      48.285962      29.769905   \n",
       "9       39.118622      30.194664      28.584404      38.329797      46.371623   \n",
       "10      27.062279      27.067901      16.489398      18.553551      30.774797   \n",
       "11      29.477076      22.342876      25.260824      46.858799      42.441574   \n",
       "12      21.980495      33.952634      28.771759      23.137365      29.683661   \n",
       "13      24.544620      16.127072      21.020319      21.517667      35.291645   \n",
       "14      82.825368      93.750642     340.397936      40.822826      64.858434   \n",
       "15      95.883174      75.837115     110.616004      43.524357      61.941302   \n",
       "16      41.100077      31.168374      56.728636      17.881988      23.522819   \n",
       "17      74.443871      29.224383     131.190941      41.256890     139.938953   \n",
       "18     222.510896     207.738038      80.352971     230.100837      92.210323   \n",
       "19      95.080503      78.408643      85.474455     620.178189     603.220092   \n",
       "20      21.592368      63.256911      37.571874      15.469279      71.542324   \n",
       "21      31.982491      23.855374      28.893059      23.465405      42.555156   \n",
       "22      69.062597      23.951527      24.003702      25.455082      40.894391   \n",
       "23      82.890722      57.260277      40.922301      75.593420     103.570111   \n",
       "24      48.972477      35.419183      89.784736      50.830995      62.833457   \n",
       "25      98.828400     143.299959     264.546101     110.475223     128.568584   \n",
       "26      55.559911      64.994975      92.609519     112.456492      68.572813   \n",
       "27      59.761577      40.005262      46.190406      30.271866      25.461142   \n",
       "28      46.896767      46.745473      26.370703      28.492148      22.010739   \n",
       "29     570.541843     182.252893      36.882279     321.256111     220.345239   \n",
       "\n",
       "    kurtosis_f_w9  kurtosis_f_w10  kurtosis_f_w11  kurtosis_f_w12  \n",
       "0       54.880235       56.951175       42.644289      118.918727  \n",
       "1      135.655396      194.396727      294.858908      537.374741  \n",
       "2      150.348389      151.177847      156.860637      115.905382  \n",
       "3       50.491362       81.876212       69.070232      142.723324  \n",
       "4       44.917769       39.354396       78.705208      176.820713  \n",
       "5      155.422302       54.005036       53.646785       61.060966  \n",
       "6       65.594571       81.105593       66.837683       61.510619  \n",
       "7      157.636975       59.939376       66.865776       91.715022  \n",
       "8      114.525041       80.717165       52.239566       94.334606  \n",
       "9       34.716442       31.226083       49.508191       34.297186  \n",
       "10      54.415505       46.828774       42.710088       52.793400  \n",
       "11      57.733548       79.930363       62.170107      103.636508  \n",
       "12      50.666970       46.655760       44.238902       47.857019  \n",
       "13      36.992259       32.222933       55.959554       38.280952  \n",
       "14     105.261496       43.192037      101.315301       99.887088  \n",
       "15      38.202959       31.752584       62.789179       34.244100  \n",
       "16      45.925652       26.525906       26.392454       36.021888  \n",
       "17      71.327693       30.567399       38.536984       51.473024  \n",
       "18      47.756320      150.702806      663.318150       65.703425  \n",
       "19      98.472945       22.263459       52.502367       90.342676  \n",
       "20      27.100936       18.777073       25.112471       47.132607  \n",
       "21      54.950908       29.127613       24.097623       24.753706  \n",
       "22      49.621631       19.847025       19.994933       24.823230  \n",
       "23     119.230986       24.368567       54.669215       45.409277  \n",
       "24     151.136037       29.640032       31.596233       75.134168  \n",
       "25     569.618089       40.722672      148.687828      257.921238  \n",
       "26     297.356866       44.128120       82.210209      131.263017  \n",
       "27      70.599621       25.570931       42.177811       97.192214  \n",
       "28      32.706802       49.447835       32.097283       31.151592  \n",
       "29     127.329866       25.578953       29.518323       16.797806  \n",
       "\n",
       "[30 rows x 572 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5e2ff7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "455e56c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iemg</th>\n",
       "      <th>mav</th>\n",
       "      <th>ssi</th>\n",
       "      <th>myopulse</th>\n",
       "      <th>wflen</th>\n",
       "      <th>diffvar</th>\n",
       "      <th>dasd</th>\n",
       "      <th>willison</th>\n",
       "      <th>power_f1</th>\n",
       "      <th>power_f2</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis_f_w3</th>\n",
       "      <th>kurtosis_f_w4</th>\n",
       "      <th>kurtosis_f_w5</th>\n",
       "      <th>kurtosis_f_w6</th>\n",
       "      <th>kurtosis_f_w7</th>\n",
       "      <th>kurtosis_f_w8</th>\n",
       "      <th>kurtosis_f_w9</th>\n",
       "      <th>kurtosis_f_w10</th>\n",
       "      <th>kurtosis_f_w11</th>\n",
       "      <th>kurtosis_f_w12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11996.863087</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>997.135823</td>\n",
       "      <td>6.224512</td>\n",
       "      <td>8143.485828</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>10277</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>...</td>\n",
       "      <td>61.940683</td>\n",
       "      <td>116.130710</td>\n",
       "      <td>31.371157</td>\n",
       "      <td>107.176549</td>\n",
       "      <td>64.459302</td>\n",
       "      <td>241.232060</td>\n",
       "      <td>54.880235</td>\n",
       "      <td>56.951175</td>\n",
       "      <td>42.644289</td>\n",
       "      <td>118.918727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10550.150337</td>\n",
       "      <td>0.036796</td>\n",
       "      <td>1019.627142</td>\n",
       "      <td>6.443555</td>\n",
       "      <td>6646.721181</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.030668</td>\n",
       "      <td>6420</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>...</td>\n",
       "      <td>160.171639</td>\n",
       "      <td>87.566832</td>\n",
       "      <td>154.616093</td>\n",
       "      <td>280.595181</td>\n",
       "      <td>160.756112</td>\n",
       "      <td>98.589493</td>\n",
       "      <td>135.655396</td>\n",
       "      <td>194.396727</td>\n",
       "      <td>294.858908</td>\n",
       "      <td>537.374741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9230.366582</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>630.013233</td>\n",
       "      <td>6.033789</td>\n",
       "      <td>6023.438546</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>...</td>\n",
       "      <td>195.666850</td>\n",
       "      <td>90.568034</td>\n",
       "      <td>94.299202</td>\n",
       "      <td>276.105575</td>\n",
       "      <td>101.698579</td>\n",
       "      <td>49.607516</td>\n",
       "      <td>150.348389</td>\n",
       "      <td>151.177847</td>\n",
       "      <td>156.860637</td>\n",
       "      <td>115.905382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15949.504704</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>1995.234560</td>\n",
       "      <td>4.999609</td>\n",
       "      <td>9318.559036</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.045251</td>\n",
       "      <td>19492</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>...</td>\n",
       "      <td>33.886272</td>\n",
       "      <td>80.377706</td>\n",
       "      <td>46.373047</td>\n",
       "      <td>64.445165</td>\n",
       "      <td>31.711430</td>\n",
       "      <td>33.328942</td>\n",
       "      <td>50.491362</td>\n",
       "      <td>81.876212</td>\n",
       "      <td>69.070232</td>\n",
       "      <td>142.723324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15936.956936</td>\n",
       "      <td>0.055584</td>\n",
       "      <td>1830.412922</td>\n",
       "      <td>5.870117</td>\n",
       "      <td>10476.465579</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.047223</td>\n",
       "      <td>21471</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>...</td>\n",
       "      <td>48.630658</td>\n",
       "      <td>39.722917</td>\n",
       "      <td>25.079065</td>\n",
       "      <td>84.877336</td>\n",
       "      <td>29.229362</td>\n",
       "      <td>48.843301</td>\n",
       "      <td>44.917769</td>\n",
       "      <td>39.354396</td>\n",
       "      <td>78.705208</td>\n",
       "      <td>176.820713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15346</th>\n",
       "      <td>7435.557234</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>369.818221</td>\n",
       "      <td>6.280762</td>\n",
       "      <td>5792.222983</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>5307</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>...</td>\n",
       "      <td>29.589425</td>\n",
       "      <td>25.460694</td>\n",
       "      <td>182.962449</td>\n",
       "      <td>34.957161</td>\n",
       "      <td>18.846525</td>\n",
       "      <td>37.714119</td>\n",
       "      <td>26.569683</td>\n",
       "      <td>32.614845</td>\n",
       "      <td>12.793219</td>\n",
       "      <td>16.657922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15347</th>\n",
       "      <td>10471.625678</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>713.052200</td>\n",
       "      <td>6.967187</td>\n",
       "      <td>8628.380136</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>11624</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>18.226247</td>\n",
       "      <td>35.715660</td>\n",
       "      <td>113.822913</td>\n",
       "      <td>64.223311</td>\n",
       "      <td>8.214120</td>\n",
       "      <td>39.211218</td>\n",
       "      <td>29.910432</td>\n",
       "      <td>26.426457</td>\n",
       "      <td>24.892159</td>\n",
       "      <td>82.241098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15348</th>\n",
       "      <td>11279.138159</td>\n",
       "      <td>0.039339</td>\n",
       "      <td>815.440185</td>\n",
       "      <td>6.917773</td>\n",
       "      <td>9229.795138</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>14370</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>...</td>\n",
       "      <td>15.486525</td>\n",
       "      <td>29.417342</td>\n",
       "      <td>25.153035</td>\n",
       "      <td>19.632020</td>\n",
       "      <td>15.590478</td>\n",
       "      <td>15.462391</td>\n",
       "      <td>34.940941</td>\n",
       "      <td>37.564280</td>\n",
       "      <td>14.208021</td>\n",
       "      <td>15.811267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15349</th>\n",
       "      <td>9210.946156</td>\n",
       "      <td>0.032125</td>\n",
       "      <td>563.422838</td>\n",
       "      <td>7.359570</td>\n",
       "      <td>8040.357766</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.033699</td>\n",
       "      <td>10798</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>...</td>\n",
       "      <td>14.527208</td>\n",
       "      <td>20.281059</td>\n",
       "      <td>20.418717</td>\n",
       "      <td>18.471741</td>\n",
       "      <td>14.276295</td>\n",
       "      <td>10.083487</td>\n",
       "      <td>27.337412</td>\n",
       "      <td>12.804675</td>\n",
       "      <td>20.220452</td>\n",
       "      <td>37.494222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15350</th>\n",
       "      <td>9557.645293</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>622.632313</td>\n",
       "      <td>7.815820</td>\n",
       "      <td>8616.136489</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.035907</td>\n",
       "      <td>11599</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>...</td>\n",
       "      <td>28.617196</td>\n",
       "      <td>17.396729</td>\n",
       "      <td>21.821603</td>\n",
       "      <td>27.706896</td>\n",
       "      <td>16.773846</td>\n",
       "      <td>23.562969</td>\n",
       "      <td>20.576495</td>\n",
       "      <td>21.367926</td>\n",
       "      <td>12.371417</td>\n",
       "      <td>39.203146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15351 rows Ã— 568 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               iemg       mav          ssi  myopulse         wflen   diffvar  \\\n",
       "0      11996.863087  0.041842   997.135823  6.224512   8143.485828  0.001888   \n",
       "1      10550.150337  0.036796  1019.627142  6.443555   6646.721181  0.001438   \n",
       "2       9230.366582  0.032193   630.013233  6.033789   6023.438546  0.001169   \n",
       "3      15949.504704  0.055627  1995.234560  4.999609   9318.559036  0.003030   \n",
       "4      15936.956936  0.055584  1830.412922  5.870117  10476.465579  0.003688   \n",
       "...             ...       ...          ...       ...           ...       ...   \n",
       "15346   7435.557234  0.025933   369.818221  6.280762   5792.222983  0.001070   \n",
       "15347  10471.625678  0.036522   713.052200  6.967187   8628.380136  0.002070   \n",
       "15348  11279.138159  0.039339   815.440185  6.917773   9229.795138  0.002355   \n",
       "15349   9210.946156  0.032125   563.422838  7.359570   8040.357766  0.001839   \n",
       "15350   9557.645293  0.033334   622.632313  7.815820   8616.136489  0.002036   \n",
       "\n",
       "           dasd  willison  power_f1  power_f2  ...  kurtosis_f_w3  \\\n",
       "0      0.034480     10277  0.002297  0.004143  ...      61.940683   \n",
       "1      0.030668      6420  0.003276  0.005005  ...     160.171639   \n",
       "2      0.027090      5052  0.001570  0.002424  ...     195.666850   \n",
       "3      0.045251     19492  0.004655  0.006518  ...      33.886272   \n",
       "4      0.047223     21471  0.003665  0.005650  ...      48.630658   \n",
       "...         ...       ...       ...       ...  ...            ...   \n",
       "15346  0.026210      5307  0.001265  0.000794  ...      29.589425   \n",
       "15347  0.035779     11624  0.002672  0.002797  ...      18.226247   \n",
       "15348  0.038498     14370  0.002550  0.002242  ...      15.486525   \n",
       "15349  0.033699     10798  0.001746  0.001153  ...      14.527208   \n",
       "15350  0.035907     11599  0.001881  0.002216  ...      28.617196   \n",
       "\n",
       "       kurtosis_f_w4  kurtosis_f_w5  kurtosis_f_w6  kurtosis_f_w7  \\\n",
       "0         116.130710      31.371157     107.176549      64.459302   \n",
       "1          87.566832     154.616093     280.595181     160.756112   \n",
       "2          90.568034      94.299202     276.105575     101.698579   \n",
       "3          80.377706      46.373047      64.445165      31.711430   \n",
       "4          39.722917      25.079065      84.877336      29.229362   \n",
       "...              ...            ...            ...            ...   \n",
       "15346      25.460694     182.962449      34.957161      18.846525   \n",
       "15347      35.715660     113.822913      64.223311       8.214120   \n",
       "15348      29.417342      25.153035      19.632020      15.590478   \n",
       "15349      20.281059      20.418717      18.471741      14.276295   \n",
       "15350      17.396729      21.821603      27.706896      16.773846   \n",
       "\n",
       "       kurtosis_f_w8  kurtosis_f_w9  kurtosis_f_w10  kurtosis_f_w11  \\\n",
       "0         241.232060      54.880235       56.951175       42.644289   \n",
       "1          98.589493     135.655396      194.396727      294.858908   \n",
       "2          49.607516     150.348389      151.177847      156.860637   \n",
       "3          33.328942      50.491362       81.876212       69.070232   \n",
       "4          48.843301      44.917769       39.354396       78.705208   \n",
       "...              ...            ...             ...             ...   \n",
       "15346      37.714119      26.569683       32.614845       12.793219   \n",
       "15347      39.211218      29.910432       26.426457       24.892159   \n",
       "15348      15.462391      34.940941       37.564280       14.208021   \n",
       "15349      10.083487      27.337412       12.804675       20.220452   \n",
       "15350      23.562969      20.576495       21.367926       12.371417   \n",
       "\n",
       "       kurtosis_f_w12  \n",
       "0          118.918727  \n",
       "1          537.374741  \n",
       "2          115.905382  \n",
       "3          142.723324  \n",
       "4          176.820713  \n",
       "...               ...  \n",
       "15346       16.657922  \n",
       "15347       82.241098  \n",
       "15348       15.811267  \n",
       "15349       37.494222  \n",
       "15350       39.203146  \n",
       "\n",
       "[15351 rows x 568 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0d588c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.iloc[:, 2] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c4519fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 10, 11, 12, 13, 14, 15, 16,  0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "06f8904c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        9\n",
       "1        9\n",
       "2        9\n",
       "3        9\n",
       "4        9\n",
       "        ..\n",
       "15346    8\n",
       "15347    8\n",
       "15348    8\n",
       "15349    8\n",
       "15350    8\n",
       "Name: gesture, Length: 15351, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "75432f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iemg</th>\n",
       "      <th>mav</th>\n",
       "      <th>ssi</th>\n",
       "      <th>myopulse</th>\n",
       "      <th>wflen</th>\n",
       "      <th>diffvar</th>\n",
       "      <th>dasd</th>\n",
       "      <th>willison</th>\n",
       "      <th>power_f1</th>\n",
       "      <th>power_f2</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis_f_w3</th>\n",
       "      <th>kurtosis_f_w4</th>\n",
       "      <th>kurtosis_f_w5</th>\n",
       "      <th>kurtosis_f_w6</th>\n",
       "      <th>kurtosis_f_w7</th>\n",
       "      <th>kurtosis_f_w8</th>\n",
       "      <th>kurtosis_f_w9</th>\n",
       "      <th>kurtosis_f_w10</th>\n",
       "      <th>kurtosis_f_w11</th>\n",
       "      <th>kurtosis_f_w12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11996.863087</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>997.135823</td>\n",
       "      <td>6.224512</td>\n",
       "      <td>8143.485828</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>10277</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>...</td>\n",
       "      <td>61.940683</td>\n",
       "      <td>116.130710</td>\n",
       "      <td>31.371157</td>\n",
       "      <td>107.176549</td>\n",
       "      <td>64.459302</td>\n",
       "      <td>241.232060</td>\n",
       "      <td>54.880235</td>\n",
       "      <td>56.951175</td>\n",
       "      <td>42.644289</td>\n",
       "      <td>118.918727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10550.150337</td>\n",
       "      <td>0.036796</td>\n",
       "      <td>1019.627142</td>\n",
       "      <td>6.443555</td>\n",
       "      <td>6646.721181</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.030668</td>\n",
       "      <td>6420</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.005005</td>\n",
       "      <td>...</td>\n",
       "      <td>160.171639</td>\n",
       "      <td>87.566832</td>\n",
       "      <td>154.616093</td>\n",
       "      <td>280.595181</td>\n",
       "      <td>160.756112</td>\n",
       "      <td>98.589493</td>\n",
       "      <td>135.655396</td>\n",
       "      <td>194.396727</td>\n",
       "      <td>294.858908</td>\n",
       "      <td>537.374741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9230.366582</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>630.013233</td>\n",
       "      <td>6.033789</td>\n",
       "      <td>6023.438546</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>5052</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.002424</td>\n",
       "      <td>...</td>\n",
       "      <td>195.666850</td>\n",
       "      <td>90.568034</td>\n",
       "      <td>94.299202</td>\n",
       "      <td>276.105575</td>\n",
       "      <td>101.698579</td>\n",
       "      <td>49.607516</td>\n",
       "      <td>150.348389</td>\n",
       "      <td>151.177847</td>\n",
       "      <td>156.860637</td>\n",
       "      <td>115.905382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15949.504704</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>1995.234560</td>\n",
       "      <td>4.999609</td>\n",
       "      <td>9318.559036</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>0.045251</td>\n",
       "      <td>19492</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>...</td>\n",
       "      <td>33.886272</td>\n",
       "      <td>80.377706</td>\n",
       "      <td>46.373047</td>\n",
       "      <td>64.445165</td>\n",
       "      <td>31.711430</td>\n",
       "      <td>33.328942</td>\n",
       "      <td>50.491362</td>\n",
       "      <td>81.876212</td>\n",
       "      <td>69.070232</td>\n",
       "      <td>142.723324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15936.956936</td>\n",
       "      <td>0.055584</td>\n",
       "      <td>1830.412922</td>\n",
       "      <td>5.870117</td>\n",
       "      <td>10476.465579</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>0.047223</td>\n",
       "      <td>21471</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>...</td>\n",
       "      <td>48.630658</td>\n",
       "      <td>39.722917</td>\n",
       "      <td>25.079065</td>\n",
       "      <td>84.877336</td>\n",
       "      <td>29.229362</td>\n",
       "      <td>48.843301</td>\n",
       "      <td>44.917769</td>\n",
       "      <td>39.354396</td>\n",
       "      <td>78.705208</td>\n",
       "      <td>176.820713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15346</th>\n",
       "      <td>7435.557234</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>369.818221</td>\n",
       "      <td>6.280762</td>\n",
       "      <td>5792.222983</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>5307</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>...</td>\n",
       "      <td>29.589425</td>\n",
       "      <td>25.460694</td>\n",
       "      <td>182.962449</td>\n",
       "      <td>34.957161</td>\n",
       "      <td>18.846525</td>\n",
       "      <td>37.714119</td>\n",
       "      <td>26.569683</td>\n",
       "      <td>32.614845</td>\n",
       "      <td>12.793219</td>\n",
       "      <td>16.657922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15347</th>\n",
       "      <td>10471.625678</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>713.052200</td>\n",
       "      <td>6.967187</td>\n",
       "      <td>8628.380136</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.035779</td>\n",
       "      <td>11624</td>\n",
       "      <td>0.002672</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>...</td>\n",
       "      <td>18.226247</td>\n",
       "      <td>35.715660</td>\n",
       "      <td>113.822913</td>\n",
       "      <td>64.223311</td>\n",
       "      <td>8.214120</td>\n",
       "      <td>39.211218</td>\n",
       "      <td>29.910432</td>\n",
       "      <td>26.426457</td>\n",
       "      <td>24.892159</td>\n",
       "      <td>82.241098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15348</th>\n",
       "      <td>11279.138159</td>\n",
       "      <td>0.039339</td>\n",
       "      <td>815.440185</td>\n",
       "      <td>6.917773</td>\n",
       "      <td>9229.795138</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>0.038498</td>\n",
       "      <td>14370</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>...</td>\n",
       "      <td>15.486525</td>\n",
       "      <td>29.417342</td>\n",
       "      <td>25.153035</td>\n",
       "      <td>19.632020</td>\n",
       "      <td>15.590478</td>\n",
       "      <td>15.462391</td>\n",
       "      <td>34.940941</td>\n",
       "      <td>37.564280</td>\n",
       "      <td>14.208021</td>\n",
       "      <td>15.811267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15349</th>\n",
       "      <td>9210.946156</td>\n",
       "      <td>0.032125</td>\n",
       "      <td>563.422838</td>\n",
       "      <td>7.359570</td>\n",
       "      <td>8040.357766</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.033699</td>\n",
       "      <td>10798</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>...</td>\n",
       "      <td>14.527208</td>\n",
       "      <td>20.281059</td>\n",
       "      <td>20.418717</td>\n",
       "      <td>18.471741</td>\n",
       "      <td>14.276295</td>\n",
       "      <td>10.083487</td>\n",
       "      <td>27.337412</td>\n",
       "      <td>12.804675</td>\n",
       "      <td>20.220452</td>\n",
       "      <td>37.494222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15350</th>\n",
       "      <td>9557.645293</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>622.632313</td>\n",
       "      <td>7.815820</td>\n",
       "      <td>8616.136489</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.035907</td>\n",
       "      <td>11599</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>...</td>\n",
       "      <td>28.617196</td>\n",
       "      <td>17.396729</td>\n",
       "      <td>21.821603</td>\n",
       "      <td>27.706896</td>\n",
       "      <td>16.773846</td>\n",
       "      <td>23.562969</td>\n",
       "      <td>20.576495</td>\n",
       "      <td>21.367926</td>\n",
       "      <td>12.371417</td>\n",
       "      <td>39.203146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15351 rows Ã— 568 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               iemg       mav          ssi  myopulse         wflen   diffvar  \\\n",
       "0      11996.863087  0.041842   997.135823  6.224512   8143.485828  0.001888   \n",
       "1      10550.150337  0.036796  1019.627142  6.443555   6646.721181  0.001438   \n",
       "2       9230.366582  0.032193   630.013233  6.033789   6023.438546  0.001169   \n",
       "3      15949.504704  0.055627  1995.234560  4.999609   9318.559036  0.003030   \n",
       "4      15936.956936  0.055584  1830.412922  5.870117  10476.465579  0.003688   \n",
       "...             ...       ...          ...       ...           ...       ...   \n",
       "15346   7435.557234  0.025933   369.818221  6.280762   5792.222983  0.001070   \n",
       "15347  10471.625678  0.036522   713.052200  6.967187   8628.380136  0.002070   \n",
       "15348  11279.138159  0.039339   815.440185  6.917773   9229.795138  0.002355   \n",
       "15349   9210.946156  0.032125   563.422838  7.359570   8040.357766  0.001839   \n",
       "15350   9557.645293  0.033334   622.632313  7.815820   8616.136489  0.002036   \n",
       "\n",
       "           dasd  willison  power_f1  power_f2  ...  kurtosis_f_w3  \\\n",
       "0      0.034480     10277  0.002297  0.004143  ...      61.940683   \n",
       "1      0.030668      6420  0.003276  0.005005  ...     160.171639   \n",
       "2      0.027090      5052  0.001570  0.002424  ...     195.666850   \n",
       "3      0.045251     19492  0.004655  0.006518  ...      33.886272   \n",
       "4      0.047223     21471  0.003665  0.005650  ...      48.630658   \n",
       "...         ...       ...       ...       ...  ...            ...   \n",
       "15346  0.026210      5307  0.001265  0.000794  ...      29.589425   \n",
       "15347  0.035779     11624  0.002672  0.002797  ...      18.226247   \n",
       "15348  0.038498     14370  0.002550  0.002242  ...      15.486525   \n",
       "15349  0.033699     10798  0.001746  0.001153  ...      14.527208   \n",
       "15350  0.035907     11599  0.001881  0.002216  ...      28.617196   \n",
       "\n",
       "       kurtosis_f_w4  kurtosis_f_w5  kurtosis_f_w6  kurtosis_f_w7  \\\n",
       "0         116.130710      31.371157     107.176549      64.459302   \n",
       "1          87.566832     154.616093     280.595181     160.756112   \n",
       "2          90.568034      94.299202     276.105575     101.698579   \n",
       "3          80.377706      46.373047      64.445165      31.711430   \n",
       "4          39.722917      25.079065      84.877336      29.229362   \n",
       "...              ...            ...            ...            ...   \n",
       "15346      25.460694     182.962449      34.957161      18.846525   \n",
       "15347      35.715660     113.822913      64.223311       8.214120   \n",
       "15348      29.417342      25.153035      19.632020      15.590478   \n",
       "15349      20.281059      20.418717      18.471741      14.276295   \n",
       "15350      17.396729      21.821603      27.706896      16.773846   \n",
       "\n",
       "       kurtosis_f_w8  kurtosis_f_w9  kurtosis_f_w10  kurtosis_f_w11  \\\n",
       "0         241.232060      54.880235       56.951175       42.644289   \n",
       "1          98.589493     135.655396      194.396727      294.858908   \n",
       "2          49.607516     150.348389      151.177847      156.860637   \n",
       "3          33.328942      50.491362       81.876212       69.070232   \n",
       "4          48.843301      44.917769       39.354396       78.705208   \n",
       "...              ...            ...             ...             ...   \n",
       "15346      37.714119      26.569683       32.614845       12.793219   \n",
       "15347      39.211218      29.910432       26.426457       24.892159   \n",
       "15348      15.462391      34.940941       37.564280       14.208021   \n",
       "15349      10.083487      27.337412       12.804675       20.220452   \n",
       "15350      23.562969      20.576495       21.367926       12.371417   \n",
       "\n",
       "       kurtosis_f_w12  \n",
       "0          118.918727  \n",
       "1          537.374741  \n",
       "2          115.905382  \n",
       "3          142.723324  \n",
       "4          176.820713  \n",
       "...               ...  \n",
       "15346       16.657922  \n",
       "15347       82.241098  \n",
       "15348       15.811267  \n",
       "15349       37.494222  \n",
       "15350       39.203146  \n",
       "\n",
       "[15351 rows x 568 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "17c3aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(features)\n",
    "Y = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2848eec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15351,)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ac19b9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15351, 568)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f610507a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02392332,  0.02392332, -0.1874717 , ..., -0.04878718,\n",
       "        -0.11424713,  0.38753344],\n",
       "       [-0.18785798, -0.18785798, -0.17863791, ...,  1.18393595,\n",
       "         2.47507057,  3.60160943],\n",
       "       [-0.38105841, -0.38105841, -0.3316644 , ...,  0.796314  ,\n",
       "         1.05833522,  0.36438855],\n",
       "       ...,\n",
       "       [-0.08114296, -0.08114296, -0.25883529, ..., -0.22266456,\n",
       "        -0.40618315, -0.40441409],\n",
       "       [-0.38390132, -0.38390132, -0.35781874, ..., -0.44472878,\n",
       "        -0.34445757, -0.2378717 ],\n",
       "       [-0.33314875, -0.33314875, -0.33456336, ..., -0.3679266 ,\n",
       "        -0.42503833, -0.2247458 ]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9c436891",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "74246175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11629552, -0.11629552, -0.19289876, ..., -0.19079338,\n",
       "        -0.21617383, -0.23099812],\n",
       "       [ 3.45405924,  3.45405924,  3.97590219, ..., -0.3774615 ,\n",
       "        -0.37834971, -0.37972356],\n",
       "       [ 0.19801118,  0.19801118, -0.11286955, ..., -0.28453885,\n",
       "        -0.32990027, -0.21279221],\n",
       "       ...,\n",
       "       [ 0.09244361,  0.09244361, -0.10603224, ...,  0.8706434 ,\n",
       "        -0.03801353,  0.09171287],\n",
       "       [-0.19932551, -0.19932551, -0.29204992, ..., -0.3768488 ,\n",
       "        -0.17473026, -0.38905995],\n",
       "       [-0.30814251, -0.30814251, -0.30755708, ..., -0.01050093,\n",
       "        -0.23205439, -0.20590404]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ea7fa6b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27462652, -0.27462652, -0.3221948 , ..., -0.45198375,\n",
       "        -0.31674569, -0.41387085],\n",
       "       [-0.1133239 , -0.1133239 , -0.24361123, ...,  0.32942267,\n",
       "        -0.21838578, -0.27355456],\n",
       "       [ 2.71427156,  2.71427156,  2.38994258, ..., -0.15937449,\n",
       "         0.22752784,  0.39048761],\n",
       "       ...,\n",
       "       [ 0.83047173,  0.83047173,  0.28591516, ...,  0.01952809,\n",
       "        -0.26177294, -0.23809514],\n",
       "       [-0.18717363, -0.18717363, -0.28588523, ..., -0.31540792,\n",
       "        -0.35467747, -0.33105603],\n",
       "       [-0.13234052, -0.13234052, -0.21027761, ...,  0.64867224,\n",
       "        -0.18352401,  0.04790248]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7f6252bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train.values\n",
    "# x_test = x_test.values\n",
    "\n",
    "# y_train = y_train.values\n",
    "# y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "249483d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 6, ..., 6, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d57dd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, output_shape):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_shape, 32),  # Input layer: Fully connected (linear) with 64 units\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(64, output_shape)  # Output layer: Fully connected (linear) with 'output_shape' units\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1ca74a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(x_train).type(torch.float32), torch.tensor(y_train).type(torch.LongTensor))\n",
    "test_dataset = TensorDataset(torch.tensor(x_test).type(torch.float32), torch.tensor(y_test).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1104d92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "89752b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and whether to shuffle the data\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "6910964d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12280"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2443ad47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12280, 568)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ee0d6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model(x_train.shape[1], len(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef7072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8f511bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "09d8bf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f389dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Train Loss: 1.4938, Val Loss: 1.0819, Val Accuracy: 63.79%\n",
      "Epoch [2/50] - Train Loss: 0.8986, Val Loss: 0.9366, Val Accuracy: 69.16%\n",
      "Epoch [3/50] - Train Loss: 0.7881, Val Loss: 0.8759, Val Accuracy: 72.45%\n",
      "Epoch [4/50] - Train Loss: 0.7137, Val Loss: 0.8795, Val Accuracy: 73.66%\n",
      "Epoch [5/50] - Train Loss: 0.6595, Val Loss: 0.8652, Val Accuracy: 74.31%\n",
      "Epoch [6/50] - Train Loss: 0.6511, Val Loss: 0.8050, Val Accuracy: 75.87%\n",
      "Epoch [7/50] - Train Loss: 0.6010, Val Loss: 0.9471, Val Accuracy: 73.10%\n",
      "Epoch [8/50] - Train Loss: 0.6138, Val Loss: 0.8277, Val Accuracy: 75.55%\n",
      "Epoch [9/50] - Train Loss: 0.5269, Val Loss: 0.8251, Val Accuracy: 77.53%\n",
      "Epoch [10/50] - Train Loss: 0.4757, Val Loss: 0.8799, Val Accuracy: 77.89%\n",
      "Epoch [11/50] - Train Loss: 0.4662, Val Loss: 0.8168, Val Accuracy: 78.35%\n",
      "Epoch [12/50] - Train Loss: 0.4685, Val Loss: 1.0161, Val Accuracy: 75.71%\n",
      "Epoch [13/50] - Train Loss: 0.4553, Val Loss: 0.9261, Val Accuracy: 77.21%\n",
      "Epoch [14/50] - Train Loss: 0.4248, Val Loss: 0.9124, Val Accuracy: 77.63%\n",
      "Epoch [15/50] - Train Loss: 0.4272, Val Loss: 0.8687, Val Accuracy: 77.27%\n",
      "Epoch [16/50] - Train Loss: 0.4197, Val Loss: 0.9326, Val Accuracy: 77.47%\n",
      "Epoch [17/50] - Train Loss: 0.4195, Val Loss: 0.8980, Val Accuracy: 79.55%\n",
      "Epoch [18/50] - Train Loss: 0.4342, Val Loss: 0.9430, Val Accuracy: 78.22%\n",
      "Epoch [19/50] - Train Loss: 0.4181, Val Loss: 1.0057, Val Accuracy: 76.82%\n",
      "Epoch [20/50] - Train Loss: 0.4031, Val Loss: 0.9388, Val Accuracy: 76.88%\n",
      "Epoch [21/50] - Train Loss: 0.3681, Val Loss: 1.0118, Val Accuracy: 79.75%\n",
      "Epoch [22/50] - Train Loss: 0.3732, Val Loss: 0.9982, Val Accuracy: 77.43%\n",
      "Epoch [23/50] - Train Loss: 0.4775, Val Loss: 0.9285, Val Accuracy: 77.76%\n",
      "Epoch [24/50] - Train Loss: 0.3763, Val Loss: 1.0954, Val Accuracy: 77.82%\n",
      "Epoch [25/50] - Train Loss: 0.4050, Val Loss: 0.9375, Val Accuracy: 78.64%\n",
      "Epoch [26/50] - Train Loss: 0.4035, Val Loss: 0.8807, Val Accuracy: 79.26%\n",
      "Epoch [27/50] - Train Loss: 0.3385, Val Loss: 0.9349, Val Accuracy: 79.36%\n",
      "Epoch [28/50] - Train Loss: 0.3668, Val Loss: 0.9444, Val Accuracy: 77.99%\n",
      "Epoch [29/50] - Train Loss: 0.3343, Val Loss: 0.9130, Val Accuracy: 80.20%\n",
      "Epoch [30/50] - Train Loss: 0.3093, Val Loss: 0.9435, Val Accuracy: 78.54%\n",
      "Epoch [31/50] - Train Loss: 0.3023, Val Loss: 1.0077, Val Accuracy: 78.25%\n",
      "Epoch [32/50] - Train Loss: 0.3109, Val Loss: 1.0369, Val Accuracy: 79.55%\n",
      "Epoch [33/50] - Train Loss: 0.3099, Val Loss: 1.0494, Val Accuracy: 79.94%\n",
      "Epoch [34/50] - Train Loss: 0.3223, Val Loss: 1.1617, Val Accuracy: 78.15%\n",
      "Epoch [35/50] - Train Loss: 0.3800, Val Loss: 1.2817, Val Accuracy: 74.70%\n",
      "Epoch [36/50] - Train Loss: 0.3991, Val Loss: 0.9360, Val Accuracy: 79.19%\n",
      "Epoch [37/50] - Train Loss: 0.3397, Val Loss: 0.9872, Val Accuracy: 79.84%\n",
      "Epoch [38/50] - Train Loss: 0.3337, Val Loss: 1.0339, Val Accuracy: 80.33%\n",
      "Epoch [39/50] - Train Loss: 0.3149, Val Loss: 0.9752, Val Accuracy: 79.84%\n",
      "Epoch [40/50] - Train Loss: 0.2634, Val Loss: 1.0128, Val Accuracy: 79.13%\n",
      "Epoch [41/50] - Train Loss: 0.3087, Val Loss: 1.0411, Val Accuracy: 79.94%\n",
      "Epoch [42/50] - Train Loss: 0.3097, Val Loss: 1.0527, Val Accuracy: 80.49%\n",
      "Epoch [43/50] - Train Loss: 0.2616, Val Loss: 1.1609, Val Accuracy: 80.01%\n",
      "Epoch [44/50] - Train Loss: 0.2933, Val Loss: 1.0477, Val Accuracy: 80.69%\n",
      "Epoch [45/50] - Train Loss: 0.2984, Val Loss: 1.0691, Val Accuracy: 78.96%\n",
      "Epoch [46/50] - Train Loss: 0.2668, Val Loss: 1.1315, Val Accuracy: 79.75%\n",
      "Epoch [47/50] - Train Loss: 0.2464, Val Loss: 1.2356, Val Accuracy: 81.21%\n",
      "Epoch [48/50] - Train Loss: 0.2384, Val Loss: 1.0103, Val Accuracy: 80.89%\n",
      "Epoch [49/50] - Train Loss: 0.2637, Val Loss: 1.1141, Val Accuracy: 79.09%\n",
      "Epoch [50/50] - Train Loss: 0.2838, Val Loss: 1.0937, Val Accuracy: 81.31%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    val_loss = total_val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6c8801de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.tolist())\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2b2c501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6d52cfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 2,\n",
       " 14,\n",
       " 6,\n",
       " 4,\n",
       " 11,\n",
       " 8,\n",
       " 7,\n",
       " 15,\n",
       " 16,\n",
       " 1,\n",
       " 7,\n",
       " 14,\n",
       " 15,\n",
       " 9,\n",
       " 16,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 2,\n",
       " 10,\n",
       " 13,\n",
       " 0,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 11,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 11,\n",
       " 8,\n",
       " 13,\n",
       " 4,\n",
       " 9,\n",
       " 16,\n",
       " 6,\n",
       " 15,\n",
       " 4,\n",
       " 7,\n",
       " 16,\n",
       " 15,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 15,\n",
       " 9,\n",
       " 7,\n",
       " 2,\n",
       " 14,\n",
       " 3,\n",
       " 9,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 7,\n",
       " 11,\n",
       " 10,\n",
       " 4,\n",
       " 1,\n",
       " 13,\n",
       " 3,\n",
       " 12,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 13,\n",
       " 5,\n",
       " 16,\n",
       " 9,\n",
       " 10,\n",
       " 7,\n",
       " 4,\n",
       " 16,\n",
       " 13,\n",
       " 11,\n",
       " 10,\n",
       " 6,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 13,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 14,\n",
       " 12,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 16,\n",
       " 5,\n",
       " 16,\n",
       " 4,\n",
       " 8,\n",
       " 3,\n",
       " 15,\n",
       " 13,\n",
       " 7,\n",
       " 13,\n",
       " 10,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 2,\n",
       " 10,\n",
       " 12,\n",
       " 6,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 15,\n",
       " 6,\n",
       " 12,\n",
       " 0,\n",
       " 11,\n",
       " 13,\n",
       " 7,\n",
       " 15,\n",
       " 11,\n",
       " 14,\n",
       " 12,\n",
       " 1,\n",
       " 12,\n",
       " 8,\n",
       " 2,\n",
       " 8,\n",
       " 12,\n",
       " 6,\n",
       " 13,\n",
       " 0,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 16,\n",
       " 14,\n",
       " 15,\n",
       " 3,\n",
       " 14,\n",
       " 10,\n",
       " 8,\n",
       " 15,\n",
       " 13,\n",
       " 8,\n",
       " 14,\n",
       " 5,\n",
       " 4,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 13,\n",
       " 9,\n",
       " 0,\n",
       " 15,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 7,\n",
       " 11,\n",
       " 7,\n",
       " 11,\n",
       " 2,\n",
       " 15,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 16,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 15,\n",
       " 14,\n",
       " 1,\n",
       " 2,\n",
       " 12,\n",
       " 16,\n",
       " 5,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 8,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 15,\n",
       " 1,\n",
       " 12,\n",
       " 1,\n",
       " 11,\n",
       " 6,\n",
       " 14,\n",
       " 16,\n",
       " 10,\n",
       " 0,\n",
       " 14,\n",
       " 12,\n",
       " 10,\n",
       " 6,\n",
       " 11,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 15,\n",
       " 1,\n",
       " 14,\n",
       " 1,\n",
       " 13,\n",
       " 9,\n",
       " 6,\n",
       " 3,\n",
       " 10,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 11,\n",
       " 14,\n",
       " 2,\n",
       " 15,\n",
       " 9,\n",
       " 16,\n",
       " 2,\n",
       " 11,\n",
       " 11,\n",
       " 3,\n",
       " 11,\n",
       " 16,\n",
       " 13,\n",
       " 6,\n",
       " 2,\n",
       " 12,\n",
       " 5,\n",
       " 13,\n",
       " 3,\n",
       " 16,\n",
       " 15,\n",
       " 0,\n",
       " 6,\n",
       " 3,\n",
       " 0,\n",
       " 14,\n",
       " 11,\n",
       " 4,\n",
       " 10,\n",
       " 3,\n",
       " 14,\n",
       " 10,\n",
       " 6,\n",
       " 13,\n",
       " 1,\n",
       " 11,\n",
       " 8,\n",
       " 14,\n",
       " 15,\n",
       " 0,\n",
       " 2,\n",
       " 12,\n",
       " 6,\n",
       " 11,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 16,\n",
       " 5,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 14,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 14,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 0,\n",
       " 12,\n",
       " 3,\n",
       " 11,\n",
       " 2,\n",
       " 0,\n",
       " 8,\n",
       " 13,\n",
       " 0,\n",
       " 13,\n",
       " 6,\n",
       " 11,\n",
       " 1,\n",
       " 10,\n",
       " 4,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 16,\n",
       " 1,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 14,\n",
       " 4,\n",
       " 9,\n",
       " 14,\n",
       " 14,\n",
       " 9,\n",
       " 11,\n",
       " 0,\n",
       " 5,\n",
       " 7,\n",
       " 11,\n",
       " 7,\n",
       " 0,\n",
       " 12,\n",
       " 14,\n",
       " 3,\n",
       " 16,\n",
       " 10,\n",
       " 14,\n",
       " 15,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 12,\n",
       " 16,\n",
       " 1,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 1,\n",
       " 16,\n",
       " 11,\n",
       " 8,\n",
       " 5,\n",
       " 12,\n",
       " 3,\n",
       " 2,\n",
       " 11,\n",
       " 0,\n",
       " 9,\n",
       " 12,\n",
       " 16,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 3,\n",
       " 11,\n",
       " 10,\n",
       " 12,\n",
       " 16,\n",
       " 7,\n",
       " 0,\n",
       " 14,\n",
       " 15,\n",
       " 7,\n",
       " 16,\n",
       " 7,\n",
       " 6,\n",
       " 2,\n",
       " 13,\n",
       " 16,\n",
       " 15,\n",
       " 7,\n",
       " 16,\n",
       " 10,\n",
       " 14,\n",
       " 13,\n",
       " 2,\n",
       " 10,\n",
       " 7,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 2,\n",
       " 9,\n",
       " 14,\n",
       " 6,\n",
       " 7,\n",
       " 14,\n",
       " 2,\n",
       " 12,\n",
       " 7,\n",
       " 9,\n",
       " 7,\n",
       " 13,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 10,\n",
       " 14,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 12,\n",
       " 0,\n",
       " 11,\n",
       " 0,\n",
       " 16,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 0,\n",
       " 4,\n",
       " 16,\n",
       " 15,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 12,\n",
       " 8,\n",
       " 4,\n",
       " 14,\n",
       " 9,\n",
       " 6,\n",
       " 5,\n",
       " 13,\n",
       " 4,\n",
       " 1,\n",
       " 11,\n",
       " 11,\n",
       " 6,\n",
       " 10,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 3,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 6,\n",
       " 13,\n",
       " 9,\n",
       " 0,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 4,\n",
       " 14,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 16,\n",
       " 2,\n",
       " 0,\n",
       " 16,\n",
       " 3,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 4,\n",
       " 16,\n",
       " 12,\n",
       " 13,\n",
       " 3,\n",
       " 14,\n",
       " 1,\n",
       " 16,\n",
       " 14,\n",
       " 15,\n",
       " 0,\n",
       " 9,\n",
       " 5,\n",
       " 15,\n",
       " 13,\n",
       " 7,\n",
       " 15,\n",
       " 15,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 11,\n",
       " 16,\n",
       " 11,\n",
       " 16,\n",
       " 15,\n",
       " 0,\n",
       " 7,\n",
       " 14,\n",
       " 9,\n",
       " 12,\n",
       " 6,\n",
       " 14,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 11,\n",
       " 13,\n",
       " 5,\n",
       " 5,\n",
       " 11,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 11,\n",
       " 7,\n",
       " 10,\n",
       " 7,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 16,\n",
       " 16,\n",
       " 2,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 14,\n",
       " 6,\n",
       " 11,\n",
       " 3,\n",
       " 15,\n",
       " 10,\n",
       " 15,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 15,\n",
       " 6,\n",
       " 7,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 0,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 7,\n",
       " 11,\n",
       " 15,\n",
       " 16,\n",
       " 12,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 1,\n",
       " 14,\n",
       " 0,\n",
       " 3,\n",
       " 14,\n",
       " 11,\n",
       " 5,\n",
       " 16,\n",
       " 11,\n",
       " 13,\n",
       " 5,\n",
       " 8,\n",
       " 11,\n",
       " 4,\n",
       " 5,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 13,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 14,\n",
       " 8,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 15,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 13,\n",
       " 11,\n",
       " 13,\n",
       " 11,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 16,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 6,\n",
       " 13,\n",
       " 10,\n",
       " 0,\n",
       " 5,\n",
       " 6,\n",
       " 11,\n",
       " 10,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 12,\n",
       " 4,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 10,\n",
       " 15,\n",
       " 4,\n",
       " 15,\n",
       " 2,\n",
       " 15,\n",
       " 3,\n",
       " 15,\n",
       " 3,\n",
       " 10,\n",
       " 15,\n",
       " 2,\n",
       " 1,\n",
       " 16,\n",
       " 6,\n",
       " 16,\n",
       " 14,\n",
       " 9,\n",
       " 11,\n",
       " 7,\n",
       " 14,\n",
       " 15,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 13,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 16,\n",
       " 1,\n",
       " 15,\n",
       " 14,\n",
       " 13,\n",
       " 2,\n",
       " 12,\n",
       " 14,\n",
       " 7,\n",
       " 3,\n",
       " 16,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 16,\n",
       " 2,\n",
       " 12,\n",
       " 9,\n",
       " 6,\n",
       " 10,\n",
       " 3,\n",
       " 14,\n",
       " 9,\n",
       " 5,\n",
       " 11,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 16,\n",
       " 7,\n",
       " 4,\n",
       " 15,\n",
       " 5,\n",
       " 6,\n",
       " 16,\n",
       " 0,\n",
       " 4,\n",
       " 11,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 2,\n",
       " 6,\n",
       " 15,\n",
       " 2,\n",
       " 16,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 14,\n",
       " 9,\n",
       " 10,\n",
       " 1,\n",
       " 7,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 0,\n",
       " 12,\n",
       " 10,\n",
       " 5,\n",
       " 16,\n",
       " 0,\n",
       " 5,\n",
       " 1,\n",
       " 12,\n",
       " 12,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 13,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 1,\n",
       " 13,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 0,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 9,\n",
       " 13,\n",
       " 9,\n",
       " 11,\n",
       " 0,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 13,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 15,\n",
       " 1,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 16,\n",
       " 3,\n",
       " 12,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 13,\n",
       " 12,\n",
       " 4,\n",
       " 11,\n",
       " 5,\n",
       " 9,\n",
       " 11,\n",
       " 14,\n",
       " 6,\n",
       " 5,\n",
       " 15,\n",
       " 6,\n",
       " 11,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 10,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 13,\n",
       " 8,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 14,\n",
       " 11,\n",
       " 2,\n",
       " 8,\n",
       " 2,\n",
       " 1,\n",
       " 6,\n",
       " 14,\n",
       " 16,\n",
       " 10,\n",
       " 12,\n",
       " 2,\n",
       " 6,\n",
       " 14,\n",
       " 9,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 9,\n",
       " 13,\n",
       " 5,\n",
       " 14,\n",
       " 0,\n",
       " 13,\n",
       " 12,\n",
       " 9,\n",
       " 15,\n",
       " 4,\n",
       " 13,\n",
       " 15,\n",
       " 11,\n",
       " 2,\n",
       " 2,\n",
       " 12,\n",
       " 0,\n",
       " 14,\n",
       " 9,\n",
       " 5,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 4,\n",
       " 12,\n",
       " 3,\n",
       " 16,\n",
       " 9,\n",
       " 3,\n",
       " 11,\n",
       " 4,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 12,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 0,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 14,\n",
       " 8,\n",
       " 15,\n",
       " 14,\n",
       " 11,\n",
       " 10,\n",
       " 15,\n",
       " 14,\n",
       " 16,\n",
       " 1,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 16,\n",
       " 9,\n",
       " 14,\n",
       " 12,\n",
       " 2,\n",
       " 16,\n",
       " 11,\n",
       " 9,\n",
       " 13,\n",
       " 11,\n",
       " 6,\n",
       " 3,\n",
       " 11,\n",
       " 9,\n",
       " 12,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 12,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 14,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 16,\n",
       " 11,\n",
       " 8,\n",
       " 5,\n",
       " 2,\n",
       " 11,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 16,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 12,\n",
       " 16,\n",
       " 3,\n",
       " 9,\n",
       " 1,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 12,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 6,\n",
       " 15,\n",
       " 9,\n",
       " 15,\n",
       " 6,\n",
       " 8,\n",
       " 13,\n",
       " 14,\n",
       " 10,\n",
       " 0,\n",
       " 7,\n",
       " 6,\n",
       " 14,\n",
       " 3,\n",
       " 7,\n",
       " 4,\n",
       " 15,\n",
       " 2,\n",
       " 13,\n",
       " 16,\n",
       " 11,\n",
       " 0,\n",
       " 0,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 3,\n",
       " 7,\n",
       " 15,\n",
       " 16,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "19419ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.66      0.65       159\n",
      "           1       0.66      0.68      0.67       165\n",
      "           2       0.72      0.78      0.75       180\n",
      "           3       0.76      0.66      0.70       204\n",
      "           4       0.85      0.82      0.84       186\n",
      "           5       0.91      0.87      0.89       180\n",
      "           6       0.68      0.73      0.70       184\n",
      "           7       0.81      0.85      0.83       170\n",
      "           8       0.85      0.70      0.77       192\n",
      "           9       0.78      0.84      0.81       207\n",
      "          10       0.99      0.95      0.97       167\n",
      "          11       0.94      0.90      0.92       191\n",
      "          12       0.90      0.91      0.91       149\n",
      "          13       0.89      0.90      0.89       192\n",
      "          14       0.88      0.91      0.89       177\n",
      "          15       0.73      0.71      0.72       191\n",
      "          16       0.90      0.97      0.93       177\n",
      "\n",
      "    accuracy                           0.81      3071\n",
      "   macro avg       0.82      0.81      0.81      3071\n",
      "weighted avg       0.82      0.81      0.81      3071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd27c7e",
   "metadata": {},
   "source": [
    "## General split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24238618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "5c0fefdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/gesture-recognition-and-biometrics-electromyogram-grabmyo-1.0.2/features_split.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "6eaf5bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>session</th>\n",
       "      <th>participant</th>\n",
       "      <th>gesture</th>\n",
       "      <th>index</th>\n",
       "      <th>iemg</th>\n",
       "      <th>mav</th>\n",
       "      <th>ssi</th>\n",
       "      <th>myopulse</th>\n",
       "      <th>wflen</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis_f_w3</th>\n",
       "      <th>kurtosis_f_w4</th>\n",
       "      <th>kurtosis_f_w5</th>\n",
       "      <th>kurtosis_f_w6</th>\n",
       "      <th>kurtosis_f_w7</th>\n",
       "      <th>kurtosis_f_w8</th>\n",
       "      <th>kurtosis_f_w9</th>\n",
       "      <th>kurtosis_f_w10</th>\n",
       "      <th>kurtosis_f_w11</th>\n",
       "      <th>kurtosis_f_w12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11996.863087</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>997.135823</td>\n",
       "      <td>6.224512</td>\n",
       "      <td>8143.485828</td>\n",
       "      <td>...</td>\n",
       "      <td>61.940683</td>\n",
       "      <td>116.130710</td>\n",
       "      <td>31.371157</td>\n",
       "      <td>107.176549</td>\n",
       "      <td>64.459302</td>\n",
       "      <td>241.232060</td>\n",
       "      <td>54.880235</td>\n",
       "      <td>56.951175</td>\n",
       "      <td>42.644289</td>\n",
       "      <td>118.918727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10550.150337</td>\n",
       "      <td>0.036796</td>\n",
       "      <td>1019.627142</td>\n",
       "      <td>6.443555</td>\n",
       "      <td>6646.721181</td>\n",
       "      <td>...</td>\n",
       "      <td>160.171639</td>\n",
       "      <td>87.566832</td>\n",
       "      <td>154.616093</td>\n",
       "      <td>280.595181</td>\n",
       "      <td>160.756112</td>\n",
       "      <td>98.589493</td>\n",
       "      <td>135.655396</td>\n",
       "      <td>194.396727</td>\n",
       "      <td>294.858908</td>\n",
       "      <td>537.374741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9230.366582</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>630.013233</td>\n",
       "      <td>6.033789</td>\n",
       "      <td>6023.438546</td>\n",
       "      <td>...</td>\n",
       "      <td>195.666850</td>\n",
       "      <td>90.568034</td>\n",
       "      <td>94.299202</td>\n",
       "      <td>276.105575</td>\n",
       "      <td>101.698579</td>\n",
       "      <td>49.607516</td>\n",
       "      <td>150.348389</td>\n",
       "      <td>151.177847</td>\n",
       "      <td>156.860637</td>\n",
       "      <td>115.905382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15949.504704</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>1995.234560</td>\n",
       "      <td>4.999609</td>\n",
       "      <td>9318.559036</td>\n",
       "      <td>...</td>\n",
       "      <td>33.886272</td>\n",
       "      <td>80.377706</td>\n",
       "      <td>46.373047</td>\n",
       "      <td>64.445165</td>\n",
       "      <td>31.711430</td>\n",
       "      <td>33.328942</td>\n",
       "      <td>50.491362</td>\n",
       "      <td>81.876212</td>\n",
       "      <td>69.070232</td>\n",
       "      <td>142.723324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15936.956936</td>\n",
       "      <td>0.055584</td>\n",
       "      <td>1830.412922</td>\n",
       "      <td>5.870117</td>\n",
       "      <td>10476.465579</td>\n",
       "      <td>...</td>\n",
       "      <td>48.630658</td>\n",
       "      <td>39.722917</td>\n",
       "      <td>25.079065</td>\n",
       "      <td>84.877336</td>\n",
       "      <td>29.229362</td>\n",
       "      <td>48.843301</td>\n",
       "      <td>44.917769</td>\n",
       "      <td>39.354396</td>\n",
       "      <td>78.705208</td>\n",
       "      <td>176.820713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 573 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  session  participant  gesture  index          iemg       mav  \\\n",
       "0           0        1            1       10      0  11996.863087  0.041842   \n",
       "1           1        1            1       10      0  10550.150337  0.036796   \n",
       "2           2        1            1       10      0   9230.366582  0.032193   \n",
       "3           3        1            1       10      0  15949.504704  0.055627   \n",
       "4           4        1            1       10      0  15936.956936  0.055584   \n",
       "\n",
       "           ssi  myopulse         wflen  ...  kurtosis_f_w3  kurtosis_f_w4  \\\n",
       "0   997.135823  6.224512   8143.485828  ...      61.940683     116.130710   \n",
       "1  1019.627142  6.443555   6646.721181  ...     160.171639      87.566832   \n",
       "2   630.013233  6.033789   6023.438546  ...     195.666850      90.568034   \n",
       "3  1995.234560  4.999609   9318.559036  ...      33.886272      80.377706   \n",
       "4  1830.412922  5.870117  10476.465579  ...      48.630658      39.722917   \n",
       "\n",
       "   kurtosis_f_w5  kurtosis_f_w6  kurtosis_f_w7  kurtosis_f_w8  kurtosis_f_w9  \\\n",
       "0      31.371157     107.176549      64.459302     241.232060      54.880235   \n",
       "1     154.616093     280.595181     160.756112      98.589493     135.655396   \n",
       "2      94.299202     276.105575     101.698579      49.607516     150.348389   \n",
       "3      46.373047      64.445165      31.711430      33.328942      50.491362   \n",
       "4      25.079065      84.877336      29.229362      48.843301      44.917769   \n",
       "\n",
       "   kurtosis_f_w10  kurtosis_f_w11  kurtosis_f_w12  \n",
       "0       56.951175       42.644289      118.918727  \n",
       "1      194.396727      294.858908      537.374741  \n",
       "2      151.177847      156.860637      115.905382  \n",
       "3       81.876212       69.070232      142.723324  \n",
       "4       39.354396       78.705208      176.820713  \n",
       "\n",
       "[5 rows x 573 columns]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "b2e97bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'session', 'participant', 'gesture', 'index', 'iemg',\n",
       "       'mav', 'ssi', 'myopulse', 'wflen',\n",
       "       ...\n",
       "       'kurtosis_f_w3', 'kurtosis_f_w4', 'kurtosis_f_w5', 'kurtosis_f_w6',\n",
       "       'kurtosis_f_w7', 'kurtosis_f_w8', 'kurtosis_f_w9', 'kurtosis_f_w10',\n",
       "       'kurtosis_f_w11', 'kurtosis_f_w12'],\n",
       "      dtype='object', length=573)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "23ac7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "02a95fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 4:].values\n",
    "Y = (data.iloc[:, 2]-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "ea2fc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "54579326",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "0a841551",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "9c9788ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, output_shape):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_shape, 32),  # Input layer: Fully connected (linear) with 64 units\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(64, output_shape)  # Output layer: Fully connected (linear) with 'output_shape' units\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "f1716454",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(x_train).type(torch.float32), torch.tensor(y_train).type(torch.LongTensor))\n",
    "test_dataset = TensorDataset(torch.tensor(x_test).type(torch.float32), torch.tensor(y_test).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "3f85db70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "a9380055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and whether to shuffle the data\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "4a15a50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12280"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "50366915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12280, 568)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "ede084c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model(x_train.shape[1], len(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fc999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "4a705d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "18d940c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "766fcb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Train Loss: 2.5523, Val Loss: 22793884.8442, Val Accuracy: 28.82%\n",
      "Epoch [2/100] - Train Loss: 1.9588, Val Loss: 89489548.2231, Val Accuracy: 47.38%\n",
      "Epoch [3/100] - Train Loss: 1.4388, Val Loss: 146670934.5514, Val Accuracy: 58.87%\n",
      "Epoch [4/100] - Train Loss: 1.1389, Val Loss: 186222422.3765, Val Accuracy: 64.34%\n",
      "Epoch [5/100] - Train Loss: 0.9826, Val Loss: 241710395.6160, Val Accuracy: 66.56%\n",
      "Epoch [6/100] - Train Loss: 0.8839, Val Loss: 262570496.8720, Val Accuracy: 69.65%\n",
      "Epoch [7/100] - Train Loss: 0.8154, Val Loss: 273289552.8364, Val Accuracy: 70.99%\n",
      "Epoch [8/100] - Train Loss: 0.7639, Val Loss: 271160811.4649, Val Accuracy: 71.38%\n",
      "Epoch [9/100] - Train Loss: 0.7216, Val Loss: 353456683.4313, Val Accuracy: 72.78%\n",
      "Epoch [10/100] - Train Loss: 0.6857, Val Loss: 316731232.7453, Val Accuracy: 74.11%\n",
      "Epoch [11/100] - Train Loss: 0.6555, Val Loss: 337435360.7227, Val Accuracy: 74.41%\n",
      "Epoch [12/100] - Train Loss: 0.6300, Val Loss: 371203776.6950, Val Accuracy: 75.64%\n",
      "Epoch [13/100] - Train Loss: 0.6064, Val Loss: 499385120.6858, Val Accuracy: 76.16%\n",
      "Epoch [14/100] - Train Loss: 0.5836, Val Loss: 543521408.6691, Val Accuracy: 76.26%\n",
      "Epoch [15/100] - Train Loss: 0.5666, Val Loss: 510709664.6578, Val Accuracy: 76.82%\n",
      "Epoch [16/100] - Train Loss: 0.5483, Val Loss: 509940437.9851, Val Accuracy: 76.88%\n",
      "Epoch [17/100] - Train Loss: 0.5301, Val Loss: 583259563.3116, Val Accuracy: 77.27%\n",
      "Epoch [18/100] - Train Loss: 0.5176, Val Loss: 468073589.9771, Val Accuracy: 77.11%\n",
      "Epoch [19/100] - Train Loss: 0.5042, Val Loss: 556793099.2975, Val Accuracy: 77.92%\n",
      "Epoch [20/100] - Train Loss: 0.4925, Val Loss: 641788480.6244, Val Accuracy: 78.12%\n",
      "Epoch [21/100] - Train Loss: 0.4809, Val Loss: 663473067.3024, Val Accuracy: 77.27%\n",
      "Epoch [22/100] - Train Loss: 0.4695, Val Loss: 545917440.6094, Val Accuracy: 78.96%\n",
      "Epoch [23/100] - Train Loss: 0.4573, Val Loss: 724143360.5962, Val Accuracy: 78.77%\n",
      "Epoch [24/100] - Train Loss: 0.4507, Val Loss: 667202901.9349, Val Accuracy: 78.57%\n",
      "Epoch [25/100] - Train Loss: 0.4386, Val Loss: 744247360.5961, Val Accuracy: 79.09%\n",
      "Epoch [26/100] - Train Loss: 0.4328, Val Loss: 716576149.9500, Val Accuracy: 78.57%\n",
      "Epoch [27/100] - Train Loss: 0.4229, Val Loss: 834603328.5971, Val Accuracy: 79.32%\n",
      "Epoch [28/100] - Train Loss: 0.4134, Val Loss: 758386987.2557, Val Accuracy: 79.75%\n",
      "Epoch [29/100] - Train Loss: 0.4071, Val Loss: 744966507.2633, Val Accuracy: 79.16%\n",
      "Epoch [30/100] - Train Loss: 0.3976, Val Loss: 829374443.2486, Val Accuracy: 79.84%\n",
      "Epoch [31/100] - Train Loss: 0.3910, Val Loss: 792240704.5890, Val Accuracy: 80.30%\n",
      "Epoch [32/100] - Train Loss: 0.3820, Val Loss: 897030699.2662, Val Accuracy: 79.88%\n",
      "Epoch [33/100] - Train Loss: 0.3799, Val Loss: 796655787.2543, Val Accuracy: 80.20%\n",
      "Epoch [34/100] - Train Loss: 0.3705, Val Loss: 911052373.9181, Val Accuracy: 80.20%\n",
      "Epoch [35/100] - Train Loss: 0.3641, Val Loss: 937024896.5834, Val Accuracy: 80.46%\n",
      "Epoch [36/100] - Train Loss: 0.3620, Val Loss: 870378901.9174, Val Accuracy: 79.94%\n",
      "Epoch [37/100] - Train Loss: 0.3556, Val Loss: 914952619.2437, Val Accuracy: 81.08%\n",
      "Epoch [38/100] - Train Loss: 0.3492, Val Loss: 885759488.5871, Val Accuracy: 80.27%\n",
      "Epoch [39/100] - Train Loss: 0.3422, Val Loss: 945465301.9252, Val Accuracy: 80.82%\n",
      "Epoch [40/100] - Train Loss: 0.3361, Val Loss: 1013923648.5890, Val Accuracy: 80.82%\n",
      "Epoch [41/100] - Train Loss: 0.3316, Val Loss: 1047678549.9206, Val Accuracy: 80.59%\n",
      "Epoch [42/100] - Train Loss: 0.3264, Val Loss: 992836437.9237, Val Accuracy: 80.85%\n",
      "Epoch [43/100] - Train Loss: 0.3207, Val Loss: 897533312.5906, Val Accuracy: 81.05%\n",
      "Epoch [44/100] - Train Loss: 0.3184, Val Loss: 917294763.2532, Val Accuracy: 80.85%\n",
      "Epoch [45/100] - Train Loss: 0.3124, Val Loss: 1013566571.2636, Val Accuracy: 81.08%\n",
      "Epoch [46/100] - Train Loss: 0.3077, Val Loss: 1084570795.2602, Val Accuracy: 80.79%\n",
      "Epoch [47/100] - Train Loss: 0.3059, Val Loss: 1007988139.2601, Val Accuracy: 80.53%\n",
      "Epoch [48/100] - Train Loss: 0.3009, Val Loss: 1041583829.9152, Val Accuracy: 81.18%\n",
      "Epoch [49/100] - Train Loss: 0.2932, Val Loss: 1184800171.2530, Val Accuracy: 81.21%\n",
      "Epoch [50/100] - Train Loss: 0.2911, Val Loss: 1139838869.9328, Val Accuracy: 80.79%\n",
      "Epoch [51/100] - Train Loss: 0.2865, Val Loss: 1264863147.2624, Val Accuracy: 80.98%\n",
      "Epoch [52/100] - Train Loss: 0.2834, Val Loss: 1341820715.2773, Val Accuracy: 80.82%\n",
      "Epoch [53/100] - Train Loss: 0.2810, Val Loss: 1392467541.9420, Val Accuracy: 80.89%\n",
      "Epoch [54/100] - Train Loss: 0.2762, Val Loss: 1077067349.9364, Val Accuracy: 80.89%\n",
      "Epoch [55/100] - Train Loss: 0.2723, Val Loss: 1308275243.2748, Val Accuracy: 81.15%\n",
      "Epoch [56/100] - Train Loss: 0.2698, Val Loss: 1448895360.6183, Val Accuracy: 80.92%\n",
      "Epoch [57/100] - Train Loss: 0.2656, Val Loss: 1514983467.2771, Val Accuracy: 81.15%\n",
      "Epoch [58/100] - Train Loss: 0.2611, Val Loss: 1388684971.2846, Val Accuracy: 81.47%\n",
      "Epoch [59/100] - Train Loss: 0.2595, Val Loss: 1318309760.6220, Val Accuracy: 82.12%\n",
      "Epoch [60/100] - Train Loss: 0.2599, Val Loss: 1459118805.9562, Val Accuracy: 81.24%\n",
      "Epoch [61/100] - Train Loss: 0.2525, Val Loss: 1467742336.6240, Val Accuracy: 81.37%\n",
      "Epoch [62/100] - Train Loss: 0.2490, Val Loss: 1518317141.9749, Val Accuracy: 80.85%\n",
      "Epoch [63/100] - Train Loss: 0.2466, Val Loss: 1700716672.6314, Val Accuracy: 81.31%\n",
      "Epoch [64/100] - Train Loss: 0.2430, Val Loss: 1556146347.3032, Val Accuracy: 81.76%\n",
      "Epoch [65/100] - Train Loss: 0.2407, Val Loss: 1585317376.6360, Val Accuracy: 81.47%\n",
      "Epoch [66/100] - Train Loss: 0.2387, Val Loss: 1468705493.9863, Val Accuracy: 81.47%\n",
      "Epoch [67/100] - Train Loss: 0.2349, Val Loss: 1610766763.3049, Val Accuracy: 81.86%\n",
      "Epoch [68/100] - Train Loss: 0.2331, Val Loss: 1684418901.9777, Val Accuracy: 81.63%\n",
      "Epoch [69/100] - Train Loss: 0.2271, Val Loss: 1625761237.9687, Val Accuracy: 82.09%\n",
      "Epoch [70/100] - Train Loss: 0.2246, Val Loss: 1423835904.6470, Val Accuracy: 81.80%\n",
      "Epoch [71/100] - Train Loss: 0.2252, Val Loss: 1547434752.6601, Val Accuracy: 81.50%\n",
      "Epoch [72/100] - Train Loss: 0.2250, Val Loss: 1648778624.6506, Val Accuracy: 81.67%\n",
      "Epoch [73/100] - Train Loss: 0.2220, Val Loss: 1663032021.9994, Val Accuracy: 81.86%\n",
      "Epoch [74/100] - Train Loss: 0.2176, Val Loss: 1617186005.9999, Val Accuracy: 82.12%\n",
      "Epoch [75/100] - Train Loss: 0.2157, Val Loss: 1730966400.6845, Val Accuracy: 81.76%\n",
      "Epoch [76/100] - Train Loss: 0.2138, Val Loss: 1551834070.0075, Val Accuracy: 81.73%\n",
      "Epoch [77/100] - Train Loss: 0.2067, Val Loss: 1562303318.0164, Val Accuracy: 81.90%\n",
      "Epoch [78/100] - Train Loss: 0.2056, Val Loss: 1552843222.0143, Val Accuracy: 81.96%\n",
      "Epoch [79/100] - Train Loss: 0.2054, Val Loss: 1735039403.3623, Val Accuracy: 81.86%\n",
      "Epoch [80/100] - Train Loss: 0.1997, Val Loss: 1814684886.0288, Val Accuracy: 82.03%\n",
      "Epoch [81/100] - Train Loss: 0.2000, Val Loss: 1813722326.0342, Val Accuracy: 81.99%\n",
      "Epoch [82/100] - Train Loss: 0.2005, Val Loss: 1799808896.7128, Val Accuracy: 81.47%\n",
      "Epoch [83/100] - Train Loss: 0.1970, Val Loss: 1847367467.3655, Val Accuracy: 81.93%\n",
      "Epoch [84/100] - Train Loss: 0.1956, Val Loss: 1779864960.7024, Val Accuracy: 82.25%\n",
      "Epoch [85/100] - Train Loss: 0.1893, Val Loss: 1817266816.6979, Val Accuracy: 82.25%\n",
      "Epoch [86/100] - Train Loss: 0.1903, Val Loss: 1944894635.3744, Val Accuracy: 82.29%\n",
      "Epoch [87/100] - Train Loss: 0.1882, Val Loss: 2081929216.7133, Val Accuracy: 82.06%\n",
      "Epoch [88/100] - Train Loss: 0.1832, Val Loss: 2114105003.3931, Val Accuracy: 82.12%\n",
      "Epoch [89/100] - Train Loss: 0.1823, Val Loss: 2051738539.4046, Val Accuracy: 81.54%\n",
      "Epoch [90/100] - Train Loss: 0.1807, Val Loss: 2205842262.0721, Val Accuracy: 82.35%\n",
      "Epoch [91/100] - Train Loss: 0.1822, Val Loss: 2032549888.7248, Val Accuracy: 82.03%\n",
      "Epoch [92/100] - Train Loss: 0.1777, Val Loss: 1958707030.0710, Val Accuracy: 81.70%\n",
      "Epoch [93/100] - Train Loss: 0.1736, Val Loss: 2221615872.7407, Val Accuracy: 81.54%\n",
      "Epoch [94/100] - Train Loss: 0.1729, Val Loss: 2348143275.4278, Val Accuracy: 81.93%\n",
      "Epoch [95/100] - Train Loss: 0.1768, Val Loss: 2333627136.7534, Val Accuracy: 82.32%\n",
      "Epoch [96/100] - Train Loss: 0.1677, Val Loss: 2425424128.7619, Val Accuracy: 81.54%\n",
      "Epoch [97/100] - Train Loss: 0.1679, Val Loss: 2623900928.7551, Val Accuracy: 81.86%\n",
      "Epoch [98/100] - Train Loss: 0.1653, Val Loss: 2505262251.4275, Val Accuracy: 82.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100] - Train Loss: 0.1605, Val Loss: 2465930752.7656, Val Accuracy: 82.06%\n",
      "Epoch [100/100] - Train Loss: 0.1619, Val Loss: 2642004992.7620, Val Accuracy: 81.86%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    val_loss = total_val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451955a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48f401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "d47d9988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.tolist())\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "e439479b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "db4c22e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.67      0.68       159\n",
      "           1       0.68      0.72      0.70       165\n",
      "           2       0.77      0.83      0.80       180\n",
      "           3       0.72      0.72      0.72       204\n",
      "           4       0.86      0.78      0.82       186\n",
      "           5       0.88      0.91      0.89       180\n",
      "           6       0.73      0.67      0.70       184\n",
      "           7       0.80      0.88      0.84       170\n",
      "           8       0.79      0.73      0.76       192\n",
      "           9       0.83      0.80      0.82       207\n",
      "          10       0.98      0.97      0.97       167\n",
      "          11       0.90      0.91      0.90       191\n",
      "          12       0.81      0.91      0.86       149\n",
      "          13       0.92      0.89      0.90       192\n",
      "          14       0.84      0.88      0.86       177\n",
      "          15       0.76      0.75      0.76       191\n",
      "          16       0.94      0.94      0.94       177\n",
      "\n",
      "    accuracy                           0.82      3071\n",
      "   macro avg       0.82      0.82      0.82      3071\n",
      "weighted avg       0.82      0.82      0.82      3071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6dcce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b712cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b9a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c8f28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ceeda61",
   "metadata": {},
   "source": [
    "## Split on participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "62da2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = list(range(5,43))\n",
    "test_participants = list(range(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "94a655c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data['participant'].isin(participants)]\n",
    "test_df = data[data['participant'].isin(test_participants)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "201da092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>participant</th>\n",
       "      <th>gesture</th>\n",
       "      <th>index</th>\n",
       "      <th>iemg</th>\n",
       "      <th>mav</th>\n",
       "      <th>ssi</th>\n",
       "      <th>myopulse</th>\n",
       "      <th>wflen</th>\n",
       "      <th>diffvar</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis_f_w3</th>\n",
       "      <th>kurtosis_f_w4</th>\n",
       "      <th>kurtosis_f_w5</th>\n",
       "      <th>kurtosis_f_w6</th>\n",
       "      <th>kurtosis_f_w7</th>\n",
       "      <th>kurtosis_f_w8</th>\n",
       "      <th>kurtosis_f_w9</th>\n",
       "      <th>kurtosis_f_w10</th>\n",
       "      <th>kurtosis_f_w11</th>\n",
       "      <th>kurtosis_f_w12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6797.436339</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>313.982240</td>\n",
       "      <td>7.401855</td>\n",
       "      <td>5733.301325</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>...</td>\n",
       "      <td>79.115045</td>\n",
       "      <td>32.591676</td>\n",
       "      <td>58.994440</td>\n",
       "      <td>54.839028</td>\n",
       "      <td>50.292229</td>\n",
       "      <td>104.307591</td>\n",
       "      <td>61.155945</td>\n",
       "      <td>196.373417</td>\n",
       "      <td>92.713179</td>\n",
       "      <td>405.962532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>14079.062764</td>\n",
       "      <td>0.049104</td>\n",
       "      <td>1291.830907</td>\n",
       "      <td>6.844043</td>\n",
       "      <td>10385.642760</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>...</td>\n",
       "      <td>12.381097</td>\n",
       "      <td>12.099599</td>\n",
       "      <td>10.995862</td>\n",
       "      <td>17.486544</td>\n",
       "      <td>14.211067</td>\n",
       "      <td>14.489695</td>\n",
       "      <td>16.576432</td>\n",
       "      <td>160.712336</td>\n",
       "      <td>79.746286</td>\n",
       "      <td>170.230168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11472.717905</td>\n",
       "      <td>0.040014</td>\n",
       "      <td>960.778273</td>\n",
       "      <td>6.713477</td>\n",
       "      <td>8706.221451</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>...</td>\n",
       "      <td>48.730862</td>\n",
       "      <td>305.459894</td>\n",
       "      <td>212.763290</td>\n",
       "      <td>36.980016</td>\n",
       "      <td>29.056499</td>\n",
       "      <td>299.450092</td>\n",
       "      <td>55.901194</td>\n",
       "      <td>64.921149</td>\n",
       "      <td>59.408569</td>\n",
       "      <td>167.539726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>17507.830172</td>\n",
       "      <td>0.061062</td>\n",
       "      <td>2332.784349</td>\n",
       "      <td>6.788477</td>\n",
       "      <td>13368.566593</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>...</td>\n",
       "      <td>51.993926</td>\n",
       "      <td>51.528948</td>\n",
       "      <td>20.339587</td>\n",
       "      <td>43.117535</td>\n",
       "      <td>34.757809</td>\n",
       "      <td>59.816264</td>\n",
       "      <td>20.777295</td>\n",
       "      <td>59.169183</td>\n",
       "      <td>45.939629</td>\n",
       "      <td>43.873509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>18614.528512</td>\n",
       "      <td>0.064922</td>\n",
       "      <td>2580.763916</td>\n",
       "      <td>6.342676</td>\n",
       "      <td>13226.563635</td>\n",
       "      <td>0.005325</td>\n",
       "      <td>...</td>\n",
       "      <td>306.354173</td>\n",
       "      <td>55.281935</td>\n",
       "      <td>15.773219</td>\n",
       "      <td>26.464685</td>\n",
       "      <td>84.562723</td>\n",
       "      <td>76.696436</td>\n",
       "      <td>19.323875</td>\n",
       "      <td>37.940924</td>\n",
       "      <td>24.052192</td>\n",
       "      <td>41.660051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15346</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7435.557234</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>369.818221</td>\n",
       "      <td>6.280762</td>\n",
       "      <td>5792.222983</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>...</td>\n",
       "      <td>29.589425</td>\n",
       "      <td>25.460694</td>\n",
       "      <td>182.962449</td>\n",
       "      <td>34.957161</td>\n",
       "      <td>18.846525</td>\n",
       "      <td>37.714119</td>\n",
       "      <td>26.569683</td>\n",
       "      <td>32.614845</td>\n",
       "      <td>12.793219</td>\n",
       "      <td>16.657922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15347</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10471.625678</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>713.052200</td>\n",
       "      <td>6.967187</td>\n",
       "      <td>8628.380136</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>...</td>\n",
       "      <td>18.226247</td>\n",
       "      <td>35.715660</td>\n",
       "      <td>113.822913</td>\n",
       "      <td>64.223311</td>\n",
       "      <td>8.214120</td>\n",
       "      <td>39.211218</td>\n",
       "      <td>29.910432</td>\n",
       "      <td>26.426457</td>\n",
       "      <td>24.892159</td>\n",
       "      <td>82.241098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15348</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>11279.138159</td>\n",
       "      <td>0.039339</td>\n",
       "      <td>815.440185</td>\n",
       "      <td>6.917773</td>\n",
       "      <td>9229.795138</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>...</td>\n",
       "      <td>15.486525</td>\n",
       "      <td>29.417342</td>\n",
       "      <td>25.153035</td>\n",
       "      <td>19.632020</td>\n",
       "      <td>15.590478</td>\n",
       "      <td>15.462391</td>\n",
       "      <td>34.940941</td>\n",
       "      <td>37.564280</td>\n",
       "      <td>14.208021</td>\n",
       "      <td>15.811267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15349</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9210.946156</td>\n",
       "      <td>0.032125</td>\n",
       "      <td>563.422838</td>\n",
       "      <td>7.359570</td>\n",
       "      <td>8040.357766</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>...</td>\n",
       "      <td>14.527208</td>\n",
       "      <td>20.281059</td>\n",
       "      <td>20.418717</td>\n",
       "      <td>18.471741</td>\n",
       "      <td>14.276295</td>\n",
       "      <td>10.083487</td>\n",
       "      <td>27.337412</td>\n",
       "      <td>12.804675</td>\n",
       "      <td>20.220452</td>\n",
       "      <td>37.494222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15350</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9557.645293</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>622.632313</td>\n",
       "      <td>7.815820</td>\n",
       "      <td>8616.136489</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>...</td>\n",
       "      <td>28.617196</td>\n",
       "      <td>17.396729</td>\n",
       "      <td>21.821603</td>\n",
       "      <td>27.706896</td>\n",
       "      <td>16.773846</td>\n",
       "      <td>23.562969</td>\n",
       "      <td>20.576495</td>\n",
       "      <td>21.367926</td>\n",
       "      <td>12.371417</td>\n",
       "      <td>39.203146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13566 rows Ã— 572 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       session  participant  gesture  index          iemg       mav  \\\n",
       "119          1           10       10      0   6797.436339  0.023708   \n",
       "120          1           10       10      0  14079.062764  0.049104   \n",
       "121          1           10       10      0  11472.717905  0.040014   \n",
       "122          1           10       10      0  17507.830172  0.061062   \n",
       "123          1           10       10      0  18614.528512  0.064922   \n",
       "...        ...          ...      ...    ...           ...       ...   \n",
       "15346        3            9        9      0   7435.557234  0.025933   \n",
       "15347        3            9        9      0  10471.625678  0.036522   \n",
       "15348        3            9        9      0  11279.138159  0.039339   \n",
       "15349        3            9        9      0   9210.946156  0.032125   \n",
       "15350        3            9        9      0   9557.645293  0.033334   \n",
       "\n",
       "               ssi  myopulse         wflen   diffvar  ...  kurtosis_f_w3  \\\n",
       "119     313.982240  7.401855   5733.301325  0.000904  ...      79.115045   \n",
       "120    1291.830907  6.844043  10385.642760  0.002613  ...      12.381097   \n",
       "121     960.778273  6.713477   8706.221451  0.002182  ...      48.730862   \n",
       "122    2332.784349  6.788477  13368.566593  0.005666  ...      51.993926   \n",
       "123    2580.763916  6.342676  13226.563635  0.005325  ...     306.354173   \n",
       "...            ...       ...           ...       ...  ...            ...   \n",
       "15346   369.818221  6.280762   5792.222983  0.001070  ...      29.589425   \n",
       "15347   713.052200  6.967187   8628.380136  0.002070  ...      18.226247   \n",
       "15348   815.440185  6.917773   9229.795138  0.002355  ...      15.486525   \n",
       "15349   563.422838  7.359570   8040.357766  0.001839  ...      14.527208   \n",
       "15350   622.632313  7.815820   8616.136489  0.002036  ...      28.617196   \n",
       "\n",
       "       kurtosis_f_w4  kurtosis_f_w5  kurtosis_f_w6  kurtosis_f_w7  \\\n",
       "119        32.591676      58.994440      54.839028      50.292229   \n",
       "120        12.099599      10.995862      17.486544      14.211067   \n",
       "121       305.459894     212.763290      36.980016      29.056499   \n",
       "122        51.528948      20.339587      43.117535      34.757809   \n",
       "123        55.281935      15.773219      26.464685      84.562723   \n",
       "...              ...            ...            ...            ...   \n",
       "15346      25.460694     182.962449      34.957161      18.846525   \n",
       "15347      35.715660     113.822913      64.223311       8.214120   \n",
       "15348      29.417342      25.153035      19.632020      15.590478   \n",
       "15349      20.281059      20.418717      18.471741      14.276295   \n",
       "15350      17.396729      21.821603      27.706896      16.773846   \n",
       "\n",
       "       kurtosis_f_w8  kurtosis_f_w9  kurtosis_f_w10  kurtosis_f_w11  \\\n",
       "119       104.307591      61.155945      196.373417       92.713179   \n",
       "120        14.489695      16.576432      160.712336       79.746286   \n",
       "121       299.450092      55.901194       64.921149       59.408569   \n",
       "122        59.816264      20.777295       59.169183       45.939629   \n",
       "123        76.696436      19.323875       37.940924       24.052192   \n",
       "...              ...            ...             ...             ...   \n",
       "15346      37.714119      26.569683       32.614845       12.793219   \n",
       "15347      39.211218      29.910432       26.426457       24.892159   \n",
       "15348      15.462391      34.940941       37.564280       14.208021   \n",
       "15349      10.083487      27.337412       12.804675       20.220452   \n",
       "15350      23.562969      20.576495       21.367926       12.371417   \n",
       "\n",
       "       kurtosis_f_w12  \n",
       "119        405.962532  \n",
       "120        170.230168  \n",
       "121        167.539726  \n",
       "122         43.873509  \n",
       "123         41.660051  \n",
       "...               ...  \n",
       "15346       16.657922  \n",
       "15347       82.241098  \n",
       "15348       15.811267  \n",
       "15349       37.494222  \n",
       "15350       39.203146  \n",
       "\n",
       "[13566 rows x 572 columns]"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "3080f99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.iloc[:, 4:].values\n",
    "y_train = (train_df.iloc[:, 2] - 1).values\n",
    "\n",
    "x_test = test_df.iloc[:, 4:].values\n",
    "y_test = (test_df.iloc[:, 2] - 1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "b0cd65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "eb3078c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "374e281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "663e5674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.71151404, -0.71151404, -0.44104706, ...,  1.18273398,\n",
       "         0.3947441 ,  2.51393605],\n",
       "       [ 0.3502168 ,  0.3502168 , -0.06034413, ...,  0.86820923,\n",
       "         0.26368513,  0.75742463],\n",
       "       [-0.02981323, -0.02981323, -0.18923188, ...,  0.02334703,\n",
       "         0.05812778,  0.73737735],\n",
       "       ...,\n",
       "       [-0.05803901, -0.05803901, -0.24581592, ..., -0.21793596,\n",
       "        -0.39872317, -0.39319618],\n",
       "       [-0.35960121, -0.35960121, -0.34393309, ..., -0.43631153,\n",
       "        -0.33795433, -0.23163008],\n",
       "       [-0.30904915, -0.30904915, -0.32088124, ..., -0.36078509,\n",
       "        -0.41728609, -0.21889639]])"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "aad3c3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1428, 568)"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "07655ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13566, 568)"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "57056f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, output_shape):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_shape, 32),  # Input layer: Fully connected (linear) with 64 units\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(64, output_shape)  # Output layer: Fully connected (linear) with 'output_shape' units\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195513cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "0389d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(x_train).type(torch.float32), torch.tensor(y_train).type(torch.LongTensor))\n",
    "test_dataset = TensorDataset(torch.tensor(x_test).type(torch.float32), torch.tensor(y_test).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "bc5c89db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "e97bb9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and whether to shuffle the data\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af8b9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "c6b93e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model(x_train.shape[1], len(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583af4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "a4643bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "6374bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "bcc87418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Train Loss: 2.2616, Val Loss: 1.7581, Val Accuracy: 46.85%\n",
      "Epoch [2/100] - Train Loss: 1.3030, Val Loss: 1.4237, Val Accuracy: 58.12%\n",
      "Epoch [3/100] - Train Loss: 0.9410, Val Loss: 1.3434, Val Accuracy: 58.33%\n",
      "Epoch [4/100] - Train Loss: 0.7860, Val Loss: 1.3820, Val Accuracy: 58.75%\n",
      "Epoch [5/100] - Train Loss: 0.6940, Val Loss: 1.3767, Val Accuracy: 60.78%\n",
      "Epoch [6/100] - Train Loss: 0.6312, Val Loss: 1.3106, Val Accuracy: 61.06%\n",
      "Epoch [7/100] - Train Loss: 0.5876, Val Loss: 1.4357, Val Accuracy: 60.85%\n",
      "Epoch [8/100] - Train Loss: 0.5474, Val Loss: 1.4316, Val Accuracy: 61.97%\n",
      "Epoch [9/100] - Train Loss: 0.5167, Val Loss: 1.5336, Val Accuracy: 59.73%\n",
      "Epoch [10/100] - Train Loss: 0.4869, Val Loss: 1.5775, Val Accuracy: 59.24%\n",
      "Epoch [11/100] - Train Loss: 0.4674, Val Loss: 1.5543, Val Accuracy: 62.25%\n",
      "Epoch [12/100] - Train Loss: 0.4422, Val Loss: 1.6126, Val Accuracy: 62.11%\n",
      "Epoch [13/100] - Train Loss: 0.4295, Val Loss: 1.6357, Val Accuracy: 61.27%\n",
      "Epoch [14/100] - Train Loss: 0.4103, Val Loss: 1.6373, Val Accuracy: 62.46%\n",
      "Epoch [15/100] - Train Loss: 0.3970, Val Loss: 1.7100, Val Accuracy: 59.80%\n",
      "Epoch [16/100] - Train Loss: 0.3845, Val Loss: 1.6430, Val Accuracy: 61.13%\n",
      "Epoch [17/100] - Train Loss: 0.3686, Val Loss: 1.7613, Val Accuracy: 61.69%\n",
      "Epoch [18/100] - Train Loss: 0.3582, Val Loss: 1.7600, Val Accuracy: 61.27%\n",
      "Epoch [19/100] - Train Loss: 0.3431, Val Loss: 1.7385, Val Accuracy: 62.82%\n",
      "Epoch [20/100] - Train Loss: 0.3330, Val Loss: 1.8293, Val Accuracy: 60.43%\n",
      "Epoch [21/100] - Train Loss: 0.3298, Val Loss: 1.8117, Val Accuracy: 62.04%\n",
      "Epoch [22/100] - Train Loss: 0.3125, Val Loss: 1.8283, Val Accuracy: 60.85%\n",
      "Epoch [23/100] - Train Loss: 0.3078, Val Loss: 1.8820, Val Accuracy: 61.55%\n",
      "Epoch [24/100] - Train Loss: 0.2983, Val Loss: 1.8630, Val Accuracy: 61.27%\n",
      "Epoch [25/100] - Train Loss: 0.2934, Val Loss: 1.9303, Val Accuracy: 60.78%\n",
      "Epoch [26/100] - Train Loss: 0.2833, Val Loss: 2.1102, Val Accuracy: 60.01%\n",
      "Epoch [27/100] - Train Loss: 0.2852, Val Loss: 1.9999, Val Accuracy: 61.13%\n",
      "Epoch [28/100] - Train Loss: 0.2738, Val Loss: 2.0697, Val Accuracy: 60.15%\n",
      "Epoch [29/100] - Train Loss: 0.2614, Val Loss: 2.0733, Val Accuracy: 60.36%\n",
      "Epoch [30/100] - Train Loss: 0.2552, Val Loss: 2.0539, Val Accuracy: 61.20%\n",
      "Epoch [31/100] - Train Loss: 0.2471, Val Loss: 2.1053, Val Accuracy: 61.13%\n",
      "Epoch [32/100] - Train Loss: 0.2431, Val Loss: 2.1539, Val Accuracy: 58.89%\n",
      "Epoch [33/100] - Train Loss: 0.2364, Val Loss: 2.1597, Val Accuracy: 60.92%\n",
      "Epoch [34/100] - Train Loss: 0.2361, Val Loss: 2.1361, Val Accuracy: 60.50%\n",
      "Epoch [35/100] - Train Loss: 0.2256, Val Loss: 2.1800, Val Accuracy: 60.29%\n",
      "Epoch [36/100] - Train Loss: 0.2224, Val Loss: 2.2303, Val Accuracy: 60.08%\n",
      "Epoch [37/100] - Train Loss: 0.2166, Val Loss: 2.2292, Val Accuracy: 61.83%\n",
      "Epoch [38/100] - Train Loss: 0.2193, Val Loss: 2.2305, Val Accuracy: 61.06%\n",
      "Epoch [39/100] - Train Loss: 0.2075, Val Loss: 2.2520, Val Accuracy: 61.13%\n",
      "Epoch [40/100] - Train Loss: 0.1995, Val Loss: 2.2689, Val Accuracy: 60.92%\n",
      "Epoch [41/100] - Train Loss: 0.1956, Val Loss: 2.2746, Val Accuracy: 60.85%\n",
      "Epoch [42/100] - Train Loss: 0.2003, Val Loss: 2.3443, Val Accuracy: 60.36%\n",
      "Epoch [43/100] - Train Loss: 0.1938, Val Loss: 2.4651, Val Accuracy: 61.55%\n",
      "Epoch [44/100] - Train Loss: 0.1926, Val Loss: 2.5297, Val Accuracy: 59.66%\n",
      "Epoch [45/100] - Train Loss: 0.1891, Val Loss: 2.5499, Val Accuracy: 61.13%\n",
      "Epoch [46/100] - Train Loss: 0.1772, Val Loss: 2.5310, Val Accuracy: 61.41%\n",
      "Epoch [47/100] - Train Loss: 0.1744, Val Loss: 2.6342, Val Accuracy: 60.50%\n",
      "Epoch [48/100] - Train Loss: 0.1713, Val Loss: 2.6920, Val Accuracy: 59.73%\n",
      "Epoch [49/100] - Train Loss: 0.1714, Val Loss: 2.7741, Val Accuracy: 59.24%\n",
      "Epoch [50/100] - Train Loss: 0.1642, Val Loss: 2.7606, Val Accuracy: 59.73%\n",
      "Epoch [51/100] - Train Loss: 0.1618, Val Loss: 2.7397, Val Accuracy: 60.15%\n",
      "Epoch [52/100] - Train Loss: 0.1577, Val Loss: 2.7001, Val Accuracy: 60.57%\n",
      "Epoch [53/100] - Train Loss: 0.1575, Val Loss: 2.7155, Val Accuracy: 60.57%\n",
      "Epoch [54/100] - Train Loss: 0.1531, Val Loss: 2.8159, Val Accuracy: 60.29%\n",
      "Epoch [55/100] - Train Loss: 0.1589, Val Loss: 2.8121, Val Accuracy: 61.90%\n",
      "Epoch [56/100] - Train Loss: 0.1492, Val Loss: 2.8532, Val Accuracy: 61.06%\n",
      "Epoch [57/100] - Train Loss: 0.1401, Val Loss: 2.9090, Val Accuracy: 60.92%\n",
      "Epoch [58/100] - Train Loss: 0.1364, Val Loss: 2.9075, Val Accuracy: 60.22%\n",
      "Epoch [59/100] - Train Loss: 0.1361, Val Loss: 2.9510, Val Accuracy: 61.48%\n",
      "Epoch [60/100] - Train Loss: 0.1351, Val Loss: 2.9545, Val Accuracy: 61.41%\n",
      "Epoch [61/100] - Train Loss: 0.1320, Val Loss: 2.9840, Val Accuracy: 59.59%\n",
      "Epoch [62/100] - Train Loss: 0.1360, Val Loss: 3.0448, Val Accuracy: 60.43%\n",
      "Epoch [63/100] - Train Loss: 0.1291, Val Loss: 3.1101, Val Accuracy: 60.64%\n",
      "Epoch [64/100] - Train Loss: 0.1283, Val Loss: 3.0132, Val Accuracy: 60.85%\n",
      "Epoch [65/100] - Train Loss: 0.1230, Val Loss: 3.1837, Val Accuracy: 60.78%\n",
      "Epoch [66/100] - Train Loss: 0.1202, Val Loss: 3.2168, Val Accuracy: 59.45%\n",
      "Epoch [67/100] - Train Loss: 0.1161, Val Loss: 3.2373, Val Accuracy: 60.22%\n",
      "Epoch [68/100] - Train Loss: 0.1143, Val Loss: 3.1937, Val Accuracy: 60.92%\n",
      "Epoch [69/100] - Train Loss: 0.1085, Val Loss: 3.2038, Val Accuracy: 60.36%\n",
      "Epoch [70/100] - Train Loss: 0.1069, Val Loss: 3.2779, Val Accuracy: 60.85%\n",
      "Epoch [71/100] - Train Loss: 0.1058, Val Loss: 3.3360, Val Accuracy: 60.71%\n",
      "Epoch [72/100] - Train Loss: 0.1057, Val Loss: 3.3963, Val Accuracy: 60.85%\n",
      "Epoch [73/100] - Train Loss: 0.1144, Val Loss: 3.3581, Val Accuracy: 60.71%\n",
      "Epoch [74/100] - Train Loss: 0.1204, Val Loss: 3.4012, Val Accuracy: 61.34%\n",
      "Epoch [75/100] - Train Loss: 0.1119, Val Loss: 3.4144, Val Accuracy: 61.69%\n",
      "Epoch [76/100] - Train Loss: 0.1028, Val Loss: 3.3949, Val Accuracy: 61.41%\n",
      "Epoch [77/100] - Train Loss: 0.0995, Val Loss: 3.4302, Val Accuracy: 62.04%\n",
      "Epoch [78/100] - Train Loss: 0.0976, Val Loss: 3.5308, Val Accuracy: 61.06%\n",
      "Epoch [79/100] - Train Loss: 0.0947, Val Loss: 3.4485, Val Accuracy: 61.13%\n",
      "Epoch [80/100] - Train Loss: 0.0989, Val Loss: 3.5726, Val Accuracy: 60.64%\n",
      "Epoch [81/100] - Train Loss: 0.0937, Val Loss: 3.6024, Val Accuracy: 60.85%\n",
      "Epoch [82/100] - Train Loss: 0.0908, Val Loss: 3.5010, Val Accuracy: 61.69%\n",
      "Epoch [83/100] - Train Loss: 0.0879, Val Loss: 3.7477, Val Accuracy: 60.85%\n",
      "Epoch [84/100] - Train Loss: 0.0797, Val Loss: 3.7195, Val Accuracy: 60.78%\n",
      "Epoch [85/100] - Train Loss: 0.0794, Val Loss: 3.6928, Val Accuracy: 61.83%\n",
      "Epoch [86/100] - Train Loss: 0.0772, Val Loss: 3.7129, Val Accuracy: 61.13%\n",
      "Epoch [87/100] - Train Loss: 0.0790, Val Loss: 3.8410, Val Accuracy: 60.71%\n",
      "Epoch [88/100] - Train Loss: 0.0829, Val Loss: 3.8773, Val Accuracy: 61.06%\n",
      "Epoch [89/100] - Train Loss: 0.0817, Val Loss: 3.9033, Val Accuracy: 59.45%\n",
      "Epoch [90/100] - Train Loss: 0.0813, Val Loss: 3.9382, Val Accuracy: 60.64%\n",
      "Epoch [91/100] - Train Loss: 0.0716, Val Loss: 3.8493, Val Accuracy: 61.76%\n",
      "Epoch [92/100] - Train Loss: 0.0712, Val Loss: 3.9078, Val Accuracy: 60.71%\n",
      "Epoch [93/100] - Train Loss: 0.0756, Val Loss: 4.0132, Val Accuracy: 60.92%\n",
      "Epoch [94/100] - Train Loss: 0.0736, Val Loss: 4.0029, Val Accuracy: 60.78%\n",
      "Epoch [95/100] - Train Loss: 0.0779, Val Loss: 4.0752, Val Accuracy: 60.36%\n",
      "Epoch [96/100] - Train Loss: 0.0763, Val Loss: 4.1140, Val Accuracy: 60.64%\n",
      "Epoch [97/100] - Train Loss: 0.0722, Val Loss: 4.2343, Val Accuracy: 61.06%\n",
      "Epoch [98/100] - Train Loss: 0.0661, Val Loss: 4.1683, Val Accuracy: 60.64%\n",
      "Epoch [99/100] - Train Loss: 0.0609, Val Loss: 4.1540, Val Accuracy: 61.27%\n",
      "Epoch [100/100] - Train Loss: 0.0656, Val Loss: 4.2231, Val Accuracy: 60.99%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    val_loss = total_val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eee89ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "94a4cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "6b65d48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 9,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 12,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 12,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 6,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 12,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 0,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 12,\n",
       " 9,\n",
       " 0,\n",
       " 9,\n",
       " 12,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 1,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 12,\n",
       " 16,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 12,\n",
       " 2,\n",
       " 2,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 7,\n",
       " 7,\n",
       " 13,\n",
       " 8,\n",
       " 11,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 11,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 13,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 7,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 12,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 6,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 14,\n",
       " 12,\n",
       " 14,\n",
       " 12,\n",
       " 3,\n",
       " 14,\n",
       " 3,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 15,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 8,\n",
       " 0,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 14,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 10,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 14,\n",
       " 14,\n",
       " 3,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 14,\n",
       " 3,\n",
       " 14,\n",
       " 6,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 12,\n",
       " 3,\n",
       " 6,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 9,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 9,\n",
       " 9,\n",
       " 14,\n",
       " 12,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 15,\n",
       " 1,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 3,\n",
       " 12,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 15,\n",
       " 15,\n",
       " 12,\n",
       " 1,\n",
       " 12,\n",
       " 3,\n",
       " 13,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 13,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 10,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 13,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 13,\n",
       " 3,\n",
       " 6,\n",
       " 13,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 3,\n",
       " 10,\n",
       " 7,\n",
       " 1,\n",
       " 0,\n",
       " 13,\n",
       " 0,\n",
       " 8,\n",
       " 1,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 0,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 2,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 12,\n",
       " 1,\n",
       " 12,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 2,\n",
       " 6,\n",
       " 12,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 0,\n",
       " 8,\n",
       " 4,\n",
       " 0,\n",
       " 13,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 12,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 8,\n",
       " 0,\n",
       " 15,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 13,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 1,\n",
       " 7,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 16,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 1,\n",
       " 14,\n",
       " 1,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 3,\n",
       " 11,\n",
       " 14,\n",
       " 11,\n",
       " 12,\n",
       " 3,\n",
       " 6,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 14,\n",
       " 16,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 7,\n",
       " 14,\n",
       " 15,\n",
       " 6,\n",
       " 15,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 14,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 7,\n",
       " 10,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 14,\n",
       " 4,\n",
       " 1,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 14,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 12,\n",
       " 9,\n",
       " 11,\n",
       " 5,\n",
       " 12,\n",
       " 12,\n",
       " 2,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 0,\n",
       " 15,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 11,\n",
       " 1,\n",
       " 12,\n",
       " 11,\n",
       " 1,\n",
       " 1,\n",
       " 13,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 12,\n",
       " 10,\n",
       " 12,\n",
       " 2,\n",
       " 12,\n",
       " 12,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 13,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 12,\n",
       " 2,\n",
       " 12,\n",
       " 6,\n",
       " 1,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 1,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 0,\n",
       " 15,\n",
       " 0,\n",
       " 0,\n",
       " 15,\n",
       " ...]"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "3454b69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, ..., 8, 8, 8], dtype=int64)"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "2150215b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.61      0.57        84\n",
      "           1       0.49      0.43      0.46        84\n",
      "           2       0.52      0.57      0.55        84\n",
      "           3       0.30      0.27      0.29        84\n",
      "           4       0.72      0.50      0.59        84\n",
      "           5       0.79      0.60      0.68        84\n",
      "           6       0.37      0.55      0.44        84\n",
      "           7       0.64      0.40      0.50        84\n",
      "           8       0.69      0.52      0.59        84\n",
      "           9       0.79      0.45      0.58        84\n",
      "          10       0.83      0.89      0.86        84\n",
      "          11       0.88      0.93      0.90        84\n",
      "          12       0.47      0.74      0.58        84\n",
      "          13       0.66      0.74      0.70        84\n",
      "          14       0.52      0.79      0.63        84\n",
      "          15       0.66      0.46      0.55        84\n",
      "          16       0.88      0.92      0.90        84\n",
      "\n",
      "    accuracy                           0.61      1428\n",
      "   macro avg       0.63      0.61      0.61      1428\n",
      "weighted avg       0.63      0.61      0.61      1428\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2d719",
   "metadata": {},
   "source": [
    "### Split by session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c980d167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "e082def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = [1,3]\n",
    "test_participants = [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "e4f8cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[data['session'].isin(participants)]\n",
    "test_df = data[data['session'].isin(test_participants)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36e60310",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "0d3592d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>participant</th>\n",
       "      <th>gesture</th>\n",
       "      <th>index</th>\n",
       "      <th>iemg</th>\n",
       "      <th>mav</th>\n",
       "      <th>ssi</th>\n",
       "      <th>myopulse</th>\n",
       "      <th>wflen</th>\n",
       "      <th>diffvar</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis_f_w3</th>\n",
       "      <th>kurtosis_f_w4</th>\n",
       "      <th>kurtosis_f_w5</th>\n",
       "      <th>kurtosis_f_w6</th>\n",
       "      <th>kurtosis_f_w7</th>\n",
       "      <th>kurtosis_f_w8</th>\n",
       "      <th>kurtosis_f_w9</th>\n",
       "      <th>kurtosis_f_w10</th>\n",
       "      <th>kurtosis_f_w11</th>\n",
       "      <th>kurtosis_f_w12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>11996.863087</td>\n",
       "      <td>0.041842</td>\n",
       "      <td>997.135823</td>\n",
       "      <td>6.224512</td>\n",
       "      <td>8143.485828</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>...</td>\n",
       "      <td>61.940683</td>\n",
       "      <td>116.130710</td>\n",
       "      <td>31.371157</td>\n",
       "      <td>107.176549</td>\n",
       "      <td>64.459302</td>\n",
       "      <td>241.232060</td>\n",
       "      <td>54.880235</td>\n",
       "      <td>56.951175</td>\n",
       "      <td>42.644289</td>\n",
       "      <td>118.918727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10550.150337</td>\n",
       "      <td>0.036796</td>\n",
       "      <td>1019.627142</td>\n",
       "      <td>6.443555</td>\n",
       "      <td>6646.721181</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>...</td>\n",
       "      <td>160.171639</td>\n",
       "      <td>87.566832</td>\n",
       "      <td>154.616093</td>\n",
       "      <td>280.595181</td>\n",
       "      <td>160.756112</td>\n",
       "      <td>98.589493</td>\n",
       "      <td>135.655396</td>\n",
       "      <td>194.396727</td>\n",
       "      <td>294.858908</td>\n",
       "      <td>537.374741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9230.366582</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>630.013233</td>\n",
       "      <td>6.033789</td>\n",
       "      <td>6023.438546</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>...</td>\n",
       "      <td>195.666850</td>\n",
       "      <td>90.568034</td>\n",
       "      <td>94.299202</td>\n",
       "      <td>276.105575</td>\n",
       "      <td>101.698579</td>\n",
       "      <td>49.607516</td>\n",
       "      <td>150.348389</td>\n",
       "      <td>151.177847</td>\n",
       "      <td>156.860637</td>\n",
       "      <td>115.905382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15949.504704</td>\n",
       "      <td>0.055627</td>\n",
       "      <td>1995.234560</td>\n",
       "      <td>4.999609</td>\n",
       "      <td>9318.559036</td>\n",
       "      <td>0.003030</td>\n",
       "      <td>...</td>\n",
       "      <td>33.886272</td>\n",
       "      <td>80.377706</td>\n",
       "      <td>46.373047</td>\n",
       "      <td>64.445165</td>\n",
       "      <td>31.711430</td>\n",
       "      <td>33.328942</td>\n",
       "      <td>50.491362</td>\n",
       "      <td>81.876212</td>\n",
       "      <td>69.070232</td>\n",
       "      <td>142.723324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>15936.956936</td>\n",
       "      <td>0.055584</td>\n",
       "      <td>1830.412922</td>\n",
       "      <td>5.870117</td>\n",
       "      <td>10476.465579</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>...</td>\n",
       "      <td>48.630658</td>\n",
       "      <td>39.722917</td>\n",
       "      <td>25.079065</td>\n",
       "      <td>84.877336</td>\n",
       "      <td>29.229362</td>\n",
       "      <td>48.843301</td>\n",
       "      <td>44.917769</td>\n",
       "      <td>39.354396</td>\n",
       "      <td>78.705208</td>\n",
       "      <td>176.820713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15346</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7435.557234</td>\n",
       "      <td>0.025933</td>\n",
       "      <td>369.818221</td>\n",
       "      <td>6.280762</td>\n",
       "      <td>5792.222983</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>...</td>\n",
       "      <td>29.589425</td>\n",
       "      <td>25.460694</td>\n",
       "      <td>182.962449</td>\n",
       "      <td>34.957161</td>\n",
       "      <td>18.846525</td>\n",
       "      <td>37.714119</td>\n",
       "      <td>26.569683</td>\n",
       "      <td>32.614845</td>\n",
       "      <td>12.793219</td>\n",
       "      <td>16.657922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15347</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10471.625678</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>713.052200</td>\n",
       "      <td>6.967187</td>\n",
       "      <td>8628.380136</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>...</td>\n",
       "      <td>18.226247</td>\n",
       "      <td>35.715660</td>\n",
       "      <td>113.822913</td>\n",
       "      <td>64.223311</td>\n",
       "      <td>8.214120</td>\n",
       "      <td>39.211218</td>\n",
       "      <td>29.910432</td>\n",
       "      <td>26.426457</td>\n",
       "      <td>24.892159</td>\n",
       "      <td>82.241098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15348</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>11279.138159</td>\n",
       "      <td>0.039339</td>\n",
       "      <td>815.440185</td>\n",
       "      <td>6.917773</td>\n",
       "      <td>9229.795138</td>\n",
       "      <td>0.002355</td>\n",
       "      <td>...</td>\n",
       "      <td>15.486525</td>\n",
       "      <td>29.417342</td>\n",
       "      <td>25.153035</td>\n",
       "      <td>19.632020</td>\n",
       "      <td>15.590478</td>\n",
       "      <td>15.462391</td>\n",
       "      <td>34.940941</td>\n",
       "      <td>37.564280</td>\n",
       "      <td>14.208021</td>\n",
       "      <td>15.811267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15349</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9210.946156</td>\n",
       "      <td>0.032125</td>\n",
       "      <td>563.422838</td>\n",
       "      <td>7.359570</td>\n",
       "      <td>8040.357766</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>...</td>\n",
       "      <td>14.527208</td>\n",
       "      <td>20.281059</td>\n",
       "      <td>20.418717</td>\n",
       "      <td>18.471741</td>\n",
       "      <td>14.276295</td>\n",
       "      <td>10.083487</td>\n",
       "      <td>27.337412</td>\n",
       "      <td>12.804675</td>\n",
       "      <td>20.220452</td>\n",
       "      <td>37.494222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15350</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9557.645293</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>622.632313</td>\n",
       "      <td>7.815820</td>\n",
       "      <td>8616.136489</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>...</td>\n",
       "      <td>28.617196</td>\n",
       "      <td>17.396729</td>\n",
       "      <td>21.821603</td>\n",
       "      <td>27.706896</td>\n",
       "      <td>16.773846</td>\n",
       "      <td>23.562969</td>\n",
       "      <td>20.576495</td>\n",
       "      <td>21.367926</td>\n",
       "      <td>12.371417</td>\n",
       "      <td>39.203146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10234 rows Ã— 572 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       session  participant  gesture  index          iemg       mav  \\\n",
       "0            1            1       10      0  11996.863087  0.041842   \n",
       "1            1            1       10      0  10550.150337  0.036796   \n",
       "2            1            1       10      0   9230.366582  0.032193   \n",
       "3            1            1       10      0  15949.504704  0.055627   \n",
       "4            1            1       10      0  15936.956936  0.055584   \n",
       "...        ...          ...      ...    ...           ...       ...   \n",
       "15346        3            9        9      0   7435.557234  0.025933   \n",
       "15347        3            9        9      0  10471.625678  0.036522   \n",
       "15348        3            9        9      0  11279.138159  0.039339   \n",
       "15349        3            9        9      0   9210.946156  0.032125   \n",
       "15350        3            9        9      0   9557.645293  0.033334   \n",
       "\n",
       "               ssi  myopulse         wflen   diffvar  ...  kurtosis_f_w3  \\\n",
       "0       997.135823  6.224512   8143.485828  0.001888  ...      61.940683   \n",
       "1      1019.627142  6.443555   6646.721181  0.001438  ...     160.171639   \n",
       "2       630.013233  6.033789   6023.438546  0.001169  ...     195.666850   \n",
       "3      1995.234560  4.999609   9318.559036  0.003030  ...      33.886272   \n",
       "4      1830.412922  5.870117  10476.465579  0.003688  ...      48.630658   \n",
       "...            ...       ...           ...       ...  ...            ...   \n",
       "15346   369.818221  6.280762   5792.222983  0.001070  ...      29.589425   \n",
       "15347   713.052200  6.967187   8628.380136  0.002070  ...      18.226247   \n",
       "15348   815.440185  6.917773   9229.795138  0.002355  ...      15.486525   \n",
       "15349   563.422838  7.359570   8040.357766  0.001839  ...      14.527208   \n",
       "15350   622.632313  7.815820   8616.136489  0.002036  ...      28.617196   \n",
       "\n",
       "       kurtosis_f_w4  kurtosis_f_w5  kurtosis_f_w6  kurtosis_f_w7  \\\n",
       "0         116.130710      31.371157     107.176549      64.459302   \n",
       "1          87.566832     154.616093     280.595181     160.756112   \n",
       "2          90.568034      94.299202     276.105575     101.698579   \n",
       "3          80.377706      46.373047      64.445165      31.711430   \n",
       "4          39.722917      25.079065      84.877336      29.229362   \n",
       "...              ...            ...            ...            ...   \n",
       "15346      25.460694     182.962449      34.957161      18.846525   \n",
       "15347      35.715660     113.822913      64.223311       8.214120   \n",
       "15348      29.417342      25.153035      19.632020      15.590478   \n",
       "15349      20.281059      20.418717      18.471741      14.276295   \n",
       "15350      17.396729      21.821603      27.706896      16.773846   \n",
       "\n",
       "       kurtosis_f_w8  kurtosis_f_w9  kurtosis_f_w10  kurtosis_f_w11  \\\n",
       "0         241.232060      54.880235       56.951175       42.644289   \n",
       "1          98.589493     135.655396      194.396727      294.858908   \n",
       "2          49.607516     150.348389      151.177847      156.860637   \n",
       "3          33.328942      50.491362       81.876212       69.070232   \n",
       "4          48.843301      44.917769       39.354396       78.705208   \n",
       "...              ...            ...             ...             ...   \n",
       "15346      37.714119      26.569683       32.614845       12.793219   \n",
       "15347      39.211218      29.910432       26.426457       24.892159   \n",
       "15348      15.462391      34.940941       37.564280       14.208021   \n",
       "15349      10.083487      27.337412       12.804675       20.220452   \n",
       "15350      23.562969      20.576495       21.367926       12.371417   \n",
       "\n",
       "       kurtosis_f_w12  \n",
       "0          118.918727  \n",
       "1          537.374741  \n",
       "2          115.905382  \n",
       "3          142.723324  \n",
       "4          176.820713  \n",
       "...               ...  \n",
       "15346       16.657922  \n",
       "15347       82.241098  \n",
       "15348       15.811267  \n",
       "15349       37.494222  \n",
       "15350       39.203146  \n",
       "\n",
       "[10234 rows x 572 columns]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "b9cba7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.iloc[:, 4:].values\n",
    "y_train = (train_df.iloc[:, 2] - 1).values\n",
    "\n",
    "x_test = test_df.iloc[:, 4:].values\n",
    "y_test = (test_df.iloc[:, 2] - 1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "5d981398",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "7004f0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "63d96bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "c18322e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(input_shape, output_shape):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_shape, 32),  # Input layer: Fully connected (linear) with 64 units\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(32, 64),\n",
    "        nn.ReLU(),  # Activation function: ReLU\n",
    "        nn.Linear(64, output_shape)  # Output layer: Fully connected (linear) with 'output_shape' units\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "d5a67aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(x_train).type(torch.float32), torch.tensor(y_train).type(torch.LongTensor))\n",
    "test_dataset = TensorDataset(torch.tensor(x_test).type(torch.float32), torch.tensor(y_test).type(torch.LongTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "36978a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "3a2c43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and whether to shuffle the data\n",
    "batch_size = 128\n",
    "shuffle = True\n",
    "\n",
    "# Create data loaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912d5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "81adb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_model(x_train.shape[1], len(np.unique(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c9af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "2fafd05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "701f945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "eb0d15cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Train Loss: 2.4191, Val Loss: 1365127.5563, Val Accuracy: 38.38%\n",
      "Epoch [2/100] - Train Loss: 1.5826, Val Loss: 9924327.7019, Val Accuracy: 58.84%\n",
      "Epoch [3/100] - Train Loss: 1.1271, Val Loss: 21853153.0808, Val Accuracy: 65.88%\n",
      "Epoch [4/100] - Train Loss: 0.9310, Val Loss: 27828983.3690, Val Accuracy: 69.55%\n",
      "Epoch [5/100] - Train Loss: 0.8197, Val Loss: 34200506.5026, Val Accuracy: 72.11%\n",
      "Epoch [6/100] - Train Loss: 0.7383, Val Loss: 36717312.8797, Val Accuracy: 72.52%\n",
      "Epoch [7/100] - Train Loss: 0.6748, Val Loss: 37467072.8526, Val Accuracy: 73.99%\n",
      "Epoch [8/100] - Train Loss: 0.6270, Val Loss: 47859888.8476, Val Accuracy: 74.42%\n",
      "Epoch [9/100] - Train Loss: 0.5921, Val Loss: 45645607.2486, Val Accuracy: 74.52%\n",
      "Epoch [10/100] - Train Loss: 0.5652, Val Loss: 42729972.0853, Val Accuracy: 75.20%\n",
      "Epoch [11/100] - Train Loss: 0.5350, Val Loss: 38915735.2525, Val Accuracy: 75.83%\n",
      "Epoch [12/100] - Train Loss: 0.5119, Val Loss: 47672640.8599, Val Accuracy: 77.14%\n",
      "Epoch [13/100] - Train Loss: 0.4853, Val Loss: 86664724.0665, Val Accuracy: 76.55%\n",
      "Epoch [14/100] - Train Loss: 0.4633, Val Loss: 88890061.6927, Val Accuracy: 76.57%\n",
      "Epoch [15/100] - Train Loss: 0.4529, Val Loss: 85026317.6827, Val Accuracy: 76.96%\n",
      "Epoch [16/100] - Train Loss: 0.4307, Val Loss: 81932404.0708, Val Accuracy: 77.04%\n",
      "Epoch [17/100] - Train Loss: 0.4174, Val Loss: 78384909.7131, Val Accuracy: 76.90%\n",
      "Epoch [18/100] - Train Loss: 0.4016, Val Loss: 75472295.2895, Val Accuracy: 77.33%\n",
      "Epoch [19/100] - Train Loss: 0.3854, Val Loss: 70022407.2815, Val Accuracy: 77.29%\n",
      "Epoch [20/100] - Train Loss: 0.3755, Val Loss: 68599156.0822, Val Accuracy: 77.66%\n",
      "Epoch [21/100] - Train Loss: 0.3677, Val Loss: 65412199.3252, Val Accuracy: 76.88%\n",
      "Epoch [22/100] - Train Loss: 0.3537, Val Loss: 66018285.7340, Val Accuracy: 76.74%\n",
      "Epoch [23/100] - Train Loss: 0.3492, Val Loss: 62719687.3525, Val Accuracy: 77.55%\n",
      "Epoch [24/100] - Train Loss: 0.3417, Val Loss: 63045850.5935, Val Accuracy: 76.02%\n",
      "Epoch [25/100] - Train Loss: 0.3334, Val Loss: 65428045.7338, Val Accuracy: 77.96%\n",
      "Epoch [26/100] - Train Loss: 0.3146, Val Loss: 68037748.1543, Val Accuracy: 77.15%\n",
      "Epoch [27/100] - Train Loss: 0.3084, Val Loss: 67697076.1590, Val Accuracy: 77.66%\n",
      "Epoch [28/100] - Train Loss: 0.3049, Val Loss: 70738113.0365, Val Accuracy: 76.31%\n",
      "Epoch [29/100] - Train Loss: 0.2896, Val Loss: 70450740.1963, Val Accuracy: 77.76%\n",
      "Epoch [30/100] - Train Loss: 0.2821, Val Loss: 76075149.8170, Val Accuracy: 77.08%\n",
      "Epoch [31/100] - Train Loss: 0.2775, Val Loss: 78187316.2379, Val Accuracy: 76.80%\n",
      "Epoch [32/100] - Train Loss: 0.2750, Val Loss: 81730765.8407, Val Accuracy: 76.72%\n",
      "Epoch [33/100] - Train Loss: 0.2606, Val Loss: 80457588.2693, Val Accuracy: 77.35%\n",
      "Epoch [34/100] - Train Loss: 0.2523, Val Loss: 88110996.2567, Val Accuracy: 77.56%\n",
      "Epoch [35/100] - Train Loss: 0.2484, Val Loss: 169435841.0533, Val Accuracy: 76.98%\n",
      "Epoch [36/100] - Train Loss: 0.2421, Val Loss: 173063002.6800, Val Accuracy: 77.23%\n",
      "Epoch [37/100] - Train Loss: 0.2368, Val Loss: 175804877.8477, Val Accuracy: 77.47%\n",
      "Epoch [38/100] - Train Loss: 0.2313, Val Loss: 175110234.6514, Val Accuracy: 77.58%\n",
      "Epoch [39/100] - Train Loss: 0.2251, Val Loss: 177514868.3370, Val Accuracy: 76.43%\n",
      "Epoch [40/100] - Train Loss: 0.2244, Val Loss: 170522215.6166, Val Accuracy: 76.10%\n",
      "Epoch [41/100] - Train Loss: 0.2411, Val Loss: 170830657.1904, Val Accuracy: 76.28%\n",
      "Epoch [42/100] - Train Loss: 0.2128, Val Loss: 170227546.7590, Val Accuracy: 76.96%\n",
      "Epoch [43/100] - Train Loss: 0.2073, Val Loss: 175739009.1528, Val Accuracy: 77.04%\n",
      "Epoch [44/100] - Train Loss: 0.2036, Val Loss: 171890893.9680, Val Accuracy: 76.43%\n",
      "Epoch [45/100] - Train Loss: 0.2019, Val Loss: 171385908.3619, Val Accuracy: 77.39%\n",
      "Epoch [46/100] - Train Loss: 0.1879, Val Loss: 169566222.0132, Val Accuracy: 77.04%\n",
      "Epoch [47/100] - Train Loss: 0.1867, Val Loss: 175823962.8024, Val Accuracy: 77.41%\n",
      "Epoch [48/100] - Train Loss: 0.1825, Val Loss: 174580890.8081, Val Accuracy: 77.12%\n",
      "Epoch [49/100] - Train Loss: 0.1752, Val Loss: 178459764.4206, Val Accuracy: 77.33%\n",
      "Epoch [50/100] - Train Loss: 0.1724, Val Loss: 183333313.2472, Val Accuracy: 77.08%\n",
      "Epoch [51/100] - Train Loss: 0.1665, Val Loss: 178261159.6909, Val Accuracy: 76.51%\n",
      "Epoch [52/100] - Train Loss: 0.1601, Val Loss: 166247425.2730, Val Accuracy: 76.80%\n",
      "Epoch [53/100] - Train Loss: 0.1602, Val Loss: 169240116.5489, Val Accuracy: 75.53%\n",
      "Epoch [54/100] - Train Loss: 0.1669, Val Loss: 167587444.5211, Val Accuracy: 76.65%\n",
      "Epoch [55/100] - Train Loss: 0.1630, Val Loss: 165759156.4903, Val Accuracy: 77.56%\n",
      "Epoch [56/100] - Train Loss: 0.1536, Val Loss: 182070081.2932, Val Accuracy: 77.19%\n",
      "Epoch [57/100] - Train Loss: 0.1673, Val Loss: 180188302.1783, Val Accuracy: 77.25%\n",
      "Epoch [58/100] - Train Loss: 0.1589, Val Loss: 176147329.4172, Val Accuracy: 76.53%\n",
      "Epoch [59/100] - Train Loss: 0.1640, Val Loss: 186464794.9949, Val Accuracy: 76.59%\n",
      "Epoch [60/100] - Train Loss: 0.1441, Val Loss: 193792884.6033, Val Accuracy: 76.86%\n",
      "Epoch [61/100] - Train Loss: 0.1422, Val Loss: 200549159.7971, Val Accuracy: 77.23%\n",
      "Epoch [62/100] - Train Loss: 0.1314, Val Loss: 204225537.4299, Val Accuracy: 76.55%\n",
      "Epoch [63/100] - Train Loss: 0.1248, Val Loss: 208206593.4222, Val Accuracy: 76.76%\n",
      "Epoch [64/100] - Train Loss: 0.1229, Val Loss: 198986702.2557, Val Accuracy: 77.25%\n",
      "Epoch [65/100] - Train Loss: 0.1219, Val Loss: 201475380.6665, Val Accuracy: 76.24%\n",
      "Epoch [66/100] - Train Loss: 0.1248, Val Loss: 224214580.6708, Val Accuracy: 76.51%\n",
      "Epoch [67/100] - Train Loss: 0.1219, Val Loss: 233791310.3234, Val Accuracy: 76.78%\n",
      "Epoch [68/100] - Train Loss: 0.1132, Val Loss: 251150363.1053, Val Accuracy: 76.53%\n",
      "Epoch [69/100] - Train Loss: 0.1114, Val Loss: 259406695.9373, Val Accuracy: 77.06%\n",
      "Epoch [70/100] - Train Loss: 0.1091, Val Loss: 257289755.1875, Val Accuracy: 76.76%\n",
      "Epoch [71/100] - Train Loss: 0.1130, Val Loss: 257079783.9611, Val Accuracy: 76.57%\n",
      "Epoch [72/100] - Train Loss: 0.1071, Val Loss: 195716801.5777, Val Accuracy: 76.51%\n",
      "Epoch [73/100] - Train Loss: 0.1058, Val Loss: 208856219.1665, Val Accuracy: 76.37%\n",
      "Epoch [74/100] - Train Loss: 0.0970, Val Loss: 208171265.5665, Val Accuracy: 76.63%\n",
      "Epoch [75/100] - Train Loss: 0.1017, Val Loss: 210092737.6429, Val Accuracy: 76.14%\n",
      "Epoch [76/100] - Train Loss: 0.0940, Val Loss: 219242139.2504, Val Accuracy: 76.65%\n",
      "Epoch [77/100] - Train Loss: 0.0919, Val Loss: 219165620.8440, Val Accuracy: 76.88%\n",
      "Epoch [78/100] - Train Loss: 0.1251, Val Loss: 243174529.7724, Val Accuracy: 76.31%\n",
      "Epoch [79/100] - Train Loss: 0.1185, Val Loss: 232306152.1887, Val Accuracy: 76.06%\n",
      "Epoch [80/100] - Train Loss: 0.1010, Val Loss: 265340724.8878, Val Accuracy: 77.17%\n",
      "Epoch [81/100] - Train Loss: 0.0868, Val Loss: 265929140.9649, Val Accuracy: 76.06%\n",
      "Epoch [82/100] - Train Loss: 0.0815, Val Loss: 270091982.5355, Val Accuracy: 76.43%\n",
      "Epoch [83/100] - Train Loss: 0.0841, Val Loss: 262595841.7810, Val Accuracy: 76.28%\n",
      "Epoch [84/100] - Train Loss: 0.0896, Val Loss: 267096961.7674, Val Accuracy: 76.59%\n",
      "Epoch [85/100] - Train Loss: 0.0839, Val Loss: 276849486.5737, Val Accuracy: 76.41%\n",
      "Epoch [86/100] - Train Loss: 0.0744, Val Loss: 270521806.6442, Val Accuracy: 75.94%\n",
      "Epoch [87/100] - Train Loss: 0.0790, Val Loss: 272238542.6056, Val Accuracy: 76.72%\n",
      "Epoch [88/100] - Train Loss: 0.0783, Val Loss: 272589800.3747, Val Accuracy: 76.82%\n",
      "Epoch [89/100] - Train Loss: 0.0741, Val Loss: 289934286.6931, Val Accuracy: 76.51%\n",
      "Epoch [90/100] - Train Loss: 0.0741, Val Loss: 287199233.8924, Val Accuracy: 77.15%\n",
      "Epoch [91/100] - Train Loss: 0.0661, Val Loss: 270696296.3404, Val Accuracy: 76.53%\n",
      "Epoch [92/100] - Train Loss: 0.0641, Val Loss: 272510875.5224, Val Accuracy: 76.35%\n",
      "Epoch [93/100] - Train Loss: 0.0725, Val Loss: 273197749.1660, Val Accuracy: 76.57%\n",
      "Epoch [94/100] - Train Loss: 0.0603, Val Loss: 275136334.7655, Val Accuracy: 76.39%\n",
      "Epoch [95/100] - Train Loss: 0.0558, Val Loss: 280884097.9720, Val Accuracy: 76.72%\n",
      "Epoch [96/100] - Train Loss: 0.0584, Val Loss: 278566376.3913, Val Accuracy: 76.80%\n",
      "Epoch [97/100] - Train Loss: 0.0729, Val Loss: 272707841.9990, Val Accuracy: 76.80%\n",
      "Epoch [98/100] - Train Loss: 0.0753, Val Loss: 289343771.7241, Val Accuracy: 76.20%\n",
      "Epoch [99/100] - Train Loss: 0.0714, Val Loss: 280992053.3292, Val Accuracy: 75.86%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100] - Train Loss: 0.0618, Val Loss: 299416731.6430, Val Accuracy: 76.76%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    val_loss = total_val_loss / len(test_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31084d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "0d500a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "31ea3cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, ..., 8, 8, 8], dtype=int64)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "f44dfab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.61      0.63       301\n",
      "           1       0.65      0.51      0.57       301\n",
      "           2       0.69      0.72      0.71       301\n",
      "           3       0.57      0.64      0.61       301\n",
      "           4       0.74      0.80      0.77       301\n",
      "           5       0.88      0.87      0.87       301\n",
      "           6       0.66      0.65      0.66       301\n",
      "           7       0.78      0.85      0.81       301\n",
      "           8       0.73      0.72      0.72       301\n",
      "           9       0.79      0.82      0.80       301\n",
      "          10       0.95      0.92      0.93       301\n",
      "          11       0.86      0.85      0.86       301\n",
      "          12       0.81      0.82      0.82       301\n",
      "          13       0.84      0.83      0.84       301\n",
      "          14       0.82      0.79      0.81       301\n",
      "          15       0.70      0.66      0.68       301\n",
      "          16       0.93      0.96      0.94       301\n",
      "\n",
      "    accuracy                           0.77      5117\n",
      "   macro avg       0.77      0.77      0.77      5117\n",
      "weighted avg       0.77      0.77      0.77      5117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f6de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
